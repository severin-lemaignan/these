\chapter{Conclusion}
\label{chapter|conclusion}

\section{Knowledge-Oriented Architectures}

We have presented and illustrated in the preceding chapters how knowledge
streams can be organized within a robotic architecture. Based on the experience
gained while developing and deploying {\sc ORO}, our ontology-based knowledge
server, we have presented how symbolic knowledge could be produced from
perception and geometric reasoning in modules like {\sc SPARK}, a grounded,
perspective-aware, geometric reasoner. We have seen how symbolic knowledge
could be reused by different control systems and task planners like {\sc Cram},
{\sc SHARY}, {\sc pyRobots}, the {\sc CLSU Toolkit} or {\sc HATP} and how they
take advantage of semantic abstractions provided by knowledge base. We have
also presented the bidirectional integration of {\sc Dialogs}, a natural
language processor for English, with the knowledge base.

Altogether, these components compose an architecture that we call
\emph{knowledge-oriented}:

\begin{itemize}
    
    \item{Knowledge is explicitly stored in one central and consistent
    repository of facts, accessible by all modules.} 

    \item{Knowledge is represented in a strict formalism (OWL statements) and
    with a clearly defined vocabulary (stated in the {\tt
    commonsense.oro.owl} ontology).} 

    \item{The first two points enable both a loosely-coupled architecture where
    modules can very easily be removed or replaced by other ones as long as
    they share the same semantics (modules are defined by the knowledge they
    produce),} 

    \item{and a \emph{symbolic} reactive, event-driven approach to supervision.
    By managing events at the same level as the reasoner, we take full
    advantage of the inference abilities of ORO to trigger events whose
    \texttt{true} conditions can be inferred.} 

    \item{Finally, this architecture allows for the combination of very
    different knowledge modalities in a single homogeneous environment,
    bringing mutual benefits to components. For instance, the dialogue
    processing module can perfectly run without any geometric
    perception, but its disambiguation routines can transparently
    benefit from it when available (since richer symbolic descriptions of
    objects are then available).}

\end{itemize}

This architecture moves away from standard layered approaches. Interactions
between components are mostly bidirectional and, from the software components
point of view, we do not introduce layers of abstraction (we do, however, have
access to the lower level modules of the robot to execute actions, but all
cognition-related modules reside at the same level). This is especially visible
for the dialogue input processing. This component does not simply act as an
alternative perceptual input to the symbolic database, but also actively
queries previously acquired knowledge to disambiguate and validate the newly
created symbolic knowledge.

Our architecture relates but is to be distinguished from \emph{Beliefs,
Desires, Intentions} (BDI) architectures. BDI architectures are primarily
focused on \emph{practical reasoning}, \ie the process of deciding, step by
step, which action to perform to reach a goal (as summarised by
Woolridge~\cite{Woolridge1999}). The management of the interaction between
knowledge (the beliefs) and task and plan representation and execution (the
desires and the intentions) is central, and aims at selecting at each step the
best subgoal. It becomes then an intention that the robot commits to.

This interaction between knowledge and actions is also central to our approach
(as for any cognitive system), but task representation and task execution is
not seen as a monolithic, central function: it is one of the activity of the
robot, split between communication components (that can acquire desires from
interaction agents, amongst other things), an execution controller that may
decide to take an incoming desire into account, that create its own internal
goals, and generate and control intentions from these goals with the help of a
symbolic task planner that has also direct access to the knowledge base.

But the architecture is not really focused on this workload, and other
activities are conducted without being explicitly considered as desires:
assessment of the situation and the environment, dialogue (including
performative dialogue that can possibly change the internal state of the robot,
but does not lead to the creation of desires,  like question answering or
statement assertion), various background monitoring and recognition tasks, etc.

Regarding the anchoring question, this architecture is bidirectional. The
components we described provide a \textit{bottom-up} grounding process: SPARK
and \textsc{Dialogs} constantly build and push new symbolic contents about the
world to ORO where it becomes accessible to decisional layers. In parallel, ORO
relies on reasoning in a \textit{top-down} way to produce new facts that may
trigger in return physical behaviours. 

We believe that this \emph{knowledge-oriented} approach has a strong potential
not only to enable rich human-robot interaction, but also as a broader approach
to information alignment and fusion in complex robotic systems.  The
versatility of this paradigm could be illustrated by a simple imaginary
scenario with a blind robot and a deaf robot. The blind robot does not see (no
cameras or alike), but someone can verbally describe a scene to it. On the
other hand, the deaf robot has a good vision system, but cannot process verbal
input.  Without any changes to the software architecture that we described,
supervision modules of both robots would be able to perform equally well (to
actually implement this imaginary situation, the blind robot would of course
need \textit{a priori} 3D models of objects talked about to enable planning or
pick and place actions, and the deaf robot would require at least some gesture
interpretation to understand orders).

This architecture may also contribute to bridge the gap between robotics and
psychology: it provides clear entry points to implement some classical
psychology tests to robots. For instance, we presented experiments focused on issues related
to perspective taking. By explicitly enabling independent modeling of the
beliefs of each agent, our architecture is especially well suited to set up
cognitive and psychological experiments such as the \emph{False-Belief}
experiment~\cite{Warnier2012a}.


\subsection{Knowledge and Embodiement}

The the experiments that were presented in the previous chapter all illustrate how the
robot makes use of its embodied nature to establish a meaningful communication
with a human. Mainly, because the robot and the human share the same physical
environment and they perceive each other, we are able to create a mutual
context.

Sloman, in~\cite{Sloman2009}, argues however that the strong focus on
embodiment in the robotics community has hindered progress towards natural
human-robot interaction. Our approach has hopefully made clear that, similar to
Beetz et al.~\cite{Beetz2010}, we do not consider embodiment \emph{per se}
outside of a broader symbolic system, \ie our architecture is not bound to the
morphology or the low-level sensori-motor capabilities of a specific agent. 

However, we can build a model of the ``human point of view'' because the robot
perceives the human, and is able to estimate, at least partially, what the
human perceives or not. We infer that a human focuses on some object because
he/she points at it, looks at it, and besides, the object is visible to him.
This relies on the embodied nature of the interaction. In turn, this allows us
to understand the meaning of sentences like ``Give me that''.

We hope that this contribution shows that considering embodiment as the most
challenging and fruitful characteristic of robotics in regards to the whole AI
community does not contradict with a formal, highly symbolic approach of the
representation and decision problems that arise in robotics. 


\subsection{Lessons learned from the introduction of a symbolic knowledge layer}

When starting this PhD, we were given \emph{carte blanche} to explore ways to
explicit knowledge in our robot architecture, to make it one of the robot's
resources in its own right.


Knowledge as a observable, quantifiable, measurable, manipulable, palpable resource


Cognitive observability

Allows for both qualitative and quantitative analysis of the beliefs of the robot

Pylyshyn~\cite{Pylyshyn1989} introduces in 1989 the idea of \emph{cognitive
penetrability} in the context of the study of possible strong equivalences
between computational models and the \emph{psychological reality}:

\begin{quote}

    [One of the criterion] relies on the assumption that we can identify
    certain clear cases of phenomenon that should be accounted for at the
    knowledge level, that is, in terms of the representations alone, rather
    than in terms of properties of the cognitive architecture. Phenomena that
    depend in a rational way on subjects' goals, beliefs, and utilities are a
    case in point. For example in psychophysics we assume that if a measure
    (such as a threshold) changes systematically as we change the payoffs (that
    is, the relative cost of errors of commission and of omission), then the
    explanation of that change must be given at the knowledge level -- in terms
    of decision theory -- rather than in terms of properties of sensors or
    other mechanisms that are part of the architecture. In general showing that
    certain empirical phenomena are sensitive to goals and beliefs (or what I
    call \emph{cognitively penetrable}) is prima facie evidence that they
    should not be attributed to properties of the architecture.

\end{quote}

The introduction of an explicit \emph{knowledge level} in our architecture
makes it possible to effectively assess the cognitive penetrability of the
whole robot behaviours (this is however not new, and traditional BDI
architectures would also make this claim).

Ease modalities merging ('look at this' -> verbal and deictic, 'give me a
banana -> fetch banana picturesfrom the Web...)

Also enables new features: natural language grounding


\subsubsection{Knowledge for interaction}

\subsubsection{Real-World Symbolic Reasoning}

\begin{quote}

    Where to find milk? Milk is a subclass of dairy which is itself a subclass
    of a perishable goods. The usual storage place for perishable goods is the
    fridge, so the milk is likely to be found in a fridge.

\end{quote}

This example of reasoning, quoted from Moritz Tenorth, is a good example of
simple yet non-trivial reasoning. As a matter of fact, very few of such
reasoning cases where positively identified (and consequently implemented as
rules in ORO).

The design choices of our architecture partially explain that fact: first, the
planning task (which is the prototypical reasoning task) is delegated to a
dedicated, external planner. Then, time is not represented in ORO, and
consequently no temporal reasoning takes place at this level: action
recognition or monitoring are handled by other layers, and the underlying
reasoning tasks are not implemented as explicit symbolic rules in the knowledge
base.

The experiments we have conducted are also likely to have too simplistic
semantics to emphasise to let complex reasoning needs to emerge. More
semantically complex scenarii need to be investigated to better stress the
expressiveness and inference abilities provided by description logics.

However, it must also be noted that hundred of trivial (from a human point of
view) inferences (translating inheritance relations, domain/range constraints,
transitivity, etc.) encode a large amount of common-sense knowledge that would
be tedious, to say the least, to manually assert. These trivial inferences are
all the more important that an expressive knowledge representation language is
used: when a language like OWL allows to directly represent concepts like
partitions, cardinality restrictions, properties' ranges and domains, it leads
to a more implicit (because more abstract) description of the vocabulary, that
in turn requires more underlying reasoning. With the progress in the
understanding of the relations between expressiveness and (tractable)
satisfiability, along with the progress of reasoners, more and more of the
inferences do not need to be explicit anymore, and consequently move behind the
scenes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Towards the next generation of Knowledge Representation Systems for Robotics}
\label{sect|perspectives}

In the chapter~\ref{chapt|evaluation}, we have summarised the current state of
the art in knowledge representation, and we have put it in perspective with
some mid- or long-term views towards intelligent artifical systems, as
expressed by McCarthy and Roy.

In this chapter, we have discussed

So, we can now ask the question: what remains to be invented to get the famous
intelligent robots we dream of?

Before writing down the final mark of the thesis, we would like to feed the
reflexion on the future of knowledge and knowledge representation for robots
(service/companion robots in particular, because they are the ones with the
most obvious need of symbolic knowledge for living in complex, interactive,
semantically rich environment, but this also applies to robots in general). We
will mention three aspects, amongst many others, that we see as both important
and difficult questions.

First, one of the directions that seems both critical and under-studied in our
community is what we can call \emph{context management} in a broad sense.
Managing context means at least two things: recognising contexts and
representing contexts.

Depending on what context we talk about, recognizing contexts can be relatively
easy (who is talking to me? where am I?) to difficult (what past experience
does my interactor implicitly refers to? etc.). One of the main problem we see
with context identification is that it is a fundamentally \emph{multi-scales}
system: at every point, several temporal, spatial, social, cultural context
co-exist and overlap.

This lead to the second aspect, context representation. Techniques for
representation of overlapping pools of knowledge remain to be developed, as
well as efficient algorithms to retrieve (or discard) such context-related
pools of knowledge.

The ability to explicitly manage contexts and context switches would endow the
robot with a cognitive capability similar to what is known as
\emph{context-dependent memory} in cognitive psychology. This is also related to
Tulving's \emph{autonoetic consciousness}~\cite{Tulving1985a}: the ability to
reflect upon its own past or future experiences.

From a technical standpoint, proper context management would mean a transition
from a monolithic knowledge base to an more modular architecture, with either
multiple (overlapping) models or \emph{facets} (one per agent, one per place,
one per period of time, etc.), or maybe a systematic use of reification to
attach to each \emph{atom} of knowledge (the atom is usually the statement. It
could maybe be extended to a small set of cohesive statements) one or several
contexts. The development of modal logic for practical application is also an
important direction to examine.

Much remain to be done to this regards, starting with a formal analysis of what
are the relevant contexts for our robots.

\par

Proper management of inconsistent knowledge is another point that seems of
particular interest. Inconsistencies are mostly considered today as errors
(modeling issues, perception errors, wrong interpretations of communication, etc.)
that prevent at best further reasoning, at worst the use of the knowledge base.

However, from a cognitive point of view, logical inconsistencies are a very
valuable source of knowledge by themselves. We have previously evoked the role
of cognitive dissonances as an intrinsic motivation factor for knowledge
acquisition. This can be generalised to many sources of inconsistencies:
detection of faulty perception and setup of alternative perception strategies,
start of interactions with other agents to fix a wrong model, etc.

Technical handling of inconsistencies also require to better develop
applications of techniques like default logics to robotics.

\par

The systematic study of the relations between symbolic and geometric models are
another large field open to research. While (discrete) symbolic models are
mostly seen as abstractions of a (continuous) geometric model, this link does
not need to be unidirectional. Presupposition accommodation is an example of
{\it a priori} symbolic knowledge that may alter a geometric model. Many more
of these bidirectional relations remain to be identified.

One overlooked aspect of these links between symbolic and geometric realms is
the temporal reasoning resolution: one of the challenging task for a robot
interacting with human relates to action recognition and prediction. While mid-
to longterm action recognition is a well studied field (\cite{Ghallab1996,
Johnson2005, Tenorth2011}, early and fine grained action recognition (like
gesture initiation, back-channel communication -- nodding, social gaze, ... --,
etc.) that are important for smooth interaction, requires geometric reasoning
at relatively high temporal resolution that operates within a symbolic context.
Viewed from a slightly different perspective, better temporal resolution in the
knowledge stack could lead the way to programming paradigms that are both
massively event-oriented and semantically grounded.

\par

And maybe our final point, the knowledge-oriented architectures that are being
build in many robotic research lab around the world have a very specific
characteristic compared to the efforts of other research communities working on
the question of knowledge by human systems, like in cognitive psychology, or
computer-based systems, like in the semantic web community: robots are
\emph{embodied computers}. They act and interact in the physical world, and the
physical world plays a key role as a communication support. And at the same
time, they are computers, with unlimited access to remote sources of knowledge
via the Web, either as static database, or through exchanges with other robots.

This dual essence, both as embodied organism and disembodied agent, places the
robot at the crossing of two fundamental approaches to knowledge management:
either physically and experientially grounded, central, internal to the agent,
or on the contrary ungrounded, distributed, pervasive. The robot has this rare
privilege of being an intelligent entity that can merge and take advantage of
both. The research efforts to achieve this fusion have started, foundations have
been laid, much more remains to be explored.




