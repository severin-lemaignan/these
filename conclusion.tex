\chapter{Conclusion: On the Road to the Knowledge-Enabled Robot}
\label{chapter|conclusion}

The time has now come to reach a conclusion to this work.

We have first proposed a systematic study, build as a typology, of the
knowledge requirements of modern robotic applications in the context of service
robotics and human-robot interaction. About fifty concepts or features that
define altogether what knowledge representation means for robotics have been
proposed,followed by a review of existing tools and frameworks in the research
community.

In a second part, we have presented in depth a particular instantiation of a
knowledge representation and manipulation system that we have called
\emph{ORO}.  Main features, inner workings, algorithms, implementation of this
system have been exposed, as well as its integration with several other robot
components, for geometric reasoning, task planning or control.

The third part of the study has been focused on the processing of situated
dialogue.  Our approach and associated algorithms leading to the interactive
grounding of unconstrained verbal communication have been presented and
illustrated.

One chapter was then dedicated to evaluation. First in term of abstract
features: based on several challenges that where identified in the artificial
intelligence community, we have examined how the current systems for knowledge
management compare. Then in term of experiments and experimental tools, by
presenting six case-studies and experiments that illustrate how knowledge can
take place in our robots.

\section{The palpable knowledge}

When starting this PhD, we were given \emph{carte blanche} to explore ways to
explicit knowledge in our robot architecture, to make it one of the robot's
resources in its own right.

The main goal was to transform the knowledge in the robot from some ubiquitous,
pervasive, multi-modal and, most importantly, mostly undefined feature of the
system into an observable, quantifiable, manipulable resource, what we could
call a \emph{palpable} feature.

This transformation, both from the technical point of view (the ORO server, the
ontologies, the bindings, etc.), and as a more subtle change in the practises
related to the development of robotic components, is the main contribution of
this thesis.

Knowledge is not an abstract concept anymore: it is a set of statements, in
most cases directly intelligible to the developers, stored in one place. We can
export them, monitor them, review them, question them.

Communication between the robot's module is now conceived in term of what are
the \emph{semantics} of the information flows, instead of a simple
compatibility of interfaces. When defining the frontiers of a robotic
component, we do not think anymore only in term of ``is the interface complete
and self-contained'', but also in term of ``is the semantic complete and
consistent?''. This allow a deeper, more correct modularity: two modules that
share the same, well-defined semantic can be confidently exchanged. When we
remove or disable a component (the dialogue processing, the geometric
reasoning, ...), we know precisely what knowledge will not be available
anymore.

We call this new property of our robot, that allows for both qualitative and
quantitative analysis of the beliefs, its \emph{cognitive observability}.

It is somewhat related to the idea of \emph{cognitive penetrability} introduced
by Pylyshyn~\cite{Pylyshyn1989} in 1989, in the context of the study of
possible strong equivalences between computational models and the
\emph{psychological reality}:

\begin{quote}

    [One of the criterion] relies on the assumption that we can identify
    certain clear cases of phenomenon that should be accounted for at the
    knowledge level, that is, in terms of the representations alone, rather
    than in terms of properties of the cognitive architecture. Phenomena that
    depend in a rational way on subjects' goals, beliefs, and utilities are a
    case in point. For example in psychophysics we assume that if a measure
    (such as a threshold) changes systematically as we change the payoffs (that
    is, the relative cost of errors of commission and of omission), then the
    explanation of that change must be given at the knowledge level -- in terms
    of decision theory -- rather than in terms of properties of sensors or
    other mechanisms that are part of the architecture. In general showing that
    certain empirical phenomena are sensitive to goals and beliefs (or what I
    call \emph{cognitively penetrable}) is prima facie evidence that they
    should not be attributed to properties of the architecture.

\end{quote}

The introduction of an explicit \emph{knowledge level} in our architecture
makes it possible to effectively assess the cognitive penetrability of the
whole robot behaviours (this is however not new, and traditional BDI
architectures would also make this claim).

\section{Knowledge-oriented architectures}

We can give a broader look at the knowledge and the streams of knowledge in our
systems.  Based on the experience gained while developing and deploying {\sc
ORO}, our ontology-based knowledge server, we have presented how symbolic
knowledge could be produced from perception and geometric reasoning in modules
like {\sc SPARK}, a grounded, perspective-aware, geometric reasoner. We have
seen how symbolic knowledge could be reused by different control systems and
task planners like {\sc Cram}, {\sc SHARY}, {\sc pyRobots}, the {\sc CLSU
Toolkit} or {\sc HATP} and how they take advantage of semantic abstractions
provided by knowledge base. We have also presented the bidirectional
integration of {\sc Dialogs}, a natural language processor for English, with
the knowledge base.

Altogether, these components compose an architecture that we call
\emph{knowledge-oriented}:

\begin{itemize}
    
    \item{Knowledge is explicitly stored in one central and consistent
    repository of facts, accessible by all modules.} 

    \item{Knowledge is represented in a strict formalism (OWL statements) and
    with a clearly defined vocabulary (stated in the common-sense ontology).}

    \item{The first two points enable both a loosely-coupled architecture where
    modules can very easily be removed or replaced by other ones as long as
    they share the same semantics (modules are defined by the knowledge they
    produce),} 

    \item{and a \emph{symbolic} reactive, event-driven approach to supervision.
    By managing events at the same level as the reasoner, we take full
    advantage of the inference abilities of ORO to trigger events whose
    \texttt{true} conditions can be inferred.} 

    \item{Finally, this architecture allows for the combination of very
    different knowledge modalities in a single homogeneous environment,
    bringing mutual benefits to components. For instance, the dialogue
    processing module can perfectly run without any geometric
    perception, but its disambiguation routines can transparently
    benefit from it when available (since richer symbolic descriptions of
    objects are then available).}

\end{itemize}

This architecture moves away from standard layered approaches. Interactions
between components are mostly bidirectional and, from the software components
point of view, we do not introduce layers of abstraction (we do, however, have
access to the lower level modules of the robot to execute actions, but all
cognition-related modules reside at the same level). This is especially visible
for the dialogue input processing. This component does not simply act as an
alternative perceptual input to the symbolic database, but also actively
queries previously acquired knowledge to disambiguate and validate the newly
created symbolic knowledge.

Our architecture relates but is to be distinguished from \emph{Beliefs,
Desires, Intentions} (BDI) architectures. BDI architectures are primarily
focused on \emph{practical reasoning}, \ie the process of deciding, step by
step, which action to perform to reach a goal (as summarised by
Woolridge~\cite{Woolridge1999}). The management of the interaction between
knowledge (the beliefs) and task and plan representation and execution (the
desires and the intentions) is central, and aims at selecting at each step the
best subgoal. It becomes then an intention that the robot commits to.

This interaction between knowledge and actions is also central to our approach
(as for any cognitive system), but task representation and task execution is
not seen as a monolithic, central function: it is one of the activities of the
robot, actually split between communication components (that can acquire
desires from interaction with agents, amongst other things) and an execution
controller that may decide to take an incoming desire into account to create
its own internal goals. The controller generates and controls intentions from
these goals with the help of a symbolic task planner, that has also direct
access to the knowledge base.

The architecture is not only focused on this workload, and other
activities are conducted without being explicitly considered as desires:
assessment of the situation and the environment, dialogue (including
performative dialogue that can possibly change the internal state of the robot,
but does not lead to the creation of desires,  like question answering or
statement assertion), various background monitoring and recognition tasks, etc.

Regarding the anchoring question, this architecture is bidirectional. The
components we described provide a \textit{bottom-up} grounding process: SPARK
and \textsc{Dialogs} constantly build and push new symbolic contents about the
world to ORO where it becomes accessible to decisional layers. In parallel, ORO
relies on reasoning in a \textit{top-down} way to produce new facts that may
trigger in return physical behaviours. 

We believe that this \emph{knowledge-oriented} approach has a strong potential
not only to enable rich human-robot interaction, but also as a broader approach
to information alignment and fusion in complex robotic systems.  The
versatility of this paradigm could be illustrated by a simple imaginary
scenario with a blind robot and a deaf robot. The blind robot does not see (no
cameras or alike), but someone can verbally describe a scene to it. On the
other hand, the deaf robot has a good vision system, but cannot process verbal
input.  Without any changes to the software architecture that we described,
control modules of both robots could equally perform the same tasks since all
the knowledge is abstracted and centralised (note that to actually implement
this imaginary situation, the blind robot would of course need \textit{a
priori} 3D models of objects talked about to enable planning or pick and place
actions, and the deaf robot would require at least some gesture interpretation
to understand orders).

This architecture may also contribute to bring closer robotics and psychology:
it provides clear entry points to implement some classical psychology tests to
robots. For instance, we presented experiments focused on issues related to
perspective taking. By explicitly enabling independent modeling of the beliefs
of each agent, our architecture is especially well suited to set up cognitive
and psychological experiments that involve a theory of mind, such as
\emph{False-Belief} experiments, as recently presented in~\cite{Warnier2012a}.


\subsection*{Knowledge and Embodiement}

The the experiments that were presented in the previous chapter all illustrate how the
robot makes use of its embodied nature to establish a meaningful communication
with a human. Mainly, because the robot and the human share the same physical
environment and they perceive each other, we are able to create a mutual
context.

Sloman, in~\cite{Sloman2009}, worried however that the strong focus on
embodiment in the robotics community has hindered progress towards natural
human-robot interaction by focusing on sub-symbolic physical properties. Our
approach has hopefully made clear that, similar to Beetz et
al.~\cite{Beetz2010} and many other researches presented in the thesis, we do
not consider embodiment \emph{per se} outside of a broader symbolic system, \ie
our architecture is not bound to the morphology or the low-level sensori-motor
capabilities of a specific agent. 

However, we can build a model of the ``human point of view'' because the robot
perceives the human, and is able to estimate, at least partially, what the
human perceives or not. We infer that a human focuses on some object because
he/she points at it, looks at it, and besides, the object is visible to him.
This relies on the embodied nature of the interaction. In turn, this allows us
to understand the meaning of sentences like ``Give me that''.

We hope that this contribution shows that considering embodiment as the most
challenging and fruitful characteristic of robotics in regards to the whole AI
community does not contradict with a formal, highly symbolic approach of the
representation and decision problems that arise in robotics. 


\subsection*{Real-World Symbolic Reasoning}

\begin{quote}

    Where to find milk? Milk is a subclass of dairy which is itself a subclass
    of a perishable goods. The usual storage place for perishable goods is the
    fridge, so the milk is likely to be found in a fridge.

\end{quote}

This example of reasoning, quoted from Moritz Tenorth, is a good example of
simple yet non-trivial reasoning. As a matter of fact, only very few of such
reasoning cases where positively identified in our scenarii and experiments
(and consequently implemented as rules in ORO).

The design choices of our architecture partially explain that fact: first, the
planning task (which is the prototypical reasoning task) is delegated to a
dedicated, external planner. Then, time is not represented in ORO, and
consequently no temporal reasoning takes place at this level: action
recognition or monitoring are handled by other layers, and the underlying
reasoning tasks are not implemented as explicit symbolic rules in the knowledge
base.

The experiments we have conducted are also likely to have too simplistic
semantics to let complex reasoning needs to emerge. Scenarii with more complex
semantics would be desirable to better stress the expressiveness and inference
abilities provided by description logics.

Is reasoning at the knowledge level immature or even superfluous, then? Not so:
hundreds of trivial (from a human point of view) inferences are continuously
produced by the system (translating inheritance relations, domain/range
constraints, transitivity, etc.) and encode a large amount of common-sense
knowledge that would be tedious, to say the least, to manually assert. These
trivial inferences are all the more important that an expressive knowledge
representation language is used: when a language like OWL allows to directly
represent high-level concepts like partitions, cardinality restrictions,
properties' ranges and domains, it leads to a more implicit (because more
abstract) description of the vocabulary that in turn requires more underlying
reasoning. With the progress in the understanding of the relations between
expressiveness and (tractable) satisfiability, along with the progress of
reasoners, more and more of the inferences do not need to be explicit anymore,
and consequently move behind the scenes.

And we predict that \emph{common-sense encoding} is likely to remain the main
application of reasoning in our robotic architectures, where reasoning related
to decision making mostly happen outside the knowledge representation system.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Towards the next generation of knowledge representation systems for robotics}
\label{sect|perspectives}

In the chapter~\ref{chapt|evaluation}, we have summarised the current state of
the art in knowledge representation, and we have put it in perspective with
some mid- or long-term views towards intelligent artificial systems, as
expressed by McCarthy and Roy.

So, we can now ask the question: what remains to be invented to get the famous
``intelligent'' robots we all dream of?

Before writing down the final mark of the thesis, we would like to feed the
reflexion on the future of knowledge and knowledge representation for robots
(service/companion robots in particular, because they are the ones with the
most obvious need of symbolic knowledge for living in complex, interactive,
semantically rich environment, but this certainly applies to other robots as
well). We will mention three aspects, amongst many others, that we see as both
important and difficult questions.

First, one of the directions that seems both critical and under-studied in our
community is what we can call \emph{context management} in a broad sense.
Managing context means at least two things: recognising contexts and
representing contexts.

Depending on what context we talk about, recognizing contexts can be relatively
easy (who is talking to me? where am I?) to difficult (what past experience
does my interactor implicitly refers to?). One of the main problem we see
with context identification is that it is a fundamentally \emph{multi-scale}
problem: at any moment, several temporal, spatial, social, cultural context
co-exist and overlap.

This lead to the second aspect, context representation. Techniques for
representation of overlapping pools of knowledge remain to be developed, as
well as efficient algorithms to retrieve (or discard) such context-related
pools of knowledge.

The ability to explicitly manage contexts and context switches would endow the
robot with a cognitive capability similar to what is known as
\emph{context-dependent memory} in cognitive psychology. This is also related to
Tulving's \emph{autonoetic consciousness}~\cite{Tulving1985a}: the ability to
reflect upon its own past or future experiences.

From a technical standpoint, proper context management would mean a transition
from a monolithic knowledge base to an more modular architecture, with either
multiple (overlapping) models or \emph{facets} (one per agent, one per place,
one per period of time, etc.), or maybe a systematic use of reification to
attach to each \emph{atom} of knowledge (the atom is usually the statement. It
could maybe be extended to a small set of cohesive statements) one or several
contexts. The development of modal logic in practical applications is also an
important direction to examine.

Much remain to be done to this regard, starting with a formal analysis of what
are the relevant contexts for our robots.

\par

Proper management of inconsistent knowledge is another point that seems of
particular interest. Inconsistencies are mostly considered today as errors
(modeling issues, perception errors, wrong interpretations of communication, etc.)
that prevent, at best, further reasoning, at worst, the use of the knowledge base.

However, from a cognitive point of view, logical inconsistencies are a very
valuable source of knowledge by themselves. We have previously evoked the role
of cognitive dissonances as an intrinsic motivation factor for knowledge
acquisition. This can be generalised to many sources of inconsistencies:
detection of faulty perception and setup of alternative perception strategies,
start of interactions with other agents to fix a wrong model, etc.

Recognition of inconsistencies between different models (for instance,
divergences between the mental state of two agents) could also be a fruitful
motivation for action and interaction for the robot.

To this respect, technical handling of inconsistencies also require more
research efforts, that include the development of techniques like default logics
for robotics.

\par

The systematic study of the relations between symbolic and geometric models are
another broad field open to research. While (discrete) symbolic models are
usually seen as abstractions of a (continuous) geometric model, this link does
not need to be unidirectional. Presupposition accommodation is an example of
{\it a priori} symbolic knowledge that may alter a geometric model. Many more
of these bidirectional relations remain to be identified.

One overlooked aspect of these links between symbolic and geometric realms is
the temporal reasoning granularity: one of the challenging task for a robot
interacting with humans relates to action recognition and prediction. While
mid- to longterm action recognition is a well studied field~\cite{Ghallab1996,
Johnson2005, Tenorth2011}, early and fine grained action recognition (like
gesture initiation, back-channel communication -- nodding, social gaze, ... --,
etc.) that are important for smooth interaction, requires geometric reasoning
at relatively high temporal resolution that is able to operate within a
symbolic context.  Viewed from a slightly different perspective, better
temporal resolution in the knowledge stack could lead the way to programming
paradigms that are massively event-oriented and still semantically grounded.

\par

Our last word will be for the idea embodiment and the research perspectives it
opens: the knowledge-oriented architectures that are being build in many
robotic research lab around the world have a very specific characteristic
compared to the efforts of other research communities working on the question
of knowledge by human systems, like in cognitive psychology, or computer-based
systems, like in the semantic web community: robots are \emph{embodied
computers}. They act and interact in the physical world, and the physical world
plays a key role as a communication support. And at the same time, they are
computers, with unlimited access to remote sources of knowledge via the Web,
either as static database, or through exchanges with other robots.

This dual essence, both as embodied organism and disembodied agent, places the
robot at the crossing of two fundamental approaches to knowledge management:
either physically and experientially grounded, central, internal to the agent,
or on the contrary ungrounded, distributed, pervasive. The robot has this rare
privilege of being an intelligent entity that can merge and take advantage of
both. The research efforts to achieve this fusion have started, foundations have
been laid, much more remains to be explored.




