\chapter{Conclusion}
\label{chapter|conclusion}

\section{Discussions}
\label{sect|discussion}

\subsection{Modeling the Real World}
\label{modeling_real_world}

The main challenge we address in this work can be formulated as \emph{How to
model real-world interaction in a symbolic way, processable by the robot to
make decisions}. In the paper we used several times the term \emph{grounding} to
describe the process of binding percepts to symbols (later organized in a
first-order logic framework).  We would like to relate it to
Sloman's~\cite{Sloman2007} stance against the \emph{``Symbol Grounding meme''},
where he argues that symbolic grounding is bound to the representation of
somatic concepts (\ie roughly, the sensori-motor relationships that the robot
learns from its interaction with the world) which in turn severely constraints
the domain of concepts accessible to the robot. We could call this type of
grounding \emph{bottom-up} grounding, and Steels~\cite{Steels2007} claims it is
a solved issue.

For us, \emph{grounding} is on the contrary a \emph{top-down} activity: the
robot needs to automatically bind a representation (for instance, a word
uttered by a human, an image taken from a camera, a sentence extracted from
Wikipedia) to an unambiguous, context-dependent, internal concept. This 
concept may (or may not) be \textit{a priori} available to the robot as a
pre-loaded ontology (what we previously called the cultural background of the
robot).

Note also that perception issues have been solved in our experiments by using a
tag-based object identification method. In
section~\ref{informational_content_extraction} we give an example where the
human says ``the yellow banana is big''. It is assumed in the example that the
robot already knows about a banana instance that is yellow. In our experiments,
this kind of knowledge was either hard coded in scenario-specific ontologies
(\eg \stmt{banana\_01 type Banana} where \concept{banana\_01} is the id of the
banana's tag) or taught to the robot with prescriptive sentences like ``Learn
that this is a banana'' while pointing at the banana's tag. It would be
interesting to extend this approach with automatic classifiers (for colour,
size, etc.). If the robot later discovers a yellowish and large object, an
utterance like ``the yellow banana is big'' could be used to assert that this
object is a banana.  A similar approach focused on the combination of visual
perception and communication modalities to achieve visual learning has been
developed by~\cite{Vrecko2009}.

While the examples we develop are all based on symbols that have a physical
meaning, the system deals equally well with abstract, \emph{exo-somatic},
concepts like \emph{Time}, \emph{Event} or \emph{Place}. Demonstrating this in
real experiments would be an interesting development.

Amongst the other shortcomings of our architecture, neither the \emph{domain of
validity} nor the context of a fact are represented in a satisfying way (we do
store some kind of context --the agent's mental model for instance). This
information is meta-information on the knowledge. While the ORO framework
allows them through \emph{statement reification}, it does not offer yet a
convenient way to store them. One obvious limitation that derives from the lack
of efficient meta-knowledge is the absence of knowledge history.  With ORO, the
robot always lives in the present.

Along the same lines, our current framework lacks a proper management of
uncertainty which is essential for real world environments. A probabilistic
layer should be added by attaching truth probabilities to statements, similar
to~\cite{Jain2009}.

\subsubsection{On Thematic Roles and Action Models}

The current implementation relies on a small, predefined set of action verbs
that can be recognized from natural language
(section~\ref{processing_of_actions}).  This constraint does not come from the
resolution algorithm itself, but rather from the difficulty to automatically
extract the thematic roles associated to a verb.  This could be improved by
linking a symbolic task planner to the \textsc{Dialogs} module to dynamically
provide the list of actions that the robot can process, \ie actions for which
the robot can produce a plan. Additionally, we could exploit on-line resources
like \textsc{VerbNet}~\cite{Kipper2008}, which provides a large
machine-processable lexicon of English verbs along with their thematic roles.

\subsection{Knowledge and Embodiement}

The three experiments that were presented in the paper all illustrate how the
robot makes use of its embodied nature to establish a meaningful communication
with a human. Mainly, because the robot and the human share the same physical
environment and they perceive each other, we are able to create a mutual
context.

Sloman, in~\cite{Sloman2009}, argues however that the strong focus on
embodiment in the robotics community has hindered progress towards natural
human-robot interaction. Our approach has hopefully made clear that, similar to
Beetz et al.~\cite{Beetz2010}, we do not consider embodiment \emph{per se}
outside of a broader symbolic system, \ie our architecture is not bound to the
morphology or the low-level sensori-motor capabilities of a specific agent. 

However, we can build a model of the ``human point of view'' because the robot
perceives the human, and is able to estimate, at least partially, what the
human perceives or not. We infer that a human focuses on some object because
he/she points at it, looks at it, and besides, the object is visible to him.
This relies on the embodied nature of the interaction. In turn, this allows us
to understand the meaning of sentences like ``Give me that''.

We hope that this contribution shows that considering embodiment as the most
challenging and fruitful characteristic of robotics in regards to the whole AI
community does not contradict with a formal, highly symbolic approach of the
representation and decision problems that arise in robotics. 

Let us conclude this article briefly reviewing and linking Roy's list of challenges
for human-robot dialogue with our current approach: 
\begin{itemize}

	\item While more modalities (especially, deictic gestures and social gazes)
	can be added, we have actually proposed a \emph{cross-modal
	representation system}.

	\item One of the main feature of the \textsc{Dialogs} module is its ability
	to interactively ground concepts through disambiguation, bringing the
	ability for the robot to \emph{associate words with perceptual and action
	categories}.

	\item The ORO knowledge base offers some support for the \emph{modeling of
	context}, but a lot remains to be done in this respect.

	\item \emph{Figuring out the right granularity of models} is partially
	solved by supporting both a geometric reasoning level and a purely symbolic
	level. Generally speaking, it appears that complex robotic systems need
	to operate with a dynamic granularity, depending on the task to achieve.

	\item \emph{Temporal modeling} is currently missing in our architecture,
	and symbolic and geometric \emph{planning} is accomplished outside of the
	knowledge representation loop we presented here. We see planning as an
	essential tool to build predictive knowledge, and we are looking into this
	direction.

	\item Since we provide no time management, our system is currently not able
	to \emph{match past (learned) experiences with the current interaction}.
	This ability is obviously a key step for general action recognition, and
	seems of particular importance for the robot to assess the state of the
	interaction with the human.

	\item Finally, Roy mentions \emph{the ability to take into account the
	human perspective}: this is probably our main contribution which we are now
	trying to develop even further towards psychology-inspired experiments.

\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Knowledge-Oriented Architectures}


We have presented qnd illustrated in the preceding chapters how knowledge
streams can be organized within a robotic architecture. Based on the experience
gained while developing and deploying {\sc ORO}, our ontology-based knowledge
server, we have presented how symbolic knowledge could be produced from
perception and geometric reasoning in modules like {\sc SPARK}, a grounded,
perspective-aware, geometric reasoner. We have seen how symbolic knowledge
could be reused by different control systems and task planners like {\sc CRAM},
{\sc SHARY}, {\sc pyRobots}, the {\sc CSLU Toolkit} or {\sc HATP}and how they
take advantage of semantic abstractions provided by knowledge base. We have
also presented the bidirectional integration of {\sc Dialogs}, a natural
language processor for English, with the knowledge base.

Altogether, these components compose an architecture that we call
\emph{knowledge-oriented}:

\begin{itemize}
    
    \item{Knowledge is explicitly stored in one central and consistent
    repository of facts, accessible by all modules.} 

    \item{Knowledge is represented in a strict formalism (OWL statements) and
    with a clearly defined vocabulary (stated in the {\tt
    commonsense.oro.owl} ontology).} 

    \item{The first two points enable both a loosely-coupled architecture where
    modules can very easily be removed or replaced by other ones as long as
    they share the same semantics (modules are defined by the knowledge they
    produce),} 

    \item{and a \emph{symbolic} reactive, event-driven approach to supervision.
    By managing events at the same level as the reasoner, we take full
    advantage of the inference abilities of ORO to trigger events whose
    \texttt{true} conditions can be inferred.} 

    \item{Finally, this architecture allows for the combination of very
    different knowledge modalities in a single homogeneous environment,
    bringing mutual benefits to components. For instance, the dialogue
    processing module can perfectly run without any geometric
    perception, but its disambiguation routines can transparently
    benefit from it when available (since richer symbolic descriptions of
    objects are then available).}

\end{itemize}

This architecture moves away from standard layered approaches. Interactions
between components are mostly bidirectional and, from the software components
point of view, we do not introduce layers of abstraction (we do, however, have
access to the lower level modules of the robot to execute actions, but all
cognition-related modules reside at the same level). This is especially visible
for the dialogue input processing. This component does not simply act as an
alternative perceptual input to the symbolic database, but also actively
queries previously acquired knowledge to disambiguate and validate the newly
created symbolic knowledge, as we will present in detail in the following
chapter.

Regarding the anchoring question, this architecture is bidirectional. The
components we described provide a \textit{bottom-up} grounding process: SPARK
and \textsc{Dialogs} constantly build and push new symbolic contents about the
world to ORO where it becomes accessible to decisional layers. In parallel, ORO
relies on reasoning in a \textit{top-down} way to produce new facts that may
trigger in return physical behaviours. 

We believe that this \emph{knowledge-oriented} approach has a strong potential
not only to enable rich human-robot interaction, but also as a broader approach
to information alignment and fusion in complex robotic systems.  The
versatility of this paradigm could be illustrated by a simple imaginary
scenario with a blind robot and a deaf robot. The blind robot does not see (no
cameras or alike), but someone can verbally describe a scene to it. On the
other hand, the deaf robot has a good vision system, but cannot process verbal
input.  Without any changes to the software architecture that we described,
supervision modules of both robots would be able to perform equally well (to
actually implement this imaginary situation, the blind robot would of course
need \textit{a priori} 3D models of objects talked about to enable planning or
pick and place actions, and the deaf robot would require at least some gesture
interpretation to understand orders).

This architecture may also contribute to bridge the gap between robotics and
psychology: it provides clear entry points to implement some classical
psychology tests to robots. We presented experiments focused on issues related
to perspective taking. By explicitly enabling independent modeling of the
beliefs of each agent, our architecture is especially well suited to set up
cognitive and psychological experiments (such as the \emph{False-Belief}
experiment), which we plan to further explore.


Allen Newell's analysis:
- Knowledge level: deals with language, entailment
- Symbol level: deals with representation, inference


\subsection{Lessons learned from the introduction of a symbolic knowledge layer}

Knowledge as a observable, quantifiable, measurable, manipulable resource

Cognitive Penetrability (Zenon Pylyshyn, http://penta.ufrgs.br/edu/telelab/3/cognitip.htm)

Cognitive observability

Allows for both qualitative and quantitative analysis of the beliefs of the robot

Ease modalities merging ('look at this' -> verbal and deictic, 'give me a
banana -> fetch banana picturesfrom the Web...)

Also enables new features: natural language grounding


\subsubsection{Knowledge for interaction}

\subsubsection{Assessment of Symbolic Reasoning}

Few real-world cases of complex reasoning encountered. Yet to be discovered?

But we know for sure that thousand of trivial inferences (inheritance,
domain/range constraints, etc.) encode a large amount of common-sense.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Towards the next generation of Knowledge Representation Systems for Robotics}
\label{sect|perspectives}


Before writing down the final mark of the thesis, we would like to feed the
reflexion on the future of knowledge and knowledge representation for robots
(service/companion robots in particular, because they are the ones with the
most obvious need of symbolic knowledge for living in complex, interactive,
semantically rich environment, but this also applies to robots in general).

One of the directions that seems both critical and under-studied in our
community is what we can call \emph{context management} in a broad sense.
Managing context means at least two things: recognising contexts and
representing contexts.

Depending on what context we talk about, recognizing contexts can be relatively
easy (who is talking to me? where am I?) to difficult (what past experience
does my interactor implicitly refers to? etc.). One of the main problem we see
with context identification is that it is a fundamentally \emph{multi-scales}
system: at every point, several temporal, spatial, social, cultural context
co-exist and overlap.

This lead to the second aspect, context representation. Techniques for
representation of overlapping pools of knowledge remain to be developed, as
well as efficient algorithms to retrieve (or discard) such context-related
pools of knowledge.

The ability to explicitly manage contexts and context switches would endow the
robot with a cognitive capability similar to what is known as
\emph{context-dependent memory} in cognitive psychology. This is also related to
Tulving's \emph{autonoetic consciousness}~\cite{Tulving1985a}: the ability to
reflect upon its own past or future experiences.

From a technical standpoint, proper context management would mean a transition
from a monolithic knowledge base to an more modular architecture, with either
multiple (overlapping) models or \emph{facets} (one per agent, one per place,
one per period of time, etc.), or maybe a systematic use of reification to
attach to each \emph{atom} of knowledge (the atom is usually the statement. It
could maybe be extended to a small set of cohesive statements) one or several
contexts. The development of modal logic for practical application is also an
important direction to examine.

Much remain to be done to this regards, starting with a formal analysis of what
are the relevant contexts for our robots.

\par

Proper management of inconsistent knowledge is another point that seems of
particular interest. Inconsistencies are mostly considered today as errors
(modeling issues, perception errors, wrong interpretations of communication, etc.)
that prevent at best further reasoning, at worst the use of the knowledge base.

However, from a cognitive point of view, logical inconsistencies are a very
valuable source of knowledge by themselves. We have previously evoked the role
of cognitive dissonances as an intrinsic motivation factor for knowledge
acquisition. This can be generalised to many sources of inconsistencies:
detection of faulty perception and setup of alternative perception strategies,
start of interactions with other agents to fix a wrong model, etc.

Technical handling of inconsistencies also require to better develop
applications of techniques like default logics to robotics.

\par

The systematic study of the relations between symbolic and geometric models are
another large field open to research. While (discrete) symbolic models are
mostly seen as abstractions of a (continuous) geometric model, this link does
not need to be unidirectional. Presupposition accommodation is an example of
{\it a priori} symbolic knowledge that may alter a geometric model. Many more
of these bidirectional relations remain to be identified.

One overlooked aspect of these links between symbolic and geometric realms is
the temporal reasoning resolution: one of the challenging task for a robot
interacting with human relates to action recognition and prediction. While mid-
to longterm action recognition is a well studied field (\cite{Ghallab1996,
Johnson2005, Tenorth2011}, early and fine grained action recognition (like
gesture initiation, back-channel communication -- nodding, social gaze, ... --,
etc.) that are important for smooth interaction, requires geometric reasoning
at relatively high temporal resolution that operates within a symbolic context.
Viewed from a slightly different perspective, better temporal resolution in the
knowledge stack could lead the way to programming paradigms that are both
massively event-oriented and semantically grounded.

\par

And maybe our final point, the knowledge-oriented architectures that are being
build in many robotic research lab around the world have a very specific
characteristic compared to the efforts of other research communities working on
the question of knowledge within systems, be it humans, like in cognitive
psychology, network of computers, like in the semantic web community, or in
other systems: robots are embodied computers. They act and interact in the
physical world, and the physical world plays a key role as a communication
support. At the same time, they are computers, with unlimited access to remote
sources of knowledge via the Web, either as static database, or through
exchanges with other robots.

This dual essence, both as embodied organism and disembodied agent, places the
robot at the crossing of two fundamental approaches to knowledge management:
either physically and experientially grounded, central, internal to the agent,
or on the contrary ungrounded, distributed, pervasive. The robot is currently
the only entity that can merge and take advantage of both. The research efforts
in these directions have started, foundations have been laid (by relying on
standard ontologies to bridge the perception and the symbolic models), much
more remains to be explored.




