\chapter{Evaluation}
\label{chapt|evaluation}

The evaluation of our work is split in two main sections. First, a formal
summary and analysis of the features of knowledge representation systems that
we have presented at section~\ref{sect|surveyed-systems}. We will in particular
focus on the ORO framework.

Then, we will provide a detailed presentation of the case-studies and
experiments that have been conducted during the four years of the thesis
preparation.

These two evaluation facets will naturally lead to the conclusion at the next
chapter, where we will draw scientific and technical perspectives for knowledge
representation in robotics systems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{State of the knowledge representation in robotics}
\label{sect|evaluation-oroserver}

This section summarises the various approaches that have been surveyed in the
chapter~\ref{chapt|krs} to identify the main research trends and draw some
research perspectives from less studied fields.

The table~\ref{table|contribution-by-systems} sketches the landscape of current
research. The main fields of \emph{contribution} for the systems we have
surveyed are sorted along the different axis, dimensions of knowledge
representation that were identified in chapter~\ref{chapt|krs}.

\begin{landscape}
\begin{table}\tiny
\begin{center}
\begin{tabular}{p{0.2cm}p{3.4cm}p{1.6cm}p{1.3cm}p{1.7cm}p{1.5cm}p{2cm}p{2cm}p{1.4cm}p{1.8cm}|p{2cm}}
\toprule
\multicolumn{2}{c}{\bf Category}                                                     & ARMAR \cite{Holzapfel2008}& CAST \cite{Hawes2007}       & GSM \cite{Mavridis2006}     & {\sc Ke Jia} \cite{Chen2010}& {\sc KnowRob} \cite{Tenorth2009a}  & NKRL \cite{Sabri2011}                           & OUR-K \cite{Lim2011}          & PEIS \cite{Daoutis2009}       & ORO \cite{Lemaignan2010}                      \\
                                                                                                                                                                                                                                                                                                                                                                                                                  
\midrule                                                                                                                                                                                                                                                                                                                                                                                                          
                                                                                                                                                                                                                                                                                                                                                                                                                  
\multirow{4}{*}{\turn{90}{\bf Expressiveness}}                   & Logical formalism & TFS                       & (none)                      & (none)                      & ASP                         & Prolog                             & NKRL language (FOL + 2nd order extensions)      & DL + Horn clauses             & {\sc CycL}                    & DL (OWL)                                      \\
                                                                           & OWA/CWA &                           &                             &                             & CWA                         & CWA                                &                                                 &                               &                               & OWA                                           \\
                                                              & Modeling uncertainty &                           &                             & ++ (stochastic models)      &                             & {\it+++} ({\it ProbCog} \cite{Jain2009}) & +                                         & + (\emph{candidate} entities) &                               &                                               \\
                                                                    & Meta-cognition &                           &                             &                             &                             & ++                                 & + (transformation rules)                        &                               &                               & ++ (reification, taxonomy walking)            \\
\hline                                                                                                                                                                                                                                                                                                                                                                                                            
\multirow{7}{*}{\turn{90}{\bf Representation}}                & Space Representation &                           &                             &                             &                             &                                    &                                                 & ++                            &                               & ++ ({\em perspective-aware symbolic locations} \cite{Sisbot2011})\\
                                                               & Time representation &                           &                             & ++ (snapshots)              &                             &                                    & ++ (event oriented)                             & +                             &                               &                                               \\
                                                                    & Actions/Events &                           &                             & ++ (events classifiers)     &                             & ++ (action recognition \cite{Beetz2010a})  & +++ (everything is an event)            & +++                           &                               &                                               \\
                                                                           & Context &                           &                             &                             &                             &                                    & ++ (template matching)                          & ++                            & ++ (microtheories)            &                                               \\
                                                                          & Modality &                           &                             & ++ (recursive model)        &                             &                                    &                                                 &                               &                               & ++ (Theory of Mind \cite{Warnier2012a})       \\
                                                                     & Introspection &                           &                             &                             &                             & + (SRDF \cite{Kunze2011})          &                                                 &                               &                               &                                               \\
                                                                     & Memory models &                           &                             &                             &                             &                                    &                                                 &                               & +                             & +                                             \\
\hline                                                                                                                                                                                                                                                                                                                                                                                                            
\multirow{9}{*}{\turn{90}{\bf Reasoning}}                   & Standard FOL reasoning &                           &                             &                             & +++                         & +++                                & + (template matching)                           & +                             & ++                            & ++                                            \\
                                            & Instantiation and structure alteration & ++                        &                             &                             & ++                          & + (dynamic instantiation)          &                                                 &                               &                               & ++ (TBox alteration)                          \\
                                                                   & Lazy evaluation &                           &                             &                             &                             & +++ (Prolog, computables)          &                                                 &                               &                               &                                               \\
                                                           & Non-monotonic reasoning &                           & ++ \cite{Hawes2011}         &                             & +++ \cite{Ji2011}           &                                    &                                                 &                               &                               &                                               \\
                                                      & Presupposition accommodation &                           &                             & +++                         &                             &                                    &                                                 &                               &                               &                                               \\
                                    & Prediction, projection, diagnosis, explanation &                           &                             &                             &                             &                                    & ++                                              &                               &                               & + (explanation)                               \\
                                                                     & Task planning &                           &                             &                             & ++ \cite{Ji2011}            & +                                  &                                                 & +                             & {\it++}                       & +++ ({\em HATP})                              \\
                                                           & Physics-based reasoning &                           &                             &                             & +                           & {\it+++} ({\em naive physics \cite{Kunze2011a}}) &                                   &                               &                               &                                               \\
%                                                                         & Learning & ++                        & ++ \cite{Jacobsson2008}     &                             &                             &                                    &                                                 &                               &                               &                                               \\
\hline                                                                                                                                                                                                                                                                                                                                                                                                            
\multirow{5}{*}{\turn{90}{\bf Acquisition}}                         & Cross-modality & + (pointing gestures)     & ++                          & +++ (amodal model)          &                             &                                    &                                                 &                               & ++ ({\em ambient intelligence})& ++                                           \\
                                                                               & NLP & +++                       & +++ \cite{Kruijff2010a}     & +++                         & +++                         &                                    &                                                 &                               & + (template-based)            & +++ ({\em Dialogs} \cite{Lemaignan2011a})     \\
                                                                     & Web Resources &                           &                             &                             &                             & {\it+++} ({\em Web content processing \cite{Nyga2009}})&                             &                               &                               &                                               \\
                                                                         & Grounding &                           & ++                          & ++ (bidirectional)          &                             & +++ ({\em semantic maps \cite{Blodow2011, Klank2009}}) &                             & +++ (bottom-up)               & +++ \cite{Loutfi2008}         & ++ ({\em amodal model} \cite{Lemaignan2012})  \\
                                                              & Intrinsic Motivation &                           & ++ \cite{Hawes2011}         &                             &                             &                                    &                                                 &                               &                               &                                               \\
\hline                                                                                                                                                                                                                                                                                                                                                                                                            
\multirow{4}{*}{\turn{90}{\bf Integration}}     &   \ldots with sensori-motor layers &                           &                             & +                           &                             & +++ (computables, local geometric models) &                                          & +                             & ++ (tuple space)              &                                               \\
                                                      & \ldots with executive layers &                           & ++ (ubiquitous events)      & +                           &                             & ++ (language extensions) \cite{Beetz2010} &                                          & ++                            & ++ (tuple space)              & ++ (semantic events)                          \\
                                                          & Monitoring and debugging &                           &                             &                             &                             &                                    &                                                 &                               &                               & + ({\em remote visualisation})                \\
                                                           & Performances evaluation &                           & ++ \cite{Hawes2008}         & ++ (Token test)             & +                           & + \cite{Tenorth2011}               &                                                 &                               &                               &                                               \\

\bottomrule

%\end{tabularx}
\end{tabular}
\end{center}

\caption{Main domains of contribution of current KRS. Italics mean that the
feature is implemented as an external module. Main references are given in the
table header. When relevant, feature-specific publications have been provided.
An empty cell means that either the system has no specific focus on this domain
or we could not find relevant literature.}

\label{table|contribution-by-systems}
\end{table}
\end{landscape}

To comment this table, we propose to base the discussion on some of the
challenges that McCarthy and Roy have identified on the road to natural
interaction and, to slightly paraphrase McCarthy, ``human-level robots''.


\subsection{Logic formalisms, continuous world}

McCarthy underlines the necessity of relying on mathematical logic, as the most
fruitful formalism for machine intelligence. This is largely the case. Almost
all the systems we have surveyed rely on logical formalisms, in several cases
as a mix of declarative languages (often based on Decription Logics) and
logical programming languages (Prolog, of course, but not only).

The two exceptions (the CAST knowledge model and GSM) are however also
interesting: CAST proposes a pervasive model of knowledge that is seducing from
the grounding perspective (but likely suboptimal for deliberative tasks), and
GSM encodes knowledge in an amodal (continuous, geometric) model while
preserving features that are usually specific of symbolic models (like theory
of mind or categorical knowledge).

The interleaving of discreet symbolic models with continuous geometric models
remains a major challenge, and techniques vary a lot: besides the CAST proxies
and the GSM amodal model, most systems directly extract physical attributes of
the environment to insert them in the symbolic model ({\sc Tapas}, Ke Jia,
OUR-K, PEIS), thus skipping altogether an intermediate geometric model. {\sc KnowRob}
adopt a top-down approach where local geometric models are set up on-demand to
compute symbolic properties like relative locations.

In our approach, ORO has tight (but unidirectional) links with SPARK, the
component in charge of geometric modeling and reasoning. Similarly to the GSM
approach, SPARK is an amodal model of the environment we can manipulate (3D
motion planing, for instance takes place in MHP, the twin brother of SPARK).

\subsection{Management of uncertainty and approximation}

McCarthy also mentions the necessity to deal with \emph{approximate concepts
and approximate theories} (that includes representing them and reasoning with
them).

The most notable attempt at modeling uncertainty down to the knowledge
representation is {\sc ProgCog}, an extension of {\sc KnowRob}. The theoretical
and practical performances (including decidability) remain however difficult to
overcome.

The challenge of representing and reasoning under uncertainty has not been
tackled in ORO server, at least not \emph{within} the current ORO server
knowledge model. Two reasons explain this absence: the decisional architecture
developed at LAAS for the robots has not consistent approach to uncertainty
representation and management, neither at the \emph{symbol production} level
(\ie SPARK or {\sc Dialogs}) or at the symbol consumption level (execution
control). Thus, no strong incentive pushed us in this direction. Then, the
current tools available from the Semantic Web domain to manipulate and reason
about knowledge do not provide mature support for uncertainty management. Some
effort do exist (like the {\sc Pronto} reasoner~\cite{Klinov2008}), but this
does not appear to be a major focus in the currently available tools.

\subsection{Non-monotonic reasoning}

McCarthy gives a particular importance to non-monotonic reasoning. Systems like
Ke Jia explicitly tackle this issue.

A monotonic system does not theoretically allow for knowledge retraction during
the reasoning, which is an important issue in the robotic context where the
world model is likely to be often altered.  However it is a practical issue
only if the reasoning process has to be \emph{continuous} during the whole
robot's activity lifespan, which is rarely the case. It is often possible to
stop the reasoner, alter the knowledge, and restart the inference process on a
new domain.

Non-monotonicity can also be partially dealt with with appropriate time
representation and reasoning (the reasoner then only takes into account
statements that are set as valid for a given moment).

Finally, probabilistic reasoning also implicitly leads to non-monotonic
reasoning, by relying on a continuous description of the state of the
world.

Non-monotonicity is however of broader significance to the knowledge
representation field, in particular for the representation of common-sense
knowledge where \emph{default} representation is currently sorely lacking.

It is also related to what McCarthy calls \emph{elaboration tolerance}: the
ability to extend \emph{on demand} the closed domain of interpretation for a
given assertion, to take into account new contextual knowledge.

\subsection{Modeling of contexts}

Both Roy and McCarthy mention the importance of formalizing and reasoning about
contexts. Many of the KRS we have surveyed mention at some point the context
modeling, but no consistent interpretation, let alone theory, of context
management has clearly emerged.

Several approaches for building contextualized knowledge have been presented in
this thesis: symbolic environment interpretation, perspective taking and
independent mental state for each agent, grounded natural language resolution,
self-awareness of its own activity. Much remains to be done for a robot to
actually identify its current context as well as contexts that may be referred
to.

We will further develop this important topic in the conclusion of the thesis.

\subsection{Reasoning about time, events and actions}

While representation of spatial roles (topology, placement, with or without
perspective-taking) is well studied and KRS usually integrate with spatial
perception and reasoning components, time representation and integration in
knowledge systems is less uniform.

The two main tasks that require time reasoning are task planing and sequence
recognition (in particular, action recognition). Systems that directly
integrate task planing (like OUR-K, {\sc KnowRob}) thus have mechanisms (like
fluents in {\sc KnowRob}) to represent time.

Other approaches include storing snapshots of the knowledge state (like GSM),
that is used to move back to past mental states (of the robot or of another
agent).

Integration of these techniques with memory mechanisms (forgetting and
reinforcement learning) remains to explore.

Temporal modeling is currently absent of ORO, and symbolic and geometric
planning are accomplished outside of the main knowledge representation
layer. As a matter of fact, knowledge is mostly atemporal in the ORO server: the
set of triples stored in the server represents the beliefs of the robot
\emph{at the current time}. The robot represents knowledge about the \emph{here
and now}. Dealing with time reasoning is delegated to dedicated tools.

However, ORO does provide a simple implementation of a memory mechanism that
relies on time: when a statement is attached to a memory profile, the statement
is reified and the date of creation is stored. This allows to remove the
statement after a period of time that depends on the memory profile.

Statement reification to store timestamps of creation for all statements is
technically very easy to do in ORO server, but has not been actually enabled
because the performance hit (each reified statement produces four triples
instead of one) was not justified by any current use case of ORO server.

Regarding events, several systems (GSM, NKRL, ORO) have adopted an {\em
event-oriented} architecture where conditions (or templates) are used to
trigger decisional and execution processes.

A side note about how thematic roles and action models are initialized in ORO:
The current implementation relies on a small, predefined set of action verbs
that can be recognised from natural language
(section~\ref{processing_of_actions}).  This constraint does not come from the
resolution algorithm itself, but rather from the difficulty to automatically
extract the thematic roles associated to a verb. An interesting yet easy to
implement extension would consist in binding the dialogue processor with the
symbolic task planner. The task planner could dynamically provide the list of
actions that the robot can process, \ie actions for which the robot can produce
a plan. In certain case, it could also validate the understanding of a desire
by attempting to plan it.

Also, we could exploit on-line resources like
\textsc{VerbNet}~\cite{Kipper2008}, which provides a large machine-processable
lexicon of English verbs along with their thematic roles.


\subsection{Introspection}

the capacity of introspection,


\subsection{Grounding a multi-modal environment}

While more modalities (especially, deictic gestures and social gazes) can be
added, we have actually proposed a \emph{cross-modal representation system}.

One of the main feature of the \textsc{Dialogs} module is its ability to
interactively ground concepts through disambiguation, bringing the ability for
the robot to \emph{associate words with perceptual and action categories}.


\subsection{Common-sense}

McCarthy actually starts his list by affirming that intelligent systems must be
able to \emph{``operate successfully in the common sense informatic
situation''}. This question of the \emph{common-sense} is probably one of the
toughest because what \emph{common-sense} mean is not very clear at first place
(since, by definition, common-sense is, well, common-sense...). We feel,
however, that it is related to a diffuse cultural background, and one may even
claim that, for a system to acquire common-sense reasoning is equivalent to
solve the strong AI challenge (this is at least more or less McCarthy's opinion).

Our knowledge representation systems have a pragmatic and potentially very
powerful approach to common-sense: reuse knowledge stored on the Web. While not
so many of the systems we have surveyed directly tackle this question ({\sc
KnowRob} is the only one explicitly working on common-sense reasoning, through
physics-based reasoning, integration with Web databases or parsing of
semi-structured Web documents), several rely however on Web standards (OWL,
OpenCyc) to represent their knowledge. With the development of initiatives like
OpenMind that encode with the same standards common-sense facts and rules,
there is a strong potential for our robot to gain common-sense knowledge from
well-structured online resources in the coming years.

%%%%%%%%%% Underlying knowledge model table %%%%%%%%
\begin{table}
\begin{center}

\begin{tabular}{ll}
\toprule
{\bf Project} & {\bf Common-sense knowledge source} \\
\midrule
ARMAR/{\sc Tapas} & Custom ontology related to the kitchen\\
CAST Proxies &  None \\
GSM &  Predefined categories \\
Ke Jia & None \\
{\sc KnowRob} & {\sc OpenCyc}, processed web content, custom OWL-DL ontology, physics simulation \\
NKLR &  None \\
OUR-K & {\it A priori} knowledge structure and axioms, custom set of instances\\
ORO & {\sc OpenCyc}, custom OWL-DL ontology \\
PEIS Ecology & {\sc ResearchCyc} \\

\bottomrule

\end{tabular}
\end{center}
\caption{Underlying common-sense knowledge sources for each project.}
\label{table|knowledge-sources}
\end{table}

Table~\ref{table|knowledge-sources} lists for the systems we have surveyed the
current sources of common-sense knowledge.

\subsection{Models granularity}

\emph{Figuring out the right granularity of models} is partially solved by
supporting both a geometric reasoning level and a purely symbolic level.
Generally speaking, it appears that complex robotic systems need to operate
with a dynamic granularity, depending on the task to achieve.

\subsection{Learning, representation of experience}

Since we provide no time management, our system is currently not able to
\emph{match past (learned) experiences with the current interaction}.  This
ability is obviously a key step for general action recognition, and seems of
particular importance for the robot to assess the state of the interaction with
the human.

\subsection{Perspective-awareness} 

Finally, Roy mentions \emph{the ability to take into account the human
perspective}: this cognitive ability, that relies on the representation of
modalities, is present in ORO and GSM. ORO explicits the perspectives in
different symbolic mental states

this is probably our main contribution which we are now trying to
develop even further towards psychology-inspired experiments.


\subsection{Justification and resolution of cognitive conflicts}

Logical inconsistencies in one ORO's model (or, from a cognitive point of view,
cognitive conflicts) could be better dealt with: while (partially) provided by
the Pellet reasoner, and output to the developers for debugging purposes, the
logical justification of the inconsistency is not really exploited.


% \subsection{Performances analysis}
% 
% \subsubsection{Relaxation time}
% 
% The relaxation time (as defined at section~\ref{sect|performances}) of ORO for
% the average workloads we have encountered during the experiments is in the
% range of 100ms.  The temporal resolution that follows for the symbolic layer,
% while acceptable in most cases, is not satisfactory. In particular, multi-modal
% interaction requires fine grained temporal alignement between
% modalities~\cite{Li2012} which are not possible with the current temporal
% performances of ORO.
% 
% \subsubsection{Cognitive evaluation}
% 
% The performance evaluation of current tools is still mostly system- and
% scenario-specific (``did my system do the job for my task?''), and amongst the
% surveyed KRS, only few systems proposes an evaluation of their performances
% that could be compared to other system.
% 
% 
% \fxwarning{Mention reasoning and expressiveness, cf section oro-server}
% 


\subsection{In brief}

- binding the symbolic and geometric models
- common-sense reasoning



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Experimental evaluation}
\label{sect|experimental-evaluation}

Experimental validation of our work takes several forms that are presented in
this section.

First, it must be noted that our experiments are largely independant from the
underlying platform. Since our contributions are mainly at the symbolic
reasoning level, we rely on intermediate layers for the back and forth
conversion of sensori-motor data to symbols. These intermediate layers are
obviously much more dependant on the platform.

We present here three distinct experimental frameworks: first, we present the
MORSE simulator (section~\ref{sect|simulation}) that has been developed during
the thesis preparation with several features dedicated to human-robot
interaction simulation.

Then, several more ``traditional'' experiments on different robotic platforms
are presented (sections~\ref{sect|casestudies}, \ref{sect|expe1}
and~\ref{sect|expe2}). Most of these experiments have been conducted at
LAAS-CNRS and in other laboratories involved in the European CHRIS project
(\emph{Jido}, \emph{PR2}, \emph{ICub} and \emph{Bert} platforms).  Experiments
have also been conducted at Munich's IAS laboratory on the \emph{Rosie}
platform.

Finally, we report in section~\ref{sect|roboscopie} on the theater performance
\emph{Roboscopie} that was presented in 2011 at a large general public audience
in Toulouse.

\subsection{Simulation of HRI interaction}
\label{sect|simulation}

\subsubsection{The MORSE simulator}

The Modular OpenRobots Simulation Engine (MORSE)~\cite{Echeverria2011}
(figure~\ref{fig|morse-gui}) is a open-source tool developed for robotics
research. It is a domain independent simulator, where virtual robots can
interact with a 3D environment, using sensors and actuators that behave in the
same way as their counterparts in the real world.  MORSE relies on the advanced
3D (OpenGL shaders) and physics simulation ({\sc Bullet} physics engine)
capabilities of the Blender \emph{Game Engine}, a real-time 3D runtime
integrated to the open-source Blender modeling toolkit.  This allows for
semi-realistic simulation of complex environments.

\begin{figure}[t]
      \centering
      \includegraphics[width=0.7\columnwidth]{experiments/morse-interface.jpg}
      \caption{Screenshot of the MORSE graphical interface (inside Blender).}
      \label{fig|morse-gui}
\end{figure}


The MORSE components (sensors and actuators) exchange data with the robotics
software via middleware bindings, using a \emph{Software In The Loop} (SAIL)
architecture. Middleware supported in the current version include LAAS'
Pocolibs library, ROS and YARP, as well as a socket-based raw protocol. This
design allows in principle to use the same software in both the real robots and
the simulator. Instructions given to the robot are interpreted in the simulator
to provide the control of actuators, such as the motion of the robot and its
arms.  The data from simulated sensors is sent back through the middlewares,
{\it e.g.}~exporting the images from cameras, or the positions of the robot,
human and other objects of interest.

MORSE provides support for several classes of robots \textit{out of the box}, and
allows for easy customization of those, either by composing individual sensors
and actuators with empty robot structures directly in the MORSE interface, or
through a Python-based script language that permits to conveniently describe
robots and simulation scenarii.

Other experiments using simulation have been carried to gather data for HRI
\cite{Chernova2011}. However, these do not involve the actual robot software,
and it is another human who takes the role of the robot.

\paragraph{Contribution} While not directly linked to the main topic of this
thesis, I have been deeply involved in the design and development of MORSE: I'm
responsible for most of the original software design, and large parts of the
core foundations of the project.

\subsubsection{HRI specific features}
\label{sect|morse-hri}

\begin{figure}[t]
      \centering
      \includegraphics[width=0.7\columnwidth]{experiments/morse-kinect.jpg}
      \caption{The experimental setup with the human avatar controlled from a
      Kinect.}
      \label{fig|kinect-setup}
\end{figure}

An interactive \emph{human avatar} is available in MORSE. It provides a
first-person immersive experience: when started, one can take the role of the
human and control it via the keyboard, a WiiMote or through a Kinect device
(Fig.~\ref{fig|kinect-setup}).

When in first-person mode, the user can interact in several ways with the
environment. He/she can pick and release objects, can open drawers and
cupboards. As any other object, the avatar physically interacts (collision
detection) with the surrounding furnitures.  MORSE exports the position
and posture of the avatar as a global joint state to be used by the real
software components of the robot.

MORSE also offers a special sensor that exports abstracted informations of
objects visible to the robot (called the \emph{semantic camera}). This
sensor typically exports the name, type (glass, table, bottle, etc.), color and
location of objects. Since human-robot interaction often involves
semantic-rich environments, this abstract sensor simplifies the
experiments on such scenarii, by avoiding the added complexity of processing
camera images to detect the objects of interest and exploiting the inherent
knowledge of the simulated world.

\subsubsection{An experimental framework}

Due to its nature, MORSE offers two main advantages compared to experiments on
a real robot: light-weight deployment and repeatability. MORSE is already used
for human-robot interaction both at the LAAS-CNRS and at the Technical
University of Munich, Germany.

MORSE is integrated to the LAAS architecture.  In particular, both the human
posture and the object features are integrated with SPARK, a
module dedicated to geometric and temporal reasoning.  This module is a key
component providing a base of facts such as objects' relative placements,
visibility and reachability by the agents present in the scene. It additionally
provides a stable state of the world to the motion planners.

\subsection{Case studies}
\label{sect|casestudies}

This section reports on three small experiments conducted in the first half of
the PhD thesis preparation. Each of them illustrate one specific aspect of the
knowledge base.

The first experiment, \emph{Point \& Learn} shows how the structure
(\emph{TBox}) of the knowledge base can be altered (in this case expanded) at
runtime through pointing interaction.

The second experiment, \emph{Odd One Out} shows how the knowledge model can be
used along with the categorisation routines to isolate an ``odd" object, given
a simple context.

Lastly, the third case study is an implementation of the \emph{Spy Game} where
one of the player think of an object and the other one must guess by asking
questions.

It must be noted that these three experiments have been implemented on three
distinct robotic platforms: the \textit{BERT2} robot from the Bristol Robotics
Laboratory (a YARP-based architecture), the \textit{Rosie} robot from the
Technical University of Munich (a ROS-based architecture) and the \textit{Jido}
robot at LAAS-CNRS (based on the LAAS Pocolibs middleware).

\subsubsection{Knowledge acquisition: Point \& Learn}
\label{expe|pointandlearn}

\begin{figure}
\centering

\centering
  \includegraphics[width=0.4\columnwidth]{experiments/bristol_integration.jpg}
  \caption{Teaching the Bert robot new objects}
  \label{fig|bristol}

\end{figure}

We have implemented a \textit{Point \& learn} behaviour on the Bert
robot~\cite{Lallee2010b} (Figure~\ref{fig|bristol}): the user shows an object
to the robot, and if the robot sees it for the first time, it will ask for its
name and type.

The object perception module relies on motion capture (VICON system) to
identify and localise objects. A so-called \textit{primitive detection} module
is responsible for updating ORO with the list of objects currently seen by the
robot as well as their state (moving or not) and their relations to other
objects (touching or not). On the other end, a human-robot interface based on
the CLSU Toolkit\footnote{\url{http://cslu.cse.ogi.edu/toolkit/}} is in charge
of speech recognition, speech synthesis and basic natural language processing.

By querying ORO for moving objects, the interface retrieves the object ID that
has the focus of attention (last moving object), and asks the human for a name
and a type if the object is new. Figure~\ref{tab|transcript_bert} reproduces a
typical dialog with Bert.

At the end of this sequence, two more RDF statements are added to the robot
knowledge base: \texttt{[5001 rdfs:label "coffee-cup"]} and \texttt{[5001
rdf:type Cup]}.

\begin{figure}[!ht]
\centering
	\begin{tabular}{r|p{0.6\columnwidth}}
	\chatN{bert} & Initializing... \textit{[about 5 sec]} ...What's next? \\
	\chatN{human} & \textit{[moves an object]} \\
	\chatN{bert} & \textit{[does not know the object]} How is it called? \\
	\chatN{human} & coffee-cup \\
	\chatN{bert} & Did you say coffee-cup? \\
	\chatN{human} & yes \\
	\chatN{bert} & Ok. Now I know. What kind of object is coffee-cup? \\
	\chatN{human} & a cup \\
	\chatN{bert} & Did you say cup? \\
	\chatN{human} & yes \\
	\chatN{bert} & So coffee cup is a cup. What's next? \\
	\end{tabular}
	\caption{Transcript of a chat with the Bert robot}
	\label{tab|transcript_bert}
\end{figure}

Due to the limitation of the speech recognition software, only a predefined set
of names or types could be recognised, thus preventing the robot to add
completely original objects.

\subsubsection{Odd one out}
\label{expe|odd_one_out}

\begin{figure}
\centering

    \subfigure[]{
        \includegraphics[width=0.4\textwidth]{experiments/kimp1.jpg}
    }
    \subfigure[]{
        \includegraphics[width=0.4\textwidth]{experiments/rviz.png}
    }

    \caption{(a) Rosie, looking for objects it may know, and (b) viewed in
    RViz. The clusters of point are given an unique identifier by the
    perception that allow the supervision create the link between the physical
    objects and their symbolic representation in ORO.}

\label{fig|kimpwatching}
\end{figure}


The \emph{Odd One Out} scenario extends the \textit{Point \& Learn} experiment
and completes an on-going experiment at the IAS laboratory where a robot is
asked to list missing items on a table being set, based on probabilistic
reasoning on previously recorded observations.
%~\cite{Pangercic2009}.

We use ORO to introduce human interactions and common-sense reasoning: the
robot picks an unknown object from the table, shows it to the user, and asks
about its name and type (Figure~\ref{fig|kimpwatching}). The user continues to
describe the object (through concepts) until a concept known by the robot is
given. The learning process starts over again with another unknown object. Once
all objects are learned, the robot tells which objects do not belong to a
typical breakfast table (\textit{i.e.} objects that are neither food or
tableware). The human interacts with the robot through a dedicated XMPP bridge,
allowing to chat with the robot with a standard Jabber messaging client.
Figure~\ref{tab|transcript_kimp} corresponds to a chat session with Rosie.

The supervision (\textsc{cram}\footnote{\textsc{cram} (Cognitive Robotic
Abstract Machine) is a RPL-derived framework for rapid development of cognitive
robot control programs we currently develop.}~\cite{Beetz2010}) automatically
updates the ORO server whenever an object enters or leaves the field of view
(the perception is based on the \textsc{CoP} framework~\cite{Klank2009}).
Therefore, the integration of ORO can be seen as an extension to the robot's
belief state that not only contains abstract identifiers of the internal object
representation used in plans, but also the semantics and roles of objects in
the scenario.

\begin{figure}[!ht]
\centering
	\begin{tabular}{r|p{0.7\columnwidth}}
	\chatN{rosie} & Servus! The Bavarian robot is up and fit! \\
	\chatN{rosie} & What is the name of the WHITE CLUSTER on the TABLE? \\
	\chatN{human} & "Rosie's mug" \emph{[Rosie adds \stmt{OBJ001 rdfs:label "Rosie's mug"} stmt]}\\
	\chatN{rosie} & What kind of object is Rosie's mug? \\
	\chatN{human} & Mug \\
	\chatN{rosie} & What is a Mug? \emph{[Rosie adds \stmt{OBJ001 rdf:type Mug} but it does not know the \emph{Mug} concept]}\\
	\chatN{human} & Tableware \emph{[Rosie knows about \emph{Tableware} - belongs to the common-sense ontology. It adds \stmt{Mug rdfs:subClassOf Tableware}]}\\
	 & (...) \\
	\chatN{rosie} &  What is the name of the BLACK CLUSTER on the TABLE? \\
	\chatN{human} & my laptop \\
	\chatN{rosie} & What kind of object is my laptop? \\
	\chatN{human} & Computer \\
	\chatN{rosie} & What is a Computer? \\
	\chatN{human} & Artifact \\
	\chatN{rosie} & Do you want me to learn more objects? \\
	\chatN{human} & no \\
	\chatN{rosie} & These objects are odd: my laptop \\
	\end{tabular}

    \caption{Transcript of a Jabber session with the robot Rosie. Compared to
    dialog with Bert (\ref{tab|transcript_bert}), we see here that the robot
    anchors the new objects in its already acquired knowledge.}
	
	\label{tab|transcript_kimp}
\end{figure}

By asking in loop the human for the categories of an object until it can
connect it to a concept it already knows, the robot accurately anchors
perception in its symbolic model and it is able to reason about it. At the end
of the experiment, the robot identifies and returns the odd objects for the
breakfast table (\textit{i.e.}, in our example, objects that are neither
\texttt{Tableware} or \texttt{Food}).

An unexpected example of what the symbolic reasoning layer brings to more
traditional robotic architectures emerged during the \emph{Odd One Out}
experiment: the perception routines provided segmented blobs corresponding to
objects, along with their colours. The supervision would then feed ORO with the
visible objects. At some point, ORO suddenly refused to add an object. What
seemed at first a communication bug between modules, was actually the
consequence of a consistency check by ORO: Because of bad light conditions, the
color recognition was not very reliable, and the same object was set to have
two different colours at the same time. That was inferred as impossible by ORO
and thus discarded. This kind of logical failure can be used to improve
low-level perception results by ``closing the loop'' with high-level, symbolic
knowledge.


\subsubsection{The spy game}
\label{spygame}

This game is based on the traditional children game ``I Spy''. The idea is to
discover the object or concept one of the participants is thinking of by asking
questions such as: ``Is it green? Is it a machine? Is it on your left?'', etc.
When playing, children exploit their knowledge about the world while
categorising and describing objects through useful discriminants that allow
them to find out the answer as fast as possible while including perspective
taking abilities~\cite{Moll2006}.

\begin{figure}
\centering

    \subfigure[]{
        \includegraphics[width=0.4\textwidth]{experiments/spy-game-real.jpg}
    }
    \subfigure[]{\includegraphics[width=0.4\textwidth]{experiments/spy-game-mhp.jpg}
    }
\caption{Spy game scenario: (a) Real environment and (b) 3D environment model, viewed in \textsc{Move3D}.}
\label{fig|spyGameScenario}
\end{figure}

The scenario for this game (Figure~\ref{fig|spyGameScenario}) consists on a
face-to-face interaction where the human thinks of an object present in the
environment, while the robot queries the human until either discovering the
object or giving up, if no object was found. A categorisation example is
presented in Figure~\ref{fig|objectsSpyGame}. The game starts with the human
user giving a first hint (communication is done through a keyboard and screen),
allowing the robot to start the search filtering those objects that fulfill
this first description. Based on this subset, ORO provides a descriptor (or set
of descriptors) that allows a maximum discrimination among objects in the
subset. The robot queries the user about the value of the descriptor (or the
most discriminant among the set of descriptors) and with this new information,
the current subset of objects is filtered again. The process is repeated until
either obtaining a single object that fulfills all the descriptor values, or
failing (\textit{i.e.} no object found). 

\begin{figure}[!h]
\centering
\begin{scriptsize}
\begin{tikzpicture}[scale=1.2] %[level/.style={sibling distance=30mm/#1},scale=0.8]
	%[edge from parent fork down,
	%every node/.style={fill=black!30,rounded corners},
	%[parent anchor=east,child anchor=west,grow=east,
	%edge from parent/.style={thick,draw}]
	\node {Artifact}
	child {node {Tableware}
		child {node {Bottle}}
		child {node {Cup}}
		}
	child {node {GameObject}}
	child {node {Furniture}
			child {node {Table}}
			child {node {Chair}}
			child {node {Shelf}}};
\end{tikzpicture}
\end{scriptsize}
\caption{Example of object categorisation used in the scenario.}
\label{fig|objectsSpyGame}			
\end{figure}

%Thing
%Object
%Tableware
%	- bottle: red, blue, yellow orange
%	- cup: white
%Furniture
%	- table: hrp2table, lowtable
%	- chair: chair1 and chair2
%	- shelf: pink_shelf
%GameObject: spacenavbox, orangebox, accesskit

We have integrated the game in the LAAS-CNRS Jido robot~\cite{Alami1998a}.
Objects are identified through a tag-based vision approach\footnote{ARToolKit:
\url{http://www.hitl.washington.edu/artoolkit/}} and we use motion capture for
human tracking. Their descriptions regarding categories (type of object) and
features (color, shape) are manually given in advance. Spatial relationships
({\tt front}, {\tt back}, {\tt left}, etc, and {\tt in}, {\tt on} and {\tt next
to}) and visibility (only visible objects for both agents can be considered in
the game) are automatically computed on-line by the \textsc{MHP/Move3D}
geometric reasoner and planner~\cite{Marin2008}.
Figure~\ref{fig|spyGameExample} shows an example of a round game.


\begin{figure}
\centering
	\begin{tabular}{r|p{0.7\columnwidth}}
		\chatN{human} & It is a tableware.\\
		\chatN{jido} & \emph{[retrieves possible objects: blue-bottle, yellow-bottle, orange-bottle, cup-with-handle]} \\
	 			& \emph{[keeps visible objects: blue-bottle, yellow-bottle, cup-with-handle]}\\
				& \emph{[obtains discriminants: type, color.]}\\
				& Which type of object is: bottle or cup? \\
		\chatN{human} & Bottle.\\
		\chatN{jido} & \emph{[obtains possible objects: blue-bottle, yellow-bottle.]}\\
				& \emph{[obtains discriminants: color.]}\\
				& What color the object is: blue or yellow?\\
		\chatN{human} & Blue.\\
		\chatN{jido} & \emph{[obtains possible objects: blue-bottle.]}\\
				& The object is the blue-bottle!	
	\end{tabular}\\
	\caption{Example of the robot playing Spy game.}
	\label{fig|spyGameExample}
\end{figure}

\subsection{Interaction experiments}

Three larger experiments have been also conducted. The first two are focused on
knowledge representation and verbal interaction: the ``Moving to London''
scenario where the human asks for help to find and pack objects, and the
``Aperitif Time'' scenario (that will be only briefly presented) where the
robot tries to prepare a tray with the drinks the human desires. In these
experiments, robot actions are limited to simple motions (like head tracking or
predefined pick-and-place).

The third experiment, prepared with Mathieu Warnier and Julien Guitton,
involves the SHARY execution controller and the HATP symbolic task planner
besides ORO and {\sc Dialogs}. In this scenario, the human and the robot try to
cooperatively remove objects from a table.

\subsubsection{First interaction experiment: ``Moving to London'' scenario}
\label{sect|expe1}

This first experiment is based on the following daily life situation: Tom and
Jerry are moving to London, so they are packing things in boxes. The scenario
takes places in the living-room, where Jido (our robot) is observing while they
move things here and there. To assess the reasoning abilities of the robot they
ask Jido for information (entered through keyboard). Ideally, the robot should
also perform actions when required (e.g. hand an object when asking ``give
me...''). However, since it is out of the scope of this work, we do not include
any motion from the robot's side.

Perception of objects is done through a tag-based system and humans are
detected through motion capture. The robot knowledge base is pre-loaded with
the \emph{ORO Commonsense Ontology}.  We next describe in detail two
situations where we can follow the internal robot's reasoning and the
interaction with the users.

\paragraph{Implicit disambiguation through visual perspective taking}

Tom enters the room while carrying a big box (Figure~\ref{fig|vpt}, page 1). He
approaches the table and asks Jido to handle him the video tape: ``Jido, can
you give me the video tape''. The \textsc{Dialogs} module queries the ontology to
identify the object the human is referring to: \stmt{?obj type VideoTape}. 

There are two video tapes in the scene: one on the table, and another one
inside the cardboard box. Thus, the knowledge base returns both: $\Rightarrow$
\concept{?obj = [videoTape1, videoTape2]}. 

However, only one is visible for Tom (the one on the
table). Thus, although there is an ambiguity from the robot's perspective
(since it can see both video tapes), based on the perspective of its human
partner it infers that Tom is referring to the video tape on the table, and not
the one inside the box which is not visible from his view. Therefore,
non-visible objects are removed obtaining: \concept{?obj =[videoTape1]}.

Since only one object is available, the robot infers
that the human refers to it and would eventually execute the command, \ie give
it to the human. Alternatively, the robot could first verify with the human if
that was the object being referred to or not before proceeding to execute the
action. Table~\ref{table|ptbeliefs} lists the robot's beliefs about itself and
its human partner involved in this situation.

\begin{table}
\begin{center}
\begin{tabular}{l}
\hline
Robot's beliefs about itself (\emph{robot's model}):\\
\hline
  \hspace{0.7cm}\stmt{videoTape1 type VideoTape}\\
  \hspace{0.7cm}\stmt{videoTape1 isOn table}\\
  \hspace{0.7cm}\stmt{videoTape1 isVisible true}\\
  \hspace{0.7cm}\stmt{videoTape2 type VideoTape}\\
  \hspace{0.7cm}\stmt{videoTape2 isIn cardBoardBox}\\
  \hspace{0.7cm}\stmt{videoTape2 isVisible true}\\
\hline
\hline
Robot's beliefs about Tom (\emph{Tom's model}):\\
\hline
  \hspace{0.7cm}\stmt{videoTape1 type VideoTape}\\
  \hspace{0.7cm}\stmt{videoTape1 isOn table}\\
  \hspace{0.7cm}\stmt{videoTape1 isVisible true}\\
  \hspace{0.7cm}\stmt{videoTape2 type VideoTape}\\
  \hspace{0.7cm}\stmt{videoTape2 isIn cardBoardBox}\\
  \hspace{0.7cm}\stmt{videoTape2 isVisible false}\\
 \hline
\end{tabular}
\end{center}
\caption{Robot's beliefs about itself and its human partner.}
\label{table|ptbeliefs}
\end{table}

\paragraph{Explicit disambiguation through verbal interaction and gestures}

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.9\linewidth]{images/dialogs/inTheBox2.jpg}
\caption{Jerry asks Jido for the content of the box by pointing at it.}
  \label{fig|pointing}
\end{figure}

In this situation, Jerry enters the
living room without knowing where Tom had placed the video tapes. So he first
asks Jido: ``What's in the box?''. Before the robot can answer the question it
has to figure out which box Jerry is talking about. Similar to the previous
situation, there are two available boxes: 

\begin{center}
\begin{tabular}{l}
\stmt{?obj type box}\\
\hspace{0.7cm}$\Rightarrow$ {\tt ?obj = [cardBoardBox, toolbox]}
\end{tabular}
\end{center}

However both are visible and the cognitive ambiguity resolution cannot be
applied. The only option is to ask Jerry which box he is referring to: ``Which
box, the toolbox or the cardboard box?'' Jerry could now simply answer the
question. Instead, he decides to point at it while indicating: ``This box''
(Figure~\ref{fig|pointing}). The robot's perception identifies the {\tt
cardBoardBox} as being pointed at and looked at by the human and updates the
ontology with this new information using a rule available in the commonsense
ontology ({\tt \textbf{pointsAt}(?ag, ?obj) $\land$ \textbf{looksAt}(?ag, ?obj) $\to$
\textbf{focusesOn}(?ag, ?obj)}) The \textsc{Dialogs} module is then able to merge both
sources of information, verbal (``this'') and gestural to distinguish the box
Jerry refers to.

\begin{center}
\begin{tabular}{l}
\stmt{Jerry pointsAt carboardBox}\\
\stmt{Jerry looksAt carboardBox}\\
$\to$ \stmt{Jerry focusesAt carboardBox}\\
\hspace{0.7cm}$\Rightarrow$ {\tt ?obj = [cardBoardBox]}
\end{tabular}
\end{center}

Finally, the \textsc{Dialogs} queries the ontology about the content of the box
and the question can be answered: ``Jido-E''. Note that the object's label is
used instead of its ID. This way we enhance interaction using familiar names
given by the users.

\begin{center}
\begin{tabular}{l}
\stmt{?obj isIn cardBoardBox}\\
\hspace{0.7cm}$\Rightarrow$ \concept{?obj = videoTape2}\\
\end{tabular}
\end{center}

At this point Jerry wants to know where the other tape is, and that is exactly
what he asks Jido: ``And where is the other tape?''. In this occasion, the
\textsc{Dialogs} module is able to interpret that Jerry is not referring to the
video which they were just talking about, but to the other one:

\begin{center}
\begin{tabular}{l}
\stmt{?obj type VideoTape}\\
\stmt{?obj differentFrom videoTape2}\\
\hspace{0.7cm}$\Rightarrow$ \concept{?obj = [videoTape1]}
\end{tabular}
\end{center}

Since there is only one possible ``other'' video (there are only two videos in
the scene), it can directly answer Jerry: ``The other tape is on the table and
next to the toolbox.''

\begin{center}
\begin{tabular}{l}
\stmt{videoTape1 isOn table}\\
\stmt{videoTape1 isNextTo toolbox}
\end{tabular}
\end{center}


\subsubsection{Second interaction experiment: ``Aperitif Time''}
\label{sect|expe2}

A similar experiment has been conducted in the appartment environment
(figure~\ref{fig|livingroom}) of the LAAS-CNRS with the PR2 robot.

\begin{figure}
    \centering
    \includegraphics[width=0.7\columnwidth]{experiments/adream-livingroom.png}

    \caption{The ``Living room'' setup, as represented by the robot during the
    experiment.}

    \label{fig|livingroom}
\end{figure}

This second experiment is focused on verbal interaction, and the perception is
limited to the human tracking with a deported ASUS XTion (similar to the
Kinect) sensor. Contrary to the first experiment, objects in the environment
are not detected. Their positions are predefined. Pick\&Place manipulation
tasks are not planned either, and the robot gestures are predefined as well.

Dialogue with the robot goes through a custom Android application running on a
touchpad held by the human. This application relies on the Google Speech API
for text-to-speech recognition, and communicates exchange messages with the
robot via the XMPP/Jabber protocol.


\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{experiments/aperitiftime.jpg}
    \caption{This experiment led to a video that can be viewed online:
\url{http://www.youtube.com/watch?v=pLz8ifvtoeQ}.}
    \label{fig|aperitif-video}
\end{figure}

The experiment consists in manipulation of drinks (soda cans, wine minibottles)
by verbal commands that take into account the human perspective, and may
require discrimination (for instance, ``Give me the juice'' can refer either to
the orange juice or to the apple juice).

The reasoning and decisional principles are similar to the previous experiment,
and are not detailed here. This experiment can be considered as a more mature
version of the first one: more objects are involved, speech understanding is
vastly improved, the decisional layer is more developed (we use the {\tt pyRobots}
environment, briefly presented at section~\ref{sect|pyrobots}, to react
to incoming orders), the robot acts (pick \& place, tracking of the human with
the head, etc.).

\paragraph{Evaluation} The results of these two first experiments is however
not fully satisfactory.

\begin{itemize}

    \item the complexity of the system makes it error prone. Besides, lack of
    sufficient decoupling between the higher-level components can lead to long
    restart procedures. In particular, the ORO knowledge base has stability
    issues (reasoner crashes) that greatly depend on the ontology content in
    ways that are difficult to predict. As a consequence, these experiments are
    difficult to set-up and reproduce.

    \item in the first experiment, the tag-based tracking of object is fragile
    (sensible to partial occlusions, lighting conditions, etc.), while in the
    second experiment, the absence of tracking of objects (due in part to the
    size of cans, too small to stick a tag, in part to the desire to avoid
    altogether perception issues in the system, and in part to the location of
    objects, sometimes only partially visible, like the can on the bottom of
    the picture~\ref{fig|aperitif-video}) leads obviously to poor robustness of
    manipulation tasks.

    \item the high-level behaviour of the robot is also simple (it waits for a
    spoken goal, executes it, and start waiting again). It was however not the
    focus of these experiments, and the integration with better high-level
    control in presented in the next experiment.

\end{itemize}

It must be also noted that the second experiment has been originally designed
as a user-study: a naive, non-expert user was given a list of drinks to ask the
robot to prepare for the aperitif. The idea was to compare the time required
for the task achievement with several conditions, including use or not of
perspective taking. We gave up with this project because of the lack of good
enough resilience of the dialogue system to non-expert inputs. For instance
ill-formed English sentences or elliptic sentences are not dealt correctly
with. A pre-study that was meant to gather example of verbal inputs and
involving about fifty users has been however conducted. It provides a valuable
database to exercise and further develop the dialogue management system.

\subsubsection{Third interaction experiment: ``Cleaning the Table''}
\label{sect|expe3}

The third experiment involved a more complex decisional layer where the ORO
server was used in conjunction with the HATP symbolic task planner and the
SHARY execution controller (the complete software architecture we deployed for
this experiment is pictured on figure~\ref{fig|archi},
page~\pageref{fig|archi}).

In this scenario (figure~\ref{fig|cleantable-video}), a human and a robot
cooperate to remove objects from a table. The robot produces symbolic plans for
both itself and the human (\cite{Alami2011}, see also section~\ref{sect|hatp})
that allow the robot to (verbally) share the task with the human (like ``I take
the green box and I put it in the trashbin, you take the black video tape and
you throw it in the trashbin''). Plans are created based on perceived
visibility and reachability of the objects, and the robot also monitors the
human activities to track the advancement of the whole plan (this last aspect
is presented in~\cite{Warnier2012}).

\begin{figure}
    \centering
    \includegraphics[width=0.7\columnwidth]{experiments/cleantable.jpg}

    \caption{Snapshot of the film of the ``Clean the Table'' scenario. The
    physical situation, the SPARK model, and the current step of the plan can
    be seen.}

    \label{fig|cleantable-video}
\end{figure}


Figure~\ref{fig|cleantable-timeline} presents a excerpt of the whole task and
shows how the different components produce and use symbolic knowledge (note
that the plan is actually computed {\it a priori}. It appears nevertheless on
this diagram for better readability).

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{experiments/manip_run.pdf}

    \caption{This diagram shows a simplified version of the ``Clean the Table'' scenario timeline.}

    \label{fig|cleantable-timeline}
\end{figure}


This experiment also led to a video that can be viewed online:
\url{http://www.youtube.com/watch?v=IODx50uV_k4}.


\subsection{The Roboscopie performance}
\label{sect|roboscopie}

The last experimental setup we have been working on is less usual, since it is
a theatre performance, involving one human actor and the PR2 robot.

Theatre with robotic actors is an emerging field, with a few previous published
results~\cite{Breazeal2003, Lin2009, Mavridis2009}.

On the 14th of October 2011, we performed for a general public audience (over
300 persons) a 18 min long live theatre play, acted by professional actor
Xavier Brossard and the LAAS/CNRS PR2 robot. The play was created and directed
by Nicolas Darrot, a mixed-media artist from Paris.

The PR2 was programmed in a 2-months course, re-using several software
components presented in this thesis, including the 3D environment for situation
assessment SPARK, the knowledge base ORO and the natural-language
processor {\sc Dialogs}.

This section presents the storyline of the play, gives details on the technical
side of the project, and underlines some of the significant outcome from the
human-robot interaction perspective.

Both a short teaser and the full-length version of the performance are
available from a dedicated website, \url{www.laas.fr/roboscopie}.

\begin{figure}
    \centering
    \includegraphics[width=0.5\columnwidth]{experiments/roboscopie.jpg}
    \caption{The PR2 robot at the beginning of the performance.}
    \label{fig|pr2-opens-curtains}
\end{figure}


\subsubsection{Storyline}

The play discusses how humans and robots can find a common ground for understanding
each other, by living in a kind of frontier world, where real objects are replaced 
by abstract, disembodied counterparts.

Xavier and PR2 share a white, almost empty, stage. To get the robot to see his
world, Xavier must keep being recognised by the human tracking module that lies
on the wall, and must stick everywhere 2D barcodes, instead of real
objects. The robot can read and identify these barcodes, and while the stage
gets covered by the tags, the robot constructs for itself a 3D world with the
right-looking objects: a phone, a lamp, a hanger...

While Xavier is drawing by hand more and more of these 2D tags, the robot tries
to offer its help. It brings first a bottle of water, then a fan... which blows
away all Xavier's code. Angry, Xavier leaves, and PR2 remains alone.

The night comes, and the robot decides to explore the stage, scattered with
those barcodes on the ground. On the next morning, Xavier discovers that the
robot's 3D model is a mess, full of random objects: an elephant, a boat, a
van... Xavier resets the robot model and starts to tidy up the place. The robot
decides to help with a trash bin, but suddenly gives up and a new program
starts: a home-training session. Xavier starts the exercises, but as the
program goes along, the robot looks more and more menacing, up to the point
that Xavier shouts ``Stop!''.

Xavier eventually shows one after the other the objects to the robot,
explaining they are all fake, and like the robot, we realize that everything
was just an experiment.

\subsubsection{Technical overview}

The PR2 robot was running softwares developed at the LAAS/CNRS. While the
performance tries to picture some of the challenges in the human-robot
interaction field, including the needed autonomy of a robot working with
humans, the robot was partially pre-programmed for this theatre performance.

Most of the behaviours were coded in Python, relying both on the PR2 ROS
middleware and on {\sc GenoM}, the LAAS own middleware.

\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{experiments/roboscopie-spark.jpg}
    \caption{The robot build a coherent 3D model of its environment through the
    SPARK module}
    \label{fig|spark-roboscopie}
\end{figure}

\paragraph{What was pre-programmed?}

\begin{itemize}
    \item{\bf General behaviour} While the real perception routines were running
(see next section), the robot did not have any mean of synchronization with the
human during the play: each sequence was manually started by one of the
engineers.

    \item{\bf Predefined positions} Places on the stage were hard-coded: for instance, the position of the table
was known to the robot from the beginning, so was the position of the entrance
door, etc.

    \item{\bf Postures and manipulation tasks} Manipulation tasks (like grasping
the fan or the paper bin) were much simplified: the robot would simply open its
gripper, and wait for \emph{something} to be detected in its hand. It would
then simply close the gripper. Likewise, the robot special postures to enter or
leave the stage with an object in hand (required to avoid collision with the
door) were all pre-defined.

    \item{\bf Speech Understanding} At the end of the play, when Xavier talks to
the robot (\emph{Stop!}, \emph{Look at this phone!}, \emph{Everything is fake},
etc.), sentences were manually typed in the system. We could have used speech
recognition as we do in the laboratory, but converting speech to its textual
version is relatively slow and error prone. So we decided to avoid it on the
stage.

While what Xavier said was actually processed by the robot (see next section),
the actions that followed (like looking at the phone, turning the head back to
the audience,...) were manually triggered.

\end{itemize}

\paragraph{What was autonomously managed by the robot?}

\begin{itemize}
    \item{\bf Navigation} All navigation tasks were computed \emph{live} by
PR2, using the ROS navigation stack. The main script just tells the robot to go
from the engineer desk to the center of the stage for instance. The robot would
then find a path that avoid obstacle.  

    \item{\bf Modeling of the environment} The 3D world that is displayed above the stage
during the show (figure~\ref{fig|spark-roboscopie}) is a live capture of the
Move3D and SPARK softwares. These softwares are used daily on the robot to
compute trajectories, symbolic locations, visibility and reachability of
objects, etc.

However, during the performance, we deactivated the computation of symbolic
facts (like {\tt xavier looksAt jacket}, {\tt RED\_PHONE isOn table},...) which
is not reliable enough to be used on the stage.

The 2D barcodes are actually a key perception mechanism for our PR2. They are
well identified {\sc ARToolkit} tags used to identify and localise (both for the
position and the orientation) objects surrounding the robot.

Besides, the robot was able to track the human whole-body posture with a
Microsoft Kinect sensor and the {\sc OpenNI} human tracker. In several
occasion, the robot automatically tracks the human head or the human hands with
this system.

    \item{\bf Speech Understanding} At the end, Xavier talks to the robot. The
textual version of what he said was fed to the system \emph{as it}. Natural
language understanding is done by the {\tt Dialogs}
module and used extensively the {\tt oro-server} knowledge
database to make sense of the word in the current context. The result of the
language processing was then added back to the knowledge base and automatically
displayed by the {\tt oro-view} OpenGL ontologies viewer.

Hence, the sentence \emph{``look at this phone''} get translated into symbolic
facts: {\tt [human desires action1, action1 type Look, action1 receivedBy
RED\_PHONE]}. The robot is able to know that <this phone> is indeed the {\tt
RED\_PHONE} by taking into account what the human focuses on.

Since the computation of symbolic facts was deactivated, we had to manually add
several symbolic facts in a so-called scenario-specific ontology.

\end{itemize}

\subsubsection{Significance for HRI}

A first noteworthy achievement of this project from the human-robot
interaction point of view is the use and display of the set of research tools
developed at LAAS in front of a general audience: while the show had been
precisely scripted and rehearsed, the robot was running the same software
components we use on a daily basis in the laboratory.

By building the performance storyline on the current, actual state of robotic
research,  the play also put light on three key questions of today's
human-robot interaction: how the human and the robot can understand each others
(the robot tries to help but remains intrusive)? how to share and coexist in a
common living space? how roles build up between the human and the robot (who
dominates)?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\recap

This chapter was focused on the evaluation, from two different perspectives.

First, we have presented a large table that summarises how the main features
and requirements of knowledge representation systems identified at
chapter~\ref{chapt|krs} are matched by the existing implementations. We have
discussed the successes and limits of the current state of the art based on a
list of high-level requirements established from McCarthy and Roy proposals
towards more advanced artificial cognition.

Amongst the fields that are identified as requiring more research efforts,
non-monotonic representations, explaining of the mental state and the mental
processes, and context representation represent largely open challenges.

Then, we have presented several experimental frameworks and experiments.

MORSE, a simulator with strong HRI capabilities has been briefly presented.
Three case-studies demonstrating the classification and learning capabilities
of ORO have been then introduced. Three larger experiments, involving verbal
interaction, task planning and execution control have been then presented and
explained.

Finally, we have mentioned the Roboscopie theatre performance as an unusual
experimental setup, targeted at a general public audience. We see it as a way
to materialize and discuss the relationships between the robots and the humans,
and to address these questions ``in the open'', outside of the closed frame of
the laboratory.



