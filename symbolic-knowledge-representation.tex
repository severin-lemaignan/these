\chapter{Symbolic knowledge representation}

\fxnote{Support material: \emph{What is a knowledge representation} by Davis,
Shrobe and Szolovits,
\url{http://groups.csail.mit.edu/medg/ftp/psz/k-rep.html}}

\section{What do we call "knowledge"?}
\label{sect|on-knowledge}


Since we will discuss at length the concept of knowledge in the context
of robotics in the coming pages, it is useful to make our terminology explicit.  No general
agreement on a definition of ``knowledge'' exists. In our context, we call
``knowledge'' \emph{an explicit set of interrelated logical facts that are
meaningful to the robot executive controller} (by \emph{meaningful} we mean
that can possibly be interpreted to lead to a purposeful action).

The relation of \emph{data} and \emph{information} to knowledge is a debated
epistemology question (the interested reader is invited to refer to the
Wikipedia page on the ``DIKW'' hierarchy). In this thesis, we will associate
data to low-level material like raw sensor output, and information to
uncontextualized symbolic facts.

To give a example, image a human watching at a book and being tracked by a
Kinect sensor: the pose of the human skeleton in the world would be the data,
the fact \concept{looksAt(human, book)} as computed by a geometric reasoning
module would be the information, the fact \concept{looksAt(john,
war\_and\_peace)}, fully grounded and connected to the whole knowledge base of
the robot would be proper knowledge.

Note that the origins of AI where in purely symbolic models, that were
not usable either. Rich interleaving between geometric reasoning and symbolic
models is required.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Typology of Knowledge Representation Requirements for Robotics}
\label{sect|features}

We now want to organize requirements from this imaginary cooking scenario where
numerous desirable features for a knowledge representation system for robotics
have been identified without any particular order.

This section proposes a more formal typology of desirable features for a such a
system. For each feature, we provide a short definition along with links to
relevant literature.

Table~\ref{table|contribution-by-systems}, at the end of the article,
summarizes all these features with the main domain of contribution of each
surveyed KRS.

As previously mentioned, we propose six main categories.

\begin{scriptsize}
\begin{center}
\begin{tikzpicture}[grow=right, level distance = 3cm, level 1/.style={sibling distance=5mm}]
    %[edge from parent fork east]
    %every node/.style={fill=black!30,rounded corners},
    %[parent anchor=east,child anchor=west,grow=east,
    %edge from parent/.style={thick,draw}]
    \node {Dimensions}
        child {node {Instantiation}}
        child {node {Integration}}
        child {node {Acquisition}}
        child {node {Reasoning}}
        child {node {Representation}}
        child {node {Expressiveness}};
\end{tikzpicture}
\end{center}
\end{scriptsize}

\subsection{Expressiveness: What Can be Represented?}
\label{sect|expressiveness}

\begin{scriptsize}
\begin{center}
\begin{tikzpicture}[grow=right, sibling distance=5mm, level distance = 3.5cm]
    \node {\bf Expressiveness}
            child {node {Meta-cognition}}
            child {node {Uncertainty}}
            child {node {Open/Closed world assumption}}
            child {node {Expressive power}}
            child {node {Logic formalism}};
\end{tikzpicture}
\end{center}
\end{scriptsize}


\subsubsection{Main Logic Formalisms}

The main role of a knowledge representation system is to provide an adequate
representation system to store facts and concepts that can be informally
described in natural language.

Formal logic aims at providing such a representation system with the added
value of providing a tractable support for inference and reasoning.

Most (but not all) of the systems we survey rely on a particular logic
formalism. The choice of the formalism has a strong impact, on one side, on the
range of ideas that can be expressed conveniently (\emph{practical
expressiveness}) or at all (\emph{theoretical expressiveness}), on the other
side, on the ability to solve the inference problem (called
\emph{satisfiability}: is a given logical sentence true in my model?) in a
tractable manner.

A large number of logic formalism do exist, we shall summarize below the most
relevant ones for systems actually deployed in current robotic architectures.

\emph{Predicate logic} is the family of logic formalisms the most commonly
found in knowledge representation. It distinguishes itself from the simpler
\emph{propositional logic} by the use of quantification to increase generality.
\emph{First-order logic} (FOL) is the subpart of \emph{predicate logic} where the
objects of \emph{predicates} (or \emph{formulae}) are simple \emph{terms},
while in \emph{higher-order logics}, predicates can be themselves objects of
other predicates.

\emph{Horn clauses} are an important subset of FOL because the satisfiability
of a set of such clauses is a $P$-complete problem (\ie practically tractable).
A Horn clause is a disjunction of literals (a \emph{clause}) with at most one
positive literal: $\neg p \lor \neg q \lor \cdots \lor \neg t \lor u$, which
can also be represented as $(p \land q \land \cdots \land t) \rightarrow u$.
Important logic programming languages like Prolog are based on Horn clauses.

The family of \emph{Description Logics}~\cite{Baader2008} also play an
important role. It is also a subset of the first-order logic, with some
extensions in second-order logic. Description logics are notable because most
of them are known to be decidable (but not always in a practically tractable
manner). In description logic, axioms are build from \emph{concepts},
\emph{roles} (that are unary or binary predicates) and \emph{individuals}. The
W3C OWL-DL standard is a widely-used language to describe domains with the
description logic.

Because Description Logics have been originally created from the perspective of
a \emph{knowledge representation language} and not a logic language, their
terminology (\emph{concept} or \emph{class}, \emph{role} or \emph{property},
\emph{individual},\ldots) is well-suited to knowledge description and we may use
it in the remaining of this paper outside of the strict context of  Description
Logics.

\emph{Modal logic}, that allow for statement qualification like
\emph{possibility} or \emph{necessity}, have been shown to be closely related
to description logics~\cite{Baader2001}. Modal logic allows to represent conveniently parallel
possible worlds and facts like ``the robot knows \emph{that the human knows}
how to read a recipe''.

\fxfatal{On modal logics, see the remark of McCarthy, in \cite{McCarthy2007}, section 3}

\emph{Temporal logic} are designed to represent and manipulate assertions whose
truth value may vary in time.

One last class of logics that is of particular relevance for robotic
applications is the \emph{probabilistic logics} or \emph{Bayesian logics}.
These logics provide a formal framework to reason on propositions whose truth
or falsity is uncertain. We elaborate below on the representation of uncertainty.

Note that most of these logic formalisms are still active research field on
their own, and practical considerations (especially the availability of
reasoners efficient enough for on-line use on a robot) often constraint the
choice of a logical formalism and a level of expressive power.

\subsubsection{Expressive Power}

Logical formalisms each bring a certain level of expressive power. For
instance, the following classical syllogism can not be represented in
propositional logic because of the use of \emph{universal quantification}:

\begin{quote}
\begin{enumerate}
    \item All men are mortal,
    \item Socrates is a man,
    \item Therefore, Socrates is mortal
\end{enumerate}
\end{quote}

However, the following weak version of the syllogism can be represented in
propositional logic:

\begin{quote}
\begin{enumerate}
    \item If Socrates is a man, then Socrates is mortal,
    \item Socrates is a man,
    \item Therefore, Socrates is mortal
\end{enumerate}
\end{quote}

Generally speaking, expressive power comes at the cost of more complex
\emph{satisfiability} and \emph{consistency}\footnote{We precise these concepts
at section~\ref{sect|reasoning}.} computations, possibly leading to
untractable, if not undecidable (\ie systems where it is proven that a
proposition can not be decided to be true or false) problems.

\begin{figure}
    \centering
    \includegraphics[width=0.6\columnwidth]{typology/expressive_overlap_dl_horn.pdf}
    \caption{expressiveness overlap of Description Logics and logic programs
    based on Horn clauses, taken from~\cite{Grosof2003}}
    \label{fig|overlap_dl_horn}
\end{figure}

Figure~\ref{fig|overlap_dl_horn} shows that the expressive power of description
logics and Horn clauses partially overlaps. In section~\ref{sect|reasoning} we
mention extensions to description logics based on rule systems to bring closer
the two approaches.

The relationships between expressive power and reasoning complexity that follow
has been extensively studied for Description Logics.
Zolin~\cite{ZolinDLComplexityNavigator} maintains a ``complexity navigator''
that allows to conveniently explore these relationships and indexes most of the
literature on that subject.

\subsubsection{Open World and Close World Assumptions}

The \emph{close world} (CWA) vs. \emph{open world} (OWA) assumption names a
modelling choice on the \emph{completeness} of a knowledge domain. In the close
world assumption, a proposition that can not be proven true is assumed to be
false (\emph{negation by failure}), while in the open world assumption, a
proposition may be considered either true, false or unknown.

This distinction is important in robotics were the robot may have to manipulate
concepts with only partial knowledge on them. For instance, let imagine a robot
that sees a bottle on a table, whose bottom is hidden by another object. The
robot can not prove that the bottle is indeed \emph{on} the table. A knowledge
representation system relying on the closed world assumption would then assume
the bottle is \emph{not} on the table ($\lnot R^{CWA}_{isOn}(bottle, table)$)
whereas with the open world assumption, the proposition $R^{OWA}_{isOn}(bottle,
table)$ would be undecided. Example in table~\ref{table|cwa-owa-example} provides
another example of consequences of the CWA/OWA choice on reasoning.

\begin{table}
    \begin{center}
    \begin{tabular}{ll}
    {\bf Action} & {\bf Part involved} \\
    \hline
    {\tt PickSoftly} & hand \\
    {\tt PickAndPlace} & arm, hand \\
    {\tt MoveArm} & arm \\
    \hline
    \end{tabular}
    \end{center}
    \caption{Assuming the question is: \emph{select actions that do not require
    to move the arm}, a CWA reasoner would return {\tt PickSoftly} whereas an
    OWA reasoner would not return anything if the {\tt PickSoftly} action is
    not explicitly said not to involve the arm.}
    \label{table|cwa-owa-example}
\end{table}

The OWL language is specifically known to assume an open world.  Domains
constrained with the closed world assumption lead to more tractable inference
problems, and allow for instance the use of logic languages like Prolog. Thus,
several approaches exists to \emph{locally close} a domain (\cf
Levesque~\cite{Levesque2008}, section 24.3.2 for a summary of those).

\subsubsection{Representation of uncertainty and likelihood}

Sources of uncertainty for a robot are two-fold: uncertainty \emph{intrinsic}
to facts (like \emph{``It may rain tomorrow''}), uncertainty caused by
imperfect perception of the world (\emph{``Is the bottle really on the
table?''}). Most logics do not account explicitly for uncertainty. It must be
either relied on specific logics (like Bayesian logics) or on extensions of
classical logics.

\subsubsection{Meta-cognition: knowledge on the knowledge}

As stated by Cox and Raja~\cite{Cox2007}, meta-cognition is composed of both
\emph{``meta-level control of cognitive activities and the introspective
monitoring of such activities to evaluate and to explain them"}.

A knowledge representation system endowed with \emph{meta-cognition} is not
only able to manipulate knowledge but also to exhibit and manipulate the
structure of its knowledge and the reasoning process. For instance, the ability
to explain a logical inconsistency in a KRS is a meta-cognitive function.
Sloman~\cite{Sloman2011} \fxfatal{...Sloman?}

At section~\ref{sect|introspection} below, we discuss the idea of
introspection.  Meta-cognition can be viewed as the technical facet of the
introspection in general.


%%%%%%%%%%
\subsection{How things are represented?}
\label{sect|higher-level-domain-representation}

\begin{scriptsize}
\begin{center}
\begin{tikzpicture}[grow=right, sibling distance=5mm, level distance = 3cm]
    \node {\bf Representation}
            child {node {Memory}}
            child {node {Introspection}}
            child {node {Possible Worlds}}
            child {node {Context}}
            child {node {Roles}};
\end{tikzpicture}
\end{center}
\end{scriptsize}


\subsubsection{Role Representations}

Thematic roles

Spatio-Temporal Representations:

\paragraph{Representation of time}

As an agent acting at human-like time scale and dealing with temporal concepts
(like actions), a robot may want to represent, and possibly to reason, about
time. Time representation is split into two distinct abilities: representing
time points (both in the past -- which is roughly equivalent to assignment of
timestamps to events the robot perceives -- and in the future), and
representing \emph{passing time} (durations, timespans) like in \emph{``the
eggs will be cooked in 10 min''}.

\fxfatal{Discuss time chronicles~\cite{Ghallab1996}}
\fxfatal{Discuss Allen intervals}

We call a system that do not account for time (\ie that permanently lives in
present) \emph{atemporal}.

\paragraph{Representation of space}

\paragraph{Representation of events and actions}

\subsubsection{Context modeling}

\emph{Knowledge is contextualized information}\fxfatal{Find someone respectable
how said that :-)}: it is essential for the robot to associate the facts it
represents to a \emph{context}. The context carries the keys for the
interpretation of the information. It covers the \emph{domain of validity} of
the facts, the \emph{common-sense} knowledge required to fill the gaps in the
representation\fxfatal{give an example}, \fxfatal{What more?}.

\subsubsection{Possible-Worlds and representing what others know}
\label{sect|possible-worlds}

    
Linked to the context representation, but seen from another angle, knowledge
representation systems may provide explicit ways to model other point of view
on the world. This ability is often referred as the \emph{perspective taking}
ability.

\cite{Levesque2008}, p. 4

\subsubsection{Introspection: Who am I? What can I do?}
\label{sect|introspection}

\paragraph{Introspection}

Introspection is the ability to self-describe: what are my capabilities, what
is my state (performing some action, idling, etc.), what are my beliefs (\ie my
knowledge), what are my intentions and my plans?

Introspection must be distinguished from meta-cognition: While introspection
may require meta-cognition (for instance to be able to expose its internal
knowledge), it is not always mandatory. The current state of the robot can be
represented as a simple instantiation of a specific category (for instance, if
the robot give an object to the human, this state could be represented with the
triplets \setstmt{robot performs action1, action1 isA Give}.

\paragraph{Modelling of the robot capabilities}

A particularly important aspect of introspection relates to the description of
its own capabilities: which sensors/actuators/computation services exist and
are currently available ?  While at a first level, these descriptions can be
static (\eg the robot has one laser scanner and two arms), at more advanced
levels, the description is updated and reflect the current (and possibly past
and future) state of the robot. Note that these description may also involve
geometric descriptions (a kinematic chain, the pose of a device, etc.) that may
be deported outside of the main knowledge base. Efforts trying to formalize,
maintain and expose the capabilities and state of a robot are not new (and
ground themselves in work and techniques for self-descriptive remote procedure
calls in computing science), but take a renewed importance with applications
for high-level multi-robot cooperation. Recent work in that direction
include~\cite{Kunze2011}.

\subsubsection{Memory}
\label{sect|memory}

Memory has been studied at length in the cognitive psychology and
neuropsychology communities: Atkinson and Shiffrin~\cite{Atkinson1968}
introduce the idea of \emph{short-term} and \emph{long-term} memory,
Anderson~\cite{Anderson1976} splits memory into \emph{declarative} (explicit)
and \emph{procedural} (implicit) memories, Tulving~\cite{Tulving1985} organizes
the concepts of \emph{procedural}, \emph{semantic} and \emph{episodic} memories
into a hierarchy. Short-term memory is refined with the concept of
\emph{working memory} by Baddeley~\cite{Baddeley2010}
(Figure~\ref{fig|memory_models}).

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{typology/memory_models.pdf}
    \caption{Overview of the main types of memories}
    \label{fig|memory_models}
\end{figure}

Most knowledge representation systems offers some kind of memory as a pool of
facts that are not forgotten by the robot until it is halted (this memory is
often referred as a \emph{working memory}, but with a meaning unrelated to
Baddeley's definition). Some systems may propose persistent storages that allow
the robot knowledge to grow over time, while other may offer a larger range of
memory categories, like short term memory (that lasts for a couple of seconds)
or episodic memory (that allows the robot to selectively remember facts
associated to specific events).

In the larger field of cognitive architectures, the {\sc Soar}
architecture~\cite{Lehman2006} is one of those that tries to reproduce a
human-like memory organization.

%%%%%%%%%%
\subsection{Reasoning Techniques}
\label{sect|reasoning}

\begin{scriptsize}
\begin{center}
\begin{tikzpicture}[grow=right, sibling distance=5mm, level distance = 4cm]
    \node {\bf Reasoning}
            child {node {Learning}}
            child {node {Naive physics}}
            child {node {Planning}}
            child {node {Prediction}}
            child {node {Presupposition}}
            child {node {Non-monotonicity}}
            child {node {Uncertainty}}
            child {node {Lazy evaluation}}
            child {node {Structural alteration}}
            child {node {Standard reasoning}};
\end{tikzpicture}
\end{center}
\end{scriptsize}


\subsubsection{Standard reasoning techniques}

Being based on logical languages to represent the knowledge, most of the
systems we survey allow various forms of \emph{reasoning}.

We call \emph{standard reasoning techniques} techniques based on logical
inference, using resolution algorithms like \emph{forward chaining},
\emph{backward chaining} or \emph{semantic tableaux}.

Main reasoning problems include \emph{concept satisfiability},
\emph{consistency checking} and \emph{instance checking}.

Concept satisfiability verifies if it is possible to find a non-empty
\emph{interpretation} of a concept (or an expression defining a concept) in the
knowledge model. For instance, the formula \concept{Plant} $\land$
\concept{isRed}, which defines the concept of red plants, is satisfiable in a
model \concept{KB} iff $\exists a, $ \concept{Plant}$(a) \land$
\concept{isRed}$(a)$, \ie if we can find at least one red plant $a$ in our
model.

Checking the consistency of a model is equivalent to checking the
satisfiability of each of the concept defined in the knowledge model.

Instance checking consists in verifying that an individual $a$ is an
interpretation of a concept (or concept expression) $C$ in the knowledge model.
A typical example would be that we are provided with an instance
\concept{object1} and we want to know if this object is a kind of
\concept{Bottle} or \concept{Glass}.

Reasoners can often provide other type of inferences, like:

\begin{itemize}
    \item class subsumption (similar to inheritance)

    \item reasoning on roles properties, including:
        \begin{itemize}
        \item entailments based on roles domain and range (for instance, if the
        domain of the role \concept{thinksOf} is known to be
        \concept{ThinkingAgent}, then \concept{thinksOf}$(a, b) \to
        $\concept{ThinkingAgent}$(a)$),

        \item universal, existential and cardinality constraints,

        \item property characteristics (inverse, symmetry, transitivity, etc.)

        \end{itemize}

    \item complex concept expressions like: \par \footnotesize \concept{Bottle}
    $\equiv$ \concept{Artifact} {\bf that} (\concept{hasShape} {\bf value}
    \concept{cylinderShape})\footnote{This example uses the \emph{Manchester
    syntax}, \url{http://www.w3.org/TR/owl2-manchester-syntax/}} \normalsize

    \item set operations like: \par \footnotesize \concept{Color} $\equiv \bigcup$ (\concept{blue}, \concept{green}, \concept{orange},
    \concept{black}, \ldots) \normalsize

\end{itemize}

\paragraph{Rule Languages}

As mentioned earlier, knowledge models based on description logics can be extended through rule languages.

Intersection of properties is an example of expression that can only be represented with rules: for instance, 

generic SWRL ({\em Semantic Web Rule Language}) rules like: \par
        \footnotesize \concept{looksAt(?agt, ?obj)} $\land$
        \concept{pointsAt(?agt,?obj)} \par $\Rightarrow$ \concept{focusesOn(?agt, ?obj)}
        \normalsize 


\subsubsection{Alteration of the knowledge structure}

In systems

All systems allow to modify the ABox, not always possible to alter the TBox


\subsubsection{Lazy evaluation}
\label{sect|lazy-evaluation}


\subsubsection{Reasoning with uncertainty}


\subsubsection{(Non) Monotonic Reasoning}

\emph{Monotonic reasoning} means that addition of new assertions to a knowledge base
can only extend the set of assertions that can be inferred, while a
\emph{non-monotonic} reasoning scheme may lead to retraction of facts.
McCarthy coined a famous example to illustrate the need of non-monotonic reasoning:

\begin{quotation}
Consider putting an axiom in a common sense database asserting that birds can
fly. Clearly the axiom must be qualified in some way since penguins, dead birds
and birds whose feet are encased in concrete can't fly. A careful construction
of the axiom might succeed in including the exceptions of penguins and dead
birds, but clearly we can think up as many additional exceptions like birds
with their feet encased in concrete as we like. Formalized non-monotonic
reasoning provides a way of saying that a bird can fly unless there
is an abnormal circumstance and reasoning that only the abnormal circumstances
whose existence follows from the facts being taken into account will be
considered.
\end{quotation}

Another important application of non-monotonic reasoning is representation of
change: for example, to make an omelette, you need to crack eggs and wipe them.
The eggs disappear and are replaced by an omelette:

\concept{Egg}$(a) \land $ \concept{Egg} $(b) \land $
\concept{MakeOmelette}$(a, b, c) \to \lnot $ \concept{Egg}$(a) \land \lnot $
\concept{Egg}$(b) \land $ \concept{Omelette}$(c)$

The insertion of the proposition \concept{MakeOmelette}$(a, b, c)$ leads to
retraction of other facts. This rule requires non-monotonic reasoning to be
applied.

\emph{Default logic} is one of the formal logic that account for representing
general truth and exceptions to it (for instance, \emph{tomatoes are red, in
general}). However, due to computational complexity of these model (most of
inferences in default logic are known to be $NP$-complete problem), classical
logics and most of the existing reasoners do not allow non-monotonic reasoning.
For instance, the SWRL rule language, usually associated to the OWL-DL ontology
language, do not allow non-monotonic reasoning (only so-called DL-safe rules
are allowed).

\fxfatal{Make clear 'who does not allow non-monotonic-reasoning': logics? rule
languages? reasoner?}

One important exception is the \emph{negation as failure} inference rule, as
implemented by {\sc Prolog} for instance, that allows for non-monotonicity
within the closed world assumption.

\fxfatal{Give here an example of non-monotonic reasoning with Prolog}
\fxwarning{Do we mention here Answer Set Programming?}

A monotonic system does not theoretically allow for knowledge retraction,
which is an important issue in the robotic context where the world model is
likely to be often altered.  However it is a practical issue only if the
reasoning process is \emph{continuous} during the whole robot's activity
lifespan. It is often possible to stop the reasoner, alter the knowledge, and
restart the inference process on a new domain.
\fxfatal{Rephrase to emphasize that when new evidences appear, it is anyway often a
good idea to restart the reasoner.}

\fxfatal{Mention that the 'change of world' issue can also be dealt with appropriate
time representation.}
\fxfatal{Mention that probabilistic reasoning lead to implicit non-monotonic reasoning}


\subsubsection{Presupposition Accommodation}
\label{sect|presupposition-accommodation}

\emph{Presupposition accommodation} is the ability for the system to
automatically create a context allowing to make sense of a proposition.

For instance, we can imagine a human telling a robot \emph{Please get me the
bottle that is behind you}. If the robot has not yet see what is behind it, it
needs to assume (and represents in its knowledge model) that a undefined bottle
can be found somewhere in the half of space behind it.

A knowledge representation system able to cope with presupposition
accommodation would be able to take into account this (usually under-defined)
information that is not grounded into perception for later inferences.

This ability to imagine a physically state of the world that is not actually
perceived can be seen as the converse of the grounding ability.

Note also that presupposition accommodation implies a bidirectional link of the
symbolic knowledge model with a geometric (or physical) model of the
environment. This article focuses on symbolic knowledge representation systems,
but we shall mention when a KRS explicitly provides support for presupposition
accommodation.

\subsubsection{Prediction, projection and diagnosis tasks}
\label{sect|prediction-projection}

Levesque~\cite{Levesque2008} distinguish two main tasks, the \emph{projection
task} and the \emph{legality task}.

\paragraph{Projection task}: determining whether or not some condition while
hold after a sequence of actions.

\paragraph{Legality task}: determining whether a sequence of action can be
performed starting in some initial state.

\paragraph{Diagnosis}: this corresponds to the ability to rewind on past events
in case of failure to provide possible explanation. This can be seen as the
temporal reverse of the projection task.

\subsubsection{Planning}
\label{sect|planning}

Making decision based on prediction

\subsubsection{Physics-based reasoning}
\label{sect|physics}

As embodied entities, robots have to interact with physical entities.
\emph{Naive physics reasoning} covers all the everyday reasoning the humans
unconsciously perform, like taking into account gravity (``if I drop a ball, it
falls down'') or common physical properties of objects (``a glass may break if
dropped'', etc.). Many of the interactions with our everyday environments are
ruled by such laws that are difficult to exhaustively encode.

Some systems \cite{Kunze2011a} rely on external dedicated physics engine to
compute symbolic facts from on-demand physics simulation. This, however,
requires a tight integration between the symbolic model and a geometric model
that carries the geometries and physical properties of objects.

\subsubsection{Learning}
\label{sect|learning}

%%%%%%%%%%%%%%%%%
\subsection{Acquiring Knowledge}

\begin{scriptsize}
\begin{center}
\begin{tikzpicture}[grow=right, sibling distance=5mm, level distance = 3cm]
    \node {\bf Acquisition}
            child {node {Dynamic instantiation}}
            child {node {Grounding}}
            child {node {Acquisition and fusion}};
\end{tikzpicture}
\end{center}
\end{scriptsize}

\subsubsection{Knowledge acquisition and modalities fusion}
\label{sect|knowledge-acquisition}

In the survey inclusion criteria, we have insisted on only considering robotic
systems that acquire knowledge by themselves, during their runtime.

\emph{Acquiring knowledge} means in our context building new statements
(usually as new logical facts) connected to our existing knowledge from
external sources of information. We consider mainly three of them:
proprioceptive/exteroceptive sensors, interaction with other agents, humans or
robots, and remote databases. This process has generally at least two steps:
the information acquisition by itself, and the \emph{transformation} of the
information into knowledge, \emph{aligned} with the robot existing model
(following our terminology for information and knowledge, as discussed in the
introduction).

Knowledge acquisition is generally not done directly in the knowledge
representation system. On the contrary, several, if not numerous, external
components are usually required to convert percepts into symbolic facts and to
ground them.

While we do not review in this article all these systems, the whole process of
knowledge acquisition is central in cognitive robotic architecture and the
design of knowledge representation systems can influence or be influenced by
the approach to knowledge acquisition.

In particular, complex robotic systems often require multi-modal perception
capabilities (for instance, a robot can only interpret an utterance like ``this
is a plate'' if it is able to understand gestures, understand natural language
and merge them in a timely manner). Multi-modal interpretation can take place
at various levels, but in many cases (especially if the modalities are of very
different natures, like in the example above) merging will require
symbolic-level reasoning. The KRS has a direct impact on the feasibility and
ease of such operations.

\subsubsection{Grounding/anchoring strategies}
\label{sect|grounding}

\emph{Grounding} (also called \emph{anchoring} when specifically referring to
building links between \emph{percepts} and \emph{physical
objects}~\cite{Coradeschi2003}) is the task consisting in building and
maintaining a bi-directional link between sub-symbolic representations (sensors
data, low-level actuation) and symbolic representations that can be
manipulated and reasoned about~\cite{Harnad1990}.

Being embodied entities with interaction with other embodied entities as a
fundamental requirement, robots and robotic is deeply concerned by the
grounding issue.

Being actually implemented on real service robots, all the symbolic knowledge
representation systems that we review in this study have some kind of grounding
process. Numerous approaches exist, like amodal
\emph{proxies}~\cite{Jacobsson2008}, grounded amodal
representations~\cite{Alami2011, Mavridis2006}, semantic maps
(Figure~\ref{fig|semanticmap}, \cite{Nuechter2008, Galindo2008,Blodow2011}) or
affordance-based planing and object classification~\cite{Lorken2008,
Varadarajan2011}.

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{typology/semanticmaps_hertzberg.png}
    \caption{Example of a semantic map, taken from~\cite{Nuechter2008}.}
    \label{fig|semanticmap}
\end{figure}

\subsubsection{Dynamic instantiation}
\label{sect|new-instances}
Ability to automatically create new object instances

%%%%%%%%%%%%%%%%%
\subsection{Practical Integration in Robotic Architectures}
\label{sect|integration-robot}

\begin{scriptsize}
\begin{center}
\begin{tikzpicture}[grow=right, sibling distance=5mm, level distance = 3cm]
    \node {\bf Integration}
            child {node {Performances}}
            child {node {Monitoring}}
            child {node {Natural Language}}
            child {node {Executive layers}}
            child {node {Sensori-motor}};
\end{tikzpicture}
\end{center}
\end{scriptsize}


Knowledge representation systems do not mean anything to robots if they are
considered in isolation. This section proposes categories of features related
to the integration of the KRS into a larger software architecture that includes
perception routines, decision-making processes and actuation control.

We also mention some practical aspects of a real-world system, like
performances and monitoring tools that come along with the KRS.

\subsubsection{Integration with sensori-motor layers}
\label{sect|integration-sensorimotor}

We have previously discussed (section~\ref{sect|grounding}) the principles of
the grounding process that aims at establishing and maintaining a connection
between percepts (and to a lesser extend, low-level actions) and symbols.

While every real-world cognitive robot need some kind of grounding, the actual
implementations lead to very different information flows.

The systems can be roughly split into two classes: \emph{passive} knowledge
repositories that process symbolic facts produced by lower-level sensori-motor
layers (\emph{push} flow); \emph{active} knowledge managers that directly query
(possibly by polling or on-demand) low-level layers.

This macroscopic distinction is however mostly a matter of defining the
frontiers of the KRS: some systems like KnowRob~\cite{Tenorth2009a} encompass
geometric reasoning layers that would be considered as external by other
systems like ORO~\cite{Lemaignan2010} that focus on the symbolic fact storage
and rely on a ecosystem of independent modules to provide and consume symbolic
knowledge.

\fxwarning{Some archi with the ability to ``listen'' to the robot internal
structures -> which ones?}

Other systems do not fit either in such a partition between active and passive
systems because they do not stand as independent modules but exist as diffuse,
\emph{ubiquitous} knowledge manipulation system~\cite{Jacobsson2008}, for
instance because they are primarily language~\cite{Ferrein2008, Sabri2011}.

\subsubsection{Integration with executive layers}
\label{sect|integration-executive-layers}

Conversely, the knowledge management module need a tight integration with the
decision-making processes. As for the integration with sensori-motor
layers, the borders of the KRS can be fuzzy and vary from one architecture to
another: many consider symbolic task planning as an integral role of the KRS,
while other have dedicated extensions for planning, some integrate learning as
an on-the-flight process that is part of the KRS, others as an independent
deliberative process, etc.

The actual integration techniques vary also widely, from language extensions
(like the integration of CRAM~\cite{Beetz2010} with KnowRob) and client-server
architectures, to event-driven models (SHARY and ORO~\cite{Alami2011}). Choices
at this level have notable consequences on the whole design of the upper
control architecture of the robot, in particular regarding its modularity and
the ease of addition of new components.

\subsubsection{Language Processing}
\label{sect|nlp}

The scope of this survey is service robotic, which suggest human-robot
interaction. While numerous modalities of communication between humans and
robots do exist and have been extensively studied, the most challenging of
these interactions is natural, bidirectional, unconstrained verbal
communication. Kruijff et al. provides in~\cite{Kruijff2010} an up-to-date
survey of literature on situated human-robot dialogue, focusing on formal
representation systems, bi-directionality of the interaction and context
building.

Natural Language Processing (NLP) is a large research field by itself, and
while the robotic community may be lagging behind on many theoretical aspects,
it brings one important aspect: the embodiment. Because the interactors, both
the robot and the human, are establishing a communication within a shared
physical context, the verbal communication channel is complemented by deictic
channels, back channels and possibly shared physical experiences: a human can
show something to a robot, saying ``Give me this''. This is simply not possible
for a virtual agent.

Several of the knowledge representation systems we present have developed
specific built-in mechanisms or extensions to parse, ground and possibly
rebuild natural language.

\subsubsection{Monitoring and debugging}
\label{sect|debugging}

It is common to have knowledge representation systems at the heart of a
cognitive robotic architecture, and therefore KRS are easily ``burried'' in the
system.

At the same time, the symbolic model often provides a valuable synthetic view
on the whole state of the robot, furthermore easily understandable by the human
developer (the fact \stmt{human1 isSitting true} is easier to interpret than
the suite of relative coordinates of each joints of the human skeleton, as
provided by the human tracker, for instance).

That is the reason why having at hand good tools to trace and visualize at
run-time the evolution of the knowledge structure and contents, as well as
post-processing tools that can be run on the trace to precisely analyze the
cognitive behaviour of the robot, is useful.

\subsubsection{Evaluation of performances}
\label{sect|performances}

Benchmarks of symbolic systems for robots are notoriously hard to conduct for
several reasons: identifying good metrics for robotic experiments in general is
difficult because of the complex interactions between tenth of modules running
in parallel. Isolating the role of one specific component is thus hard.
Furthermore, knowledge representation systems are usually tightly connected to
numerous other modules, and the lack of standard API for knowledge services
makes it especially hard to switch between KRS to compare them. Also, because
service robots are design to act in rich, dynamic environments, possibly with
humans, building repeatable experiments is challenging.

Cognitive architectures and KRS in particular are consequently often
benchmarked with synthetic datasets (which leads to other issues: how to assess
the meaningfulness of the performance of a reasoner on an artificial
ontology?~\cite{Bail2010}) or \emph{toy} experiments~\cite{Chong2009} that do
not reflect real-world complexity.

While evaluating the performances of knowledge representation systems
themselves may be difficult, the cognitive performances of the robot as a whole
can be aptly evaluated through ``classical'' tests from the psychology (like
False-Belief experiments~\cite{Leslie2000} or the Token
test~\cite{DiSimoni1978}).

However, the performance evaluation of current tools is mostly qualitative and
scenario-specific (``did my system do the job for my task?''), and amongst the
surveyed KRS, only few systems proposes an evaluation of their performances
that could be compared to other system.

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Knowledge instantiation}

How much knowledge is available? Which content? How big is the knowledge base?

\begin{itemize}
	\item  Which underlying knowledge (\emph{common-sense}, \emph{upper knowledge}\ldots{})
	\begin{itemize}
		\item  top-down approach?
	\end{itemize}

\end{itemize}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Surveyed systems}
\label{sect|surveyed-systems}

Table \ref{table|surveyed-systems} presents the eight knowledge representation
systems surveyed in this article.

This section briefly presents each of them.

\begin{landscape}
\begin{table}\footnotesize
\begin{center}

\begin{tabular}{p{2.2cm}p{1.6cm}p{4cm}lp{2.4cm}p{3.4cm}p{2.8cm}p{1.5cm}}
\toprule
{\bf Project} & {\bf Category} & {\bf Authors (Institution)} & {\bf Project homepage} & {\bf Programming language} & {\bf Knowledge model/Logical Formalism} & {\bf Reasoner} & Main reference \\
\midrule
{\sc KnowRob} & KRS & Tenorth, Beetz \par (TU Munich) & \url{ias.in.tum.de/kb/wiki} & {\sc Prolog} & {\sc Prolog} + OWL-DL & Custom \par ({\sc Prolog}) & \cite{Tenorth2009a} \\
ORO & KRS & Lemaignan, Alami \par (LAAS-CNRS) & \url{oro.openrobots.org} & {\sc Java} & OWL-DL ({\sc Jena}) & {\sc Pellet} & \cite{Lemaignan2010} \\
PEIS KR\&R & KRS & Daoutis, Coradeshi, Loutfi, Saffiotti \par (Örebro Univ.) & \url{www.aass.oru.se/~peis} & {\sc C}, {\sc CycL} & CycL (1st and 2nd order logics, modal logics) & & \cite{Daoutis2009} \\
CAST Proxies & Ubiquitous & Wyatt, Hawes, Jacobsson, Kruijff (Brimingham Univ., DFKI Saarbrücken) & & & Amodal proxies & & \cite{Jacobsson2008} \\
NKRL & Language & Zarri et al. \par (Paris Est Créteil Univ.) & & NKRL & & & \cite{Sabri2011} \\
GSM & KRS & Mavridis, Roy \par (MIT MediaLab) & & ? & & & \cite{Mavridis2006} \\
OMRKF & KRS & Suh et al. \par (Hanyang Univ.) & \url{incorl.hanyang.ac.kr/xe} & ? & Horn Clauses & ? & \cite{Suh2007} \\
Ke Jia Project & KRS & Chen et al. \par (Univ. of Science and Technology of China) & \url{www.wrighteagle.org/en} & ASP & ASP & ASP & \cite{Chen2010} \\
ARMAR/Tapas & KRS & Holzapfel, Waibel \par (Karlsruhe TH) & & ? & TFS (Typed Feature Structures) & & \cite{Holzapfel2008}\\
Golog & Language & Levesque (Toronto Univ.) & & {\sc Prolog} & & & \\
OBOC & KRS & Mendoza & & & & & \cite{Mendoza2005} \\
% & & Varadarajan, Vincze \par (TU Wien) & & & & & \cite{Varadarajan2011} \\ % -> affordances, but no implementation on a robot
% & & Kaelbling, Lozano-Pérez \par (MIT CSAIL) & & & & & \cite{Kaelbling2011} \\ % -> mostly planning under uncertainty
% & & Hertzberg (Osnabrück Univ.) \\ % -> affordances, semantic mapping
% (based on {\sc KnowRob} & & (JSK) \\

\bottomrule

\end{tabular}
\end{center}

\caption{List of surveyed systems. Projects are listed by category (\emph{KRS}
for systems that are explicit knowledge representation and reasoning modules,
\emph{Ubiquitous} for systems where knowledge processing is fully distributed,
\emph{Language} for languages used as KRS on robots), then names.}

\label{table|surveyed-systems}
\end{table}
\end{landscape}

\fxfatal{Bielefeld -> could not find much...}
\fxfatal{Kollar/Tellex -> really focusing on the natural language grounding}

\subsection{KnowRob}
\label{sect|knowrob}

\subsection{ORO}
\label{sect|oro}

\subsection{PEIS KR\&R}
\label{sect|peis-ecology}


{\sc PEIS Ecology}~\cite{Saffiotti2005} is a software \emph{ecosystem} that aim to binds autonomous
robotics with ambient intelligence (network of sensors). \emph{PEIS} stands for
\emph{Physically Embedded Intelligent System}: every robots or intelligent
device in the environment is abstracted as a PEIS.

Each PEIS physical component is running a \emph{PEIS Kernel} instance. Communication
between instance relies on a custom P2P communication protocol.

The PEIS architecture allows for adding new abilities through software components sharing the common \emph{tuple space}.

We survey here the semantic layer~\cite{Daoutis2009}, referred as \emph{PEIS KR\&R}, that includes symbolic representation and reasoning.

% More in details:
% - object identification based on viewpoint independent SIFT features
% - formalized anchoring system that explicitly match percieved attributes to predicates
% - Cyc predicates
% - ground 12 colors, based on a paper on color perception. Could be useful for us.
% - idem, they cite a paper on what spatial relations to compute
% - location of objects based on a previously provided semantic map (but not much on this semantic map)
% - two "memories": the robot memory stores the current list of percieved objects ; the archive memory stores what is not percieved anymore
% - uses directly Cyc (ie, 250 000 common sense concepts...), via CycL language -> 2nd and higher order logics (quantification over predicates, functions, etc)
% Remark: using 2nd order logic (ie meta statements), it would be easy to store the knowledge of each agent
% - disambiguation in concept name by asking human to decide amongst all concepts known by Cyc
% - template based natural language
% - experiment conducted in a "smart" indoor environmement + simple robot

\paragraph{Knowledge model} The PEIS Knowledge representation system relies on
the {\sc ResearchCyc} and {\sc CycL} language to represent knowledge. The {\sc CycL} language
allows to represent first order logic sentences and has extensions for modal logics and higher order logics.

\fxfatal{Is modal logics and higher order logics actually used in PEIS?} 

As a system relying on {\sc CycL}, contexts can be expressed as
\emph{microtheories}: the truth or falsity of a set of statement depends of the
\emph{microtheory} in which these statements are evaluated.

\fxfatal{OWA/CWA?}

\begin{figure}
	\centering
	\includegraphics[width=0.9\columnwidth]{stateofart/peis-architecture.pdf}
	\caption{The PEIS knowledge representation system, taken from~\cite{Daoutis2009}}
	\label{fig|peis-archi}
\end{figure}

The PEIS KR\&R system is deeply integrated to the general PEIS Ecology
\emph{smart} environment. Figure~\ref{fig|peis-archi} gives an overview of the
interactions between PEIS knowledge processing layers.

\paragraph{Knowledge Acquisition} The primary source for knowledge acquisition
is perception.  The PEIS ecosystem provides a SIFT-based object recognizer used
in conjunction with ceiling cameras for object localization.  Other perceptual
modalities are available (like human tracking, ambient environment monitoring).

A template-based natural language parsing system may also be used to add new
assertions to the system.

The system can ask the human for help to disambiguate between concept names.

\paragraph{Anchoring} Daoutis et al. formalize the issue of anchoring as
finding a \emph{predicate grounding relation} $g \subseteq \mathcal{P} \times
\Phi \times D(\Phi)$, where $\mathcal{P}$ is a set of predicate symbols, $\Phi$
a set of percept's attributes, and $D(\Phi)$ the domain of these attributes.

In the current implementation, object category (returned by the SIFT
classifier), color, location, spatial relations (both topological -- \emph{at},
\emph{near} -- and relative to the robot -- \emph{left}, \emph{behind}, etc.)
and visibility are the five classes of extracted attributes.

\paragraph{Integration in the robot architecture}
\label{sect|peis-integration}

The PEIS framework offers through the \emph{PEIS middleware} a practical way to
insert a new component into the shared \emph{tuple space}.  Thus, the KR\&R
module can be seamlessly integrated into the PEIS ecosystem.

\subsubsection{Notable experiments}
\label{sect|peis-expe}

\subsection{NKRL}
\label{sect|nkrl}

\emph{NKRL} stands for \emph{Narrative Knowledge Representation Language}.
While this language is developed since a long time by Zarri~\cite{Zarri1997,
Zarri2008}, recent research direction include application to the robotic
field~\cite{Sabri2011}. NKRL is not {\it per-se} a knowledge representation
system, as it is primarily a language. However, it is used as the
representation and reasoning mechanism for robots by Sabri et al.

\subsubsection{Intrinsic language features}
\label{sect|nkrl-intrinsic-features}

\paragraph{Expressiveness}

\subsubsection{Integration with physical world and in the robot architecture}
\label{sect|nkrl-integration}

...seem to be mostly WIP...


\subsection{CAST Knowledge model}
\label{sect|cast}

CAS (\emph{CoSy Architecture Schema}) Toolkit~\cite{Hawes2007} is a
comprehensive toolkit aimed at building cognitive architectures for robots
through a set of interconnected SAs (\emph{subarchitectures}). The CAS does not
expose a central knowledge base as seen in previous works. It instead
represents knowledge as unrooted \emph{proxies}. Those proxies are formally
defined in \cite{Jacobsson2008} as $p= \langle F_p, u_p \rangle$ where $F_p$ is
a set of instantiated features (like $\phi^{Colour}_{red}$) and $u_p$ a
\emph{proxies union} that form an equivalence class corresponding to one
entity.

A union of proxies forms a global amodal representation of an entity, that can
be explicitly shared and manipulated. Being not centralized, the knowledge
model can be qualified of \emph{ubiquitous}. Furthermore, knowledge source in
the CAS architecture is tightly bound to the on-line grounding process (be it
grounded in perception or in dialogue). While nothing seems to prevent it, no
{\it a priori} knowledge (including common-sense knowledge) is used.

Knowledge sharing is ensured by the event mechanism of CAST: modules can
monitor proxies for alteration by other modules. Jacobsson et al. mention how
this can apply to reinforcement learning: the vision module creates a proxy for
an orange object. This proxy get monitored by a learning module. In parallel,
the proxy is bound to an union by the natural language understanding module
that add new a feature like \emph{"this object is a fruit"}. The learning
module is called back, and can add this new information to its model.

In the presented implementation, the CAST knowledge model does no allow for
effectively representing actions or temporal information.\fxfatal{What about reasoning? can they retrieve for example 'all proxies for colorful objects'?}

\subsection{GSM}
\label{sect|gsm}

GSM (for \emph{Grounded Situation Model})~\cite{Mavridis2006} is a knowledge
representation system primarily built to ``facilitate cross-modal
interoperability'',  especially in the context of verbal interaction with a
robot.

Unlike most of the previously presented systems, GSM does not rely on any
formal language but rather on a layered data structure (Figure~\ref{fig|gsm})
that organizes the surrounding world into agents and relations between agents.
Each agent (any animate or inanimate object) is attached to a physical model (made
of \emph{body parts} that have properties like their position, color, etc.) and
a mental model (which is a recursively embedded GSM, thus allowing a sort of
theory of mind).

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{stateofart/gsm.pdf}

    \caption{Simplified hierarchical structure of the Grounded Situation Model,
    based on~\cite{Mavridis2006}}

    \label{fig|gsm}
\end{figure}

Properties are represented in three layers: a stochastic representation, close
to sensory percepts, a \emph{continuous single-valued} encoding of the
stochastic model, and a discrete, categorical model.

One notable feature of GSM is the \emph{bidirectionality} of the grounding
process: not only sensor percepts are abstracted into categories suitable for
human conversation, but human utterance (like ``There is a red ball in the
center of the table'') can also be turned into property descriptions. This
basically enable the knowledge representation system of the robot to
\emph{imagine} entities.

GSM also features several strategies for managing time and events.
\emph{Moments} are created by storing timestamped snap-shots of GSM, and
\emph{event classifiers} allow to define and detect events.

\paragraph{Experiments} GSM has mostly been tested on table-top manipulation
and interaction tasks (a ``conversational helping hand'' as stated by the
authors) realized on a 7-DOF arm equipped with force feedback, cameras for blob
tracking and speech recognition (Sphinx4). Mavridis and Roy provide in addition
an in-depth analysis of the performance of GSM by the mean of a standard
psycholinguistic test, the \emph{Token test}~\cite{DiSimoni1978}.

\subsection{OMRKF}
\label{sect|omrkf}

The Ontology-based Multi-layered Robot Knowledge Framework~\cite{Suh2007}
(OMRKF) is a knowledge representation system based on four inter-related
\emph{classes} of knowledge (Figure~\ref{fig|omrkf}). It proposes a layered
approach to knowledge representation that allows to integrate the grounding
process to the knowledge representation process. OMRKF knowledge model is
implemented with Horn clauses.

\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{stateofart/omrkf.pdf}

    \caption{OMRKF organizes knowledge into four \emph{classes}, each composed
    of three \emph{levels}. The figure shows some examples of link between
    knowledge classes and knowledge levels. Based on~\ref{Suh2007}.}

    \label{fig|omrkf}
\end{figure}

Each level of knowledge is build as three stages of ontological realization: a
\emph{meta-concept} (the level itself, like ``temporal context'', ``behaviour''
or ``object feature'', a taxonomy of concepts inside this level (for instance
$cup : Object \sqsubseteq tableware : Object$) and an instantiation of the
taxonomy ($cup1 : cup$).

Environment is represented in OMRKF in the $space : Model$ knowledge level as a
classical three layers mapping (metric, topological and semantic maps). Objects
(in $object : Model$) are localized in $space : Model$ through Voronoi nodes.

The knowledge class $Context$ proposes an explicit statement of spatial context
(mostly geometric relations between objects), temporal context and a more
general \emph{high-level} context, inferred from spatial and temporal contexts.

Finally, the $Activity$ knowledge class store compound actions in a HTN-like
structure, exploited at run-time by a planner.

\paragraph{Experiments} Experiments conducted with OMRKF include finding
kitchen objects and reporting about their state to a human.  This experiment
also shows how OMRKF can deal with objects only partially matched by their
descriptor by introducing a $candidate()$ function.

\subsection{Ke Jia Project}
\label{sect|kejia}

The Ke Jia project~\cite{Chen2010} integrates on a mobile platform a knowledge
representation language with natural language processing, task planing and
motion planing.

Knowledge representation relies on \emph{Action Language C}, itself based on
\emph{Answer Set Programming} (ASP)~\cite{Gelfond2008}. These languages, that
are syntactically close to Prolog, are based on \emph{stable models} of logic
programs, and support non-monotonic reasoning. Default and non-monotonic
reasoning has been especially researched within the Ke Jia project for symbolic
task planing~\cite{Ji2011} and underspecified natural language processing.

Amongst other features, the natural language processing capabilities of the
system support acquisition of new logical rules at run-time.

\paragraph{Experiments} The Ke Jia robot has been demonstrated in several tasks
involving human-robot interaction with natural language. These tasks include a
task with multiple \emph{pick \& carry} that are globally optimized, naive
physics reasoning via taught rules or more complex scenarii with the robot
delivering drinks, taking into account changing and mutually exclusive
preferences of users.

\subsection{ARMAR/Tapas}

{\sc Tapas} is the name of the knowledge representation system and dialogue
manager found on the ARMARIII robot~\cite{Holzapfel2008}.

Knowledge in {\sc Tapas} exists as procedural knowledge (plans) and declarative
knowledge. The later is split into \emph{lexical knowledge}, \emph{semantic
knowledge} and a database of identified objects (with their properties). The
\emph{lexical knowledge} contains lexical and grammatical informations about
the objects. The \emph{semantic knowledge} is organized into an ontology
relying on \emph{typed feature structures} (TFS,~\cite{Carpenter1992}, a
formalism originating from the computational linguistics community, and a
superset of first-order logic).

{\sc Tapas} has a strong focus on natural language grounding. It proceeds by
generating grammars from properties represented in the ontology to parse and
understand dialogue.

Another focus is put on handling unknown words and objects. {\sc Tapas}
provides routines to recognize unknown entities, and propose and interactive
and iterative verbal process to categorize (including adding new categories)
those new concepts.

%%%%%%%%%% Underlying knowledge model table %%%%%%%%
\begin{table}
\begin{center}

\begin{tabular}{lp{4cm}}
\toprule
{\bf Project} & {\bf Common-sense \par knowledge source} \\
\midrule
{\sc KnowRob} & {\sc OpenCyc}, processed web content, custom OWL-DL ontology \\
ORO & {\sc OpenCyc}, custom OWL-DL ontology \\
PEIS Ecology & {\sc ResearchCyc} \\
NKLR &  None \\
CAST Proxies &  None \\
GSM &  Predefined categories \\
OMRKF & {\it A priori} knowledge structure and axioms, custom set of instances\\
Ke Jia & None \\
ARMAR/{\sc Tapas} & Custom ontology related to the kitchen\\

\bottomrule

\end{tabular}
\end{center}
\caption{Underlying knowledge sources for each project}
\label{table|knowledge-sources}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Towards the next generation of Knowledge Representation Systems for Robotics}
\label{sect|conclusion}

This section tries to summary the various approaches we surveyed in the
previous sections and to draw some research perspectives. This section does not
exactly follow the list of compared features presented in
section~\ref{sect|features}. We have organized it along four axis: \emph{what
can be represented?}, \emph{How knowledge is created and grounded?}, \emph{What
can be done with the knowledge?} and \emph{How to use knowledge in the whole,
larger robot architecture?}.

Table~\ref{table|contribution-by-systems} proposes a summary of the main scopes
of \emph{contribution} of the systems we have surveyed. This overview helps to
identify the main weaknesses of current approaches of knowledge representation
regarding the long-term goal of \emph{human-level robots}.

\begin{landscape}
\begin{table}\footnotesize
\begin{center}

\begin{tabular}{cp{4.5cm}p{2.3cm}p{2cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1.5cm}p{2cm}p{1.5cm}p{1.5cm}}
\toprule
\multicolumn{2}{c}{\bf Category} & {\sc KnowRob} & {ORO} & {\sc PEIS} & {\sc CAST} & {\sc Ke Jia} & {\sc NKRL} & {\sc GSM} & {\sc OMRKF} & {\sc ARMAR} & {\sc Hertzberg} \\

\midrule

\multirow{5}{0.7cm}[0.2cm]{\turn[1.5cm]{90}{\bf Expr. power}} & Logical formalism & Prolog & DL (OWL) & {\sc CycL} &  & ASP &  & & Horn clauses & TFS & \\
 & OWA/CWA & CWA & OWA & ? & ? & CWA & ? & ? & ? & ? & ? \\
 & Modeling uncertainty & +++ (extension) & & & & & & ++ (stochastic) & + (\emph{candidate} entities) \\
 & Meta-cognition & ++ & ++ & & & & & & \\
\hline
\multirow{6}{0.2cm}{\turn{90}{\bf Model}} & Space Representation & & & & & & & & ++  \\
 & Time representation & & & & & & & + (snapshots) & +  \\
 & Context & & & & & & & & ++  \\
 & Alternative worlds & & +++ & & & & ++ & &  \\
 & Introspection & + (SRDF) & & & & & & &  \\
 & Memory models & & + & & & & & &  \\
\hline
\multirow{10}{0.2cm}{\turn{90}{\bf Reasoning}} & Standard FOL reasoning & +++ & +++ & & & +++ & & & +  \\
 & Knowledge structure alteration & & ++ & & & ++ & & & & ++ &  \\
 & Lazy evaluation & +++ & & & & & & &  \\
 & Reasoning under uncertainty & ++ (extension) & & & & & + & & \\
 & Non-monotonic reasoning & & & & & +++ & & &  \\
 & Presupposition accommodation & & & & & & & +++ & \\
 & Prediction, projection, diagnosis & & & & & & & & \\
 & Physics-based reasoning & +++ (extension) & & & & & & &  \\
 & Planning & + & +++ (external) & & & ++ & & & + \\
 & Learning & & + & & & & & & & ++ &  \\
\hline
\multirow{3}{0.7cm}{\turn[1cm]{90}{\bf Knw. acq.}} & Acquisition techniques & & & & & & & & \\
 & Grounding & +++ \par (semantic maps) & ++ (external amodal model) & & & & & ++ & ++ \\
 & instantiation & & & & & & & & \\
\hline
\multirow{4}{0.2cm}{\turn{90}{\bf Integ.}} & ...with sensori-motor layers & & & & & & & + & + \\
 & ...with executive layers & & & & & & & + & ++ \\
 & NLP & & +++ (external) & & & +++ & & +++ & & +++ & \\
 & Monitoring and debugging & & & & & & & & \\
 & Performances & & & & & + & & +++ (Token test) & & & \\

\bottomrule

%\end{tabularx}
\end{tabular}
\end{center}
\caption{Main domain of contribution of each surveyed systems. Rating goes from +++ (major focus) to + (secondary interest). An empty cell means that 
the system does not consider this domain.}
\label{table|contribution-by-systems}
\end{table}
\end{landscape}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{The Knowledge API}
\label{sect|knowledge-api}

During the preparation of the thesis, discussions with several people involved
in knowledge representation (namely Dominik Jain, Lars Kunze, Michael Beetz)
have led to the draft of a generic API for knowledge access and exchange.

This section presents this effort of standardisation that is (partially)
implemented by the ORO server, presented in the next chapter.

\subsection{Rationale and General Considerations}

The original idea comes from the acknowledgement that more and more software
components for robotics want to store or use symbolic data. Since established
international efforts at defining standard for inter-component communication
like ROS have already proved their usefulness, one single API for different
knowledge representation and management systems could be equally useful.

Two main previous attempts must be noted: the \emph{Knowledge Interchange
Format} (KIF) and the DIG interface.

\fxfatal{Complete this part}
The \href{http://dig.sourceforge.net/}{ DIG interface}

The \href{http://logic.stanford.edu/kif/dpans.html}{ Knowledge Interface Format
(KIF)}. In~\cite{Ginsberg1991}, Ginsberg explains why \emph{knowledge
interchange formats} are hardly a good idea.

The API is designed for robotics (even if probably useful in other contexts):
it aims to be simple and practical for clients, it explicitly supports
uncertain knowledge and multiple models, and it makes clear how knowledge is
added or retracted with explicit policies.

We have attempted to design it in a way that do not restrict expressiveness
(any logical sentence that can be expressed in the logic of predicates, with a
probabilistic extension, can be manipulated by the API), and a simple extension
mechanism should permit future evolutions in a backward compatible way.

Besides facilitating exchange of knowledge contents between systems by ensuring
one standard formalism, another outcome of the adoption by several KRS of this
API is that it allows easy switch between semantic engines (and thus
benchmarking and sharing of unit-tests).

This API was developed with Prolog-based knowledge systems, Description
Logics-based knowledge bases and Markov networks in mind, and should cover as
well other systems related to predicate logics (with or without a probabilistic
extension).

Besides standard operations on axioms and taxonomy, the API aims to cover:

\begin{itemize}
    \item  probabilities associated to statements
    \item  management of several models
    \item  explicit policies to add, retract or, more generally, alter 
    knowledge (for instance, to guarantee consistency when adding knowledge)
    \item  specific, implementation-dependent, extensions through the
    \texttt{special} method.
\end{itemize}

Implementations are not always expected to cover to whole API, but must have a
predicable behaviour when a part of the API is not implemented. In particular,
the API makes no assumptions on implementations regarding:

\begin{itemize}
    \item  the actual supported expressiveness (the API allows to express
    general first-order logics statements, but the underlying implementation
    may support only a subset, for instance, Description Logics)
    \item  Closed-world assumption vs Open-world assumption
    \item  Reasoning capabilities
\end{itemize}

\paragraph{Conventions}

\begin{itemize}

    \item  \textbf{Service}: we call \emph{service} the set of all methods
    defined by this API.

    \item  \textbf{Method}: \emph{method} refers to one single remote function
    offered by the service. The terms function, method, and procedure can be
    assumed to be interchangeable.

    \item  The \textbf{Service provider} or \textbf{implementation} is a
    software that implement the API.

    \item  A \textbf{policy} is a (extensible) set of rules that defines in
    which ways the knowledge must be altered. It is represented as a dictionary
    of (keys, values). The section~\ref{sect|kbapi-policies} details the
    content of these policies.

\end{itemize}

\subsection{Statements, partial statements and rules}


\subsubsection{Resources and literals}


All entities in the knowledge base are either \textbf{resources} or
\textbf{literals}. Resources can be classes, predicates or instances.

Resources (\ie, not literals) must start with a letter and can be comprised of
symbols [{\tt a-zA-Z0-9\_}].

They may be prefixed with a namespace prefix (like in \texttt{rdf:type}) or
come with a complete namespace URI (like
\texttt{$<$\href{http://www.w3.org/1999/02/22-rdf-syntax-ns\#type}{http://www.w3.org/1999/02/22-rdf-syntax-ns{\textbackslash}\#type}$>$}).
In this case, the full URI (the namespace and the resource name) must be
enclosed between $<$ and $>$. If no namespace is provided, it is assumed to be
defined in the server default namespace.

Literals should follow the SPARQL syntax for
literals\footnote{\url{http://www.w3.org/TR/rdf-sparql-query/\#QSynLiterals}}.
Give give below some examples of valid literals:

\begin{itemize}

    \item  The boolean {\bf true} can be represented either as
    \texttt{"true"\^{ }\^{ }xsd:boolean} or as \texttt{true},

    \item  The integer {\bf 123} can be represented either as \texttt{123\^{
    }\^{ }xsd:int} or as \texttt{123}. \texttt{"123"\^{ }\^{ }xsd:int} is also
    acceptable.

    \item  The double {\bf 1.23} can be represented either as \texttt{1.23\^{
    }\^{ }xsd:double} or as \texttt{1.23}. \texttt{"1.23"\^{ }\^{ }xsd:double}
    is also acceptable.

    \item  Strings are either single or double quoted: \texttt{"Hello Word"},
    \texttt{'Hello Word}', \texttt{"Hello Word"\^{ }\^{ }xsd:string} all
    describe the same literal.

    \item  User-defined dataypes can be represented with \texttt{"xyz"\^{ }\^{
    }$<$\href{http://example.org/ns/userDatatype}{http://example.org/ns/userDatatype}$>$}
    or \texttt{"xyz"\^{ }\^{ }ns:userDatatype}.

\end{itemize}

\subsubsection{Statements}


\paragraph{General syntax}

Two syntaxes are admissible to represent a \textbf{statement} (or
\textbf{axiom}).  The general one is a string starting with a predicate
\texttt{p} followed by a set in brackets of comma-separated arguments:

    \begin{center} \tt p(a1, ..., an) \end{center}

where \texttt{n} is the arity of the predicate. For instance:

    \begin{center}  \tt cutsWith(human0, bred0, knife0) \end{center}

The predicate must be a resource, as well as at least one of the arguments.
Other arguments may be either resources or literals.

\paragraph{Infix syntax}

As a special case, the infix syntax (\emph{subject predicate object}) is
acceptable for binary predicates (in this case without commas or brackets):

    \begin{center} \tt s p o \end{center}

For instance:

    \begin{center} \tt sky0 hasColor blue \end{center}


Further examples include:

\begin{itemize}
    \item {\tt Age(EiffelTower, 300)},

    \item {\tt filledWith(my\_cup, coffee)}, which is strictly equivalent to
    {\tt my\_cup filledWith coffee},

    \item {\tt my\_cup rdf:type opencyc:Cup},

    \item {\tt my\_cup weights 10.5} (assuming that the {\tt weights} predicate
    defines a unit).

\end{itemize}

\subsubsection{Partial statements}

A \textbf{partial statement} is a statement with a least one unbound member.
Unbound members are denoted either by a ``*'' (an anonymous variable, for
instance \texttt{* isVisible true}) or by a string starting with \texttt{?}
(named variable, for instance \{\texttt{sees(?ag, obj1), ?ag type Human}\}).

Partial statements can often be seen as masks or patterns, and used as such in
methods like \texttt{find} or \texttt{remove}.

\subsubsection{Rules}

Rules syntax is based on the human-readable form of the SWRL
syntax\footnote{\url{http://www.w3.org/Submission/SWRL/\#2.2}}, encapuslated
in a string. Atoms must be separated by either a comma, \texttt{\^{ }} (unicode
U+005E) or \texttt{{$\land$}} (logical AND, unicode U+2227).

For instance:

\begin{itemize}

    \item {\tt Yogurt(?x)->DairyProduct(?x)} asserts that all instances of type
    {\tt Yogurt} are also {\tt DairyProduct},

    \item {\tt Tableware(?o), eatsWith(?x, ?o), Yogurt(?x) -> Tablespoon(?o)}

\end{itemize}

\subsubsection{Probabilities}

Statements may have a truth probability attached to them, as a float value
comprised between 0.0 (impossible fact) and 1.0 (certain fact). If no
probability is specified, a probability of 1.0 is assumed.

Two syntaxes are possible: either included in the statement string, separated
with a colon:

\begin{center} \tt predicate(a1, ...,an):p \end{center}

For example: {\tt Age(EiffelTower, 300):0.54}, which means: the fact that the
Eiffel tower is 300 years old holds with a probability of 54\%.

Or as a independent float value:

\begin{center} \tt [predicate(a1, ...,an), p] \end{center}

For example: \texttt{[Age(EiffelTower, 300), 0.54]}

When using the infix syntax for statements, only the second syntax for
probabilities is allowed:

\texttt{[human1 holds my\_cup, 0.87]} (which mean that it is believed with 87\%
of certainty that the human holds the cup).

The type \texttt{statement} \textbf{always} means either a statement alone or a
statement with a probability.


\subsubsection{Sets of statements}

Statements can be grouped inside sets, enclosed in square brackets.

When applicable and except otherwise noted, the comma between statements (or
partial statements) in a set must be interpreted as a logical AND.

Example:

\begin{itemize}
    \item {\tt find [* filledWith coffee, * rdf:type Cup]} would return
    instances that are both cups and filled with coffee,

    \item {\tt find ["* rdf:type TemporalThing", "* rdf:type SpatialThing"]}
    would not answer anything, assuming \concept{TemporalThing} and
    \concept{SpatialThing} are disjoint types.

\end{itemize}

\subsection{Policies}
\label{sect|kbapi-policies}

Knowledge content interacts with the knowledge base through the \texttt{revise}
method. This method takes as first parameter a set of statements, and as second
parameter, a \emph{policy} that specifies what must be done with the
statements.

A policy is represented as a set of \texttt{(key, value)} pairs whose possible
values are presented in table~\ref{table|knowledge-policies}.

\begin{table}
\begin{center}

    \begin{tabular}{lp{4cm}p{9cm}}%% 22 | 5 | X | 
    \toprule
    Key & Values & Meaning \\
    
    \midrule

    { \tt method} & {\tt add} \emph{(default)} & the statements are added to the
    knowledge base, without ensuring consistency.\\ 
    
    \midrule

    & {\tt safe\_add} & the statements are added only if they (individually) do
    not lead to inconsistencies.\\ 

    \midrule
    
    & {\tt retract} & the statements are removed from the model. Associated
    probabilities are discarded.\\ 
    
    \midrule
    
    &{\tt update} & Updates objects of one or several statements in the
    specified model. If the predicate is not inferred to be \emph{functional}
    (\ie, it accept only one single value), behaves like {\tt add}.\\ 
    
    \midrule
    
    & {\tt revision} or {\tt safe\_update} & Updates objects of one or several
    statements in the specified model if it does not (individually) lead to
    inconsistencies. If the predicate is not inferred to be \emph{functional}
    (\ie, it accepts only one single value), behaves like {\tt safe\_add}.\\ 
    
    \midrule
    
    {\tt model} & {\tt all} \emph{(default)} & all existing \emph{models}
    (section~\ref{sect|kbapi-models}) are impacted by the change.\\

    \midrule
    
    & a valid model id or a set of valid model id & only the specified model(s)
    are impacted\\
    
    \bottomrule
    
    \end{tabular}

\end{center}
\caption{Knowledge revision policies.}
\label{table|knowledge-policies}
\end{table}

\fxwarning{What to do with: For more background on the knowledge revision
strategies, please visit \href{http://en.wikipedia.org/wiki/Belief_revision}{
this Wikipedia page}.}

\subsection{Models}
\label{sect|kbapi-models}

A model is a knowledge container unit (for instance, a RDF tree, a SQL
database, etc.). The service provider \textbf{may} support several models
(for instance, one per agent or one for certain facts, one for uncertain facts,
etc.). The actual use and semantic of each model is left to the client.

Each model is identified by a unique alphanumeric id. Most of
the API methods can take as parameter a model id. If omitted ({\tt null}) or
if the reserved id {\tt all} is used, the method is applied on all models
(except otherwise noted, this should be strictly equivalent to call the method
on each model separately). Service provider must offer at least one model
called {\tt default}, denoting the default robot knowledge storage.


\subsection{Core API}

\subsubsection{Service management}

\begin{itemize}

\item  \texttt{\emph{string} \textbf{hello}()}
    \begin{itemize}
    \item  Returns the version number of the service provider. Can be used to
    check connection status.
    \item  \emph{Params}: None
    \item  \emph{Return values}:
        \begin{itemize}
        \item  version number \emph{(string)}
        \end{itemize}
    \end{itemize}

\item  \texttt{\textbf{load}(\emph{string} path, [\emph{string} model])}
    \begin{itemize}
    \item  Loads the content of the specified OWL ontology. If
    $<$owl:imports$>$ are specified, the server is expected to honour them.
    \item  \emph{Params}:
        \begin{itemize}
        \item  [string] the URI of the OWL file. Must be reachable by the
        server.
        \item  \emph{opt.} [string] the model ID that receives the OWL content.
        If omitted, the content is added to all existing models. 
        \end{itemize}

    \item  \emph{Return values}: None
    \end{itemize}

\item  \texttt{\textbf{save}([\emph{string} path], [\emph{string} model])}
\begin{itemize}
\item  Saves the model content to an OWL ontology. Each models are stored in their own namespace (by appending the model id to the default namespace).
\item  \emph{Params}:
\begin{itemize}
\item  \emph{opt.} [string] the path where to export the OWL file. Must be reachable by the server. If omitted, a default, unique path, is build (the naming scheme is implementation dependent). OWL serialization (XML, turtle, n3\ldots{}) is implementation dependent.
\item  \emph{opt.} [string] the model ID that should be saved. If omitted, all models are dumped.
\end{itemize}

\item  \emph{Return values}: None
\end{itemize}

\item  \texttt{\textbf{reset}([\emph{string} model])}
\begin{itemize}
\item  Reset a model to its initial state. Note that the initial state may not be an empty set of facts if the service provider loads some initial content at model initialization.
\item  \emph{Params}:
\begin{itemize}
\item  \emph{opt.} [string] the model ID to reset. If omitted, all models are reset. 
\end{itemize}

\item  \emph{Return values}: None
\end{itemize}

\item  \texttt{\textbf{special}(\emph{string} method, [\emph{set$<$string$>$} parameters])}
\begin{itemize}
\item  This method enables implementation-dependent extensions.
\item  \emph{Params}:
\begin{itemize}
\item  [string] the name of the method to execute
\item  \emph{opt.} [set$<$string$>$] method parameters
\end{itemize}

\item  \emph{Return values}: implementation dependent
\end{itemize}

\end{itemize}

\subsubsection{Managing knowledge}


\begin{itemize}
\item  \texttt{\textbf{revise}(\emph{set$<$statement$>$} statements, [\emph{policy} policy])}
\begin{itemize}
\item  Alter the knowledge base with one or several statements, following the specified \hyperref[94422fa6b0fe23d0ab463703c0022a63]{policy}.
\item  \emph{Params}:
\begin{itemize}
\item  [set$<$statement$>$] the set of statements to add.
\item  \emph{opt.} [policy] the policy to follow. If omitted, the default policy is applied (all statements are added to all models, without checking for consistency). 
\end{itemize}

\item  \emph{Return values}: None
\end{itemize}

\item  \texttt{\textbf{add}(\emph{set$<$statement$>$} statements, [\emph{string} model])}
\begin{itemize}
\item  Adds one or several statements to the specified model. Equivalent to \texttt{revise} with the policy \texttt{{"method":"add"}}.
\item  \emph{Params}:
\begin{itemize}
\item  [set$<$statement$>$] the set of statements to add.
\item  \emph{opt.} [string] the model ID that receives the statements. If omitted, statements are added to all models. 
\end{itemize}

\item  \emph{Return values}: None
\end{itemize}

\item  \texttt{\textbf{retract}(\emph{set$<$[statement|partial{\textunderscore}statement]$>$} pattern, [\emph{string} model])}
\begin{itemize}
\item  Removes one or several statements to the specified model. Equivalent to \texttt{revise} with the policy \texttt{{"method":"retract"}}.
\item  \emph{Params}:
\begin{itemize}
\item  [set$<$statement|partial{\textunderscore}statement$>$] the set of statements to remove. If a partial statement is encountered, all statements matching this pattern are removed.
\item  \emph{opt.} [string] the model ID where the statements must be removed from. If omitted, statements are removed from each models. 
\end{itemize}

\item  \emph{Return values}: None
\end{itemize}

\item  \texttt{\textbf{update}(\emph{set$<$statement$>$} statements, [\emph{string} model]))}
\begin{itemize}
\item  Updates objects of one or several statements in the specified model, and only for \emph{functional} predicates (ie, predicates that accept only one value). Equivalent to \texttt{revise} with the policy \texttt{{"method":"update"}}.
\item  \emph{Params}:
\begin{itemize}
\item  [set$<$statement$>$] the set of statements to update. If a partial statement is encountered, all statements matching this pattern are removed.
\item  \emph{opt.} [string] the model ID to update. If omitted, statements are updated in all models. 
\end{itemize}

\item  \emph{Return values}: None
\item  
\end{itemize}

\end{itemize}

\subsubsection{Knowledge retrieval}


\begin{itemize}
\item  \texttt{\textbf{lookup}(\emph{string} concept, [\emph{string} model]))}
\begin{itemize}
\item  Searches for the given string in the knowledge base, and returns the matching resources, along with their types.
\item  \emph{Params}:
\begin{itemize}
\item  [string] the string to look for.
\item  \emph{opt.} [string] the model ID to look in. If omitted, the string is looked for in all models. 
\end{itemize}

\item  \emph{Return values}: A set of pair \texttt{[resource{\textunderscore}id, [class|instance|object{\textunderscore}property|datatype{\textunderscore}property]]}.
\end{itemize}

\item  \texttt{\textbf{about}(\emph{string} resource, [\emph{string} model]))}
\begin{itemize}
\item  Returns the list of asserted (and if available inferred) statements which the resource is part of.
\item  \emph{Params}:
\begin{itemize}
\item  [string] the resource to look for.
\item  \emph{opt.} [string] the model ID to look in. If omitted, the resource is looked for in all models. 
\end{itemize}

\item  \emph{Return values}: A set of statements.
\end{itemize}

\item  \texttt{\textbf{exist}(\emph{set$<$[statement|partial{\textunderscore}statement]$>$} pattern, [\emph{string} model]))}
\begin{itemize}
\item  Checks that the given pattern matches content in the ontology. If statements, returns true if all the statements are present in the knowledge base (asserted or inferred), if partial statements, returns true if at least one statement match the conjunction of the partial statements.
\item  \emph{Params}:
\begin{itemize}
\item  [set$<$[statement|partial{\textunderscore}statement]$>$] the pattern to check.
\item  \emph{opt.} [string] the model ID to look in. If omitted, the resource is looked for in all models. 
\end{itemize}

\item  \emph{Return values}: A boolean.
\end{itemize}

\item  \texttt{\textbf{check}(\emph{set$<$statement$>$} statements, [\emph{string} model]))}
\begin{itemize}
\item  Checks that the given statements are consistent with the knowledge base. Statements are not added to the knowledge base.
\item  \emph{Params}:
\begin{itemize}
\item  [set$<$statement$>$] the statements to test.
\item  \emph{opt.} [string] the model ID to look in. If omitted, the resource is looked for in all models. 
\end{itemize}

\item  \emph{Return values}: A boolean. \texttt{true} if statements do not contradict with the knowledge base.
\end{itemize}

\item  \texttt{\textbf{find}(\emph{set$<$string$>$} named{\textunderscore}variables, \emph{set$<$partial{\textunderscore}statement$>$} pattern, [\emph{set$<$string$>$} constraints], [\emph{string} model]))}
\begin{itemize}
\item  Retrieves ressources given a set of partially defined statements plus optional constraints about this resource. Constraints must follow the \href{http://www.w3.org/TR/rdf-sparql-query/\#tests}{ SPARQL syntax for filters}. \texttt{named{\textunderscore}variables} defines the set of variables whose bindings are looked for. Probabilities can be retrieved as well.
\item  Examples: 
\small
\begin{verbatim}

Assuming the following facts in the knowledge base:
'default' model -> cup1 type Cup, cup1 hasColor blue, knife0 type Knife, (human1 uses cup1):0.23,
(human1 uses knife0):0.46
'human1' model -> cup32 type Cup, cup32 hasColor blue

find (["obj"], ["rdf:type(?obj, Cup)", hasColor(?obj, 'blue')"]) # simple query, in every models
 -> {["obj"]:["cup1", "cup32"]}
find (["obj"], ["rdf:type(?obj, Cup)", hasColor(?obj, 'blue')"], [], "default"]) # simple query, in
the ''default'' model
 -> {["obj"]:["cup1"]}
find (["obj", "p1"], ["uses(?obj, human1):p1", hasColor(?obj, 'blue')"]) # retrieving probabilities
 -> {["obj", "p1"]:[["cup1", 0.23]]}
find (["t", "p1"], ["uses(?obj, human1):p1", rdf:type(?obj, ?t)"]) # retrieving probabilities
 -> {["t", "p1"]:[["Cup", 0.23], ["Knife", 0.46]]}

\end{verbatim}
\normalsize

\item  \emph{Params}:
\begin{itemize}
\item  [string] the name of the variable to identify, as used in the statements.
\item  [set$<$partial{\textunderscore}statement$>$] pattern build from partial statements.
\item  \emph{opt.} [set$<$partial{\textunderscore}statement$>$] pattern build from partial statements.
\item  \emph{opt.} [string] the model ID to look in. If omitted, the resource is looked for in all models. 
\end{itemize}

\item  \emph{Return values}: A set of resources.
\end{itemize}

\item  \texttt{\textbf{findmpe}(\emph{string} named{\textunderscore}variable, \emph{set$<$partial{\textunderscore}statement$>$} pattern, [\emph{set$<$string$>$} constraints], [\emph{string} model]))}
\begin{itemize}
\item  Retrieves ressources within the \emph{Most Probable Explanation} (\emph{ie} the most likely current state of the world in the given model). For implementation not supporting probabilistic reasoning, \texttt{findmpe} is strictly equivalent to \texttt{find}. See \texttt{find} for details about the use of the request.
\item  \emph{Params}:
\begin{itemize}
\item  [string] the name of the variable to identify, as used in the statements.
\item  [set$<$partial{\textunderscore}statement$>$] pattern build from partial statements.
\item  \emph{opt.} [set$<$partial{\textunderscore}statement$>$] pattern build from partial statements.
\item  \emph{opt.} [string] the model ID to look in. If omitted, the resource is looked for in all models. 
\end{itemize}

\item  \emph{Return values}: A set of resources.
\end{itemize}

\end{itemize}

\subsubsection{Managing models}


\begin{itemize}
\item  \texttt{\textbf{models}()}
\begin{itemize}
\item  Returns the set of current available models. See also \hyperref[ac5552fd6a3c08ad22387efbe42d137d]{models}
\item  \emph{Params}: None
\item  \emph{Return values}: a set of all model ids.
\end{itemize}

\item  \texttt{\textbf{addmodel}(\emph{string} id)}
\begin{itemize}
\item  Adds a new knowledge model to the knowledge base.
\item  \emph{Params}:
\begin{itemize}
\item  [string] the model ID to add.
\end{itemize}

\item  \emph{Return values}: None.
\end{itemize}

\item  \texttt{\textbf{removemodel}(\emph{string} id)}
\begin{itemize}
\item  Removes a knowledge model from the knowledge base.
\item  \emph{Params}:
\begin{itemize}
\item  [string] the model ID to remove.
\end{itemize}

\item  \emph{Return values}: None.
\end{itemize}

\item  \texttt{\textbf{isconsistent}([\emph{string} model])}
\begin{itemize}
\item  Checks that the knowledge base is globally consistent.
\item  \emph{Params}:
\begin{itemize}
\item  \emph{opt.} [string] the model ID to check. If omitted, all models are checked and \texttt{true} is answered only if all models are consistent. 
\end{itemize}

\item  \emph{Return values}: A boolean. \texttt{true} if the model is consistent.
\end{itemize}

\item  \texttt{\textbf{addrules}(\emph{set$<$string$>$} rules, [\emph{string} model])}
\begin{itemize}
\item  Add rules to the model. See \hyperref[a4f86f7bfc24194b276c22e0ef158197]{rules} for examples.
\item  \emph{Params}:
\begin{itemize}
\item  [set$<$string$>$] a set of rule to add to the model.
\item  \emph{opt.} [string] the model ID that receive the rules. If omitted, rules are added to all models. 
\end{itemize}

\item  \emph{Return values}: None.
\end{itemize}

\end{itemize}

\subsubsection{Taxonomy walking}

\fxwarning{overlaps with the \texttt{about} method {$\rightarrow$} useful? add property listing?}


\begin{itemize}
\item  \texttt{\textbf{classesof}(\emph{string} instance, [\emph{bool} direct], [\emph{string} model]))}
\begin{itemize}
\item  Returns the (asserted and if available, inferred) classes of an instance.
\item  \emph{Params}:
\begin{itemize}
\item  [string] the instance id to look for.
\item  \emph{opt.} [bool] if true, only direct types are returned. If omitted, \texttt{false} is assumed 
\item  \emph{opt.} [string] the model ID to look in. If omitted, the classes are searched in all models. 
\end{itemize}

\item  \emph{Return values}: A set of resource ids.
\end{itemize}

\item  \texttt{\textbf{instancesof}(\emph{string} class, [\emph{bool} direct], [\emph{string} model]))}
\begin{itemize}
\item  Returns the (asserted and if available, inferred) instances of a class.
\item  \emph{Params}:
\begin{itemize}
\item  [string] the class id to look for.
\item  \emph{opt.} [bool] if true, only direct instances are returned. If omitted, \texttt{false} is assumed 
\item  \emph{opt.} [string] the model ID to look in. If omitted, the instances are searched in all models. 
\end{itemize}

\item  \emph{Return values}: A set of resource ids.
\end{itemize}

\item  \texttt{\textbf{subclassesof}(\emph{string} class, [\emph{bool} direct], [\emph{string} model]))}
\begin{itemize}
\item  Returns the (asserted and if available, inferred) sub-classes of a class.
\item  \emph{Params}:
\begin{itemize}
\item  [string] the class id to look for.
\item  \emph{opt.} [bool] if true, only direct sub-classes are returned. If omitted, \texttt{false} is assumed 
\item  \emph{opt.} [string] the model ID to look in. If omitted, the sub-classes are searched in all models. 
\end{itemize}

\item  \emph{Return values}: A set of resource ids.
\end{itemize}

\item  \texttt{\textbf{superclassesof}(\emph{string} class, [\emph{bool} direct], [\emph{string} model]))}
\begin{itemize}
\item  Returns the (asserted and if available, inferred) super-classes of a class.
\item  \emph{Params}:
\begin{itemize}
\item  [string] the class id to look for.
\item  \emph{opt.} [bool] if true, only direct super-classes are returned. If omitted, \texttt{false} is assumed 
\item  \emph{opt.} [string] the model ID to look in. If omitted, the super-classes are searched in all models. 
\end{itemize}

\item  \emph{Return values}: A set of resource ids.
\end{itemize}

\end{itemize}

\subsection{API Extensions}


Implementations are free to provide extensions through the \texttt{special} method.


\subsection{Partial implementations}


Softwares implementing the API may not support some features (like storage of probabilities, management of different models, etc.).

If a method is not implemented at all, the server is expected to return an error with the JSON-RPC standard error code \texttt{-32601 Method not found}.

If the requested method exists, but the an essential feature that prevent the execution of the method is missing, the implementation should return an error with the code \texttt{-31000 Feature not implemented} and provide details in the error message.

Example of server not supporting several models in a \texttt{addmodel} query with the JSON-RPC protocol:



\small
\begin{verbatim}

{"jsonrpc": "2.0", "method": "addmodel", "params": "human_model", "id": 1}
{"jsonrpc": "2.0", "error": {"code": -31000, "message": "Feature not implemented", "data": "No
support for several models"}, "id": 1}

\end{verbatim}
\normalsize
If a method is partially implemented and does not prevent the processing of the request (like not handling probabilities attached to statements), implementations are advised to return a warning (that may be safely ignored by the client) along with the request result. Implementations are still expected to correctly parse any request and request arguments as described in the API.

For instance, an implementation that does not handles probabilities could treat all probabilities as equal to 1.0, and add a \texttt{{"notsupported": "probabilities"}} to resultsets.

Currently, the following \texttt{notsupported} flags are defined:

\begin{tabular}{ll}
\hline
\texttt{notsupported} flag & Meaning \\ 
\hline
probabilities & Probabilities are not supported \\ 
\hline
\end{tabular}

Example of server not supporting probabilities in a \texttt{add} query with the JSON-RPC protocol:



\small
\begin{verbatim}

{"jsonrpc": "2.0", "method": "add", "params": [["my_cup filledWith coffee", 0.45]], "id": 1}
{"jsonrpc": "2.0", "result": {"notsupported": "probabilities"}, "id": 1}

\end{verbatim}
\normalsize

\subsection{Notes}
\subsubsection{TBox alteration}

(for background on the disctinction between ABox and TBox, cf \href{http://en.wikipedia.org/wiki/Description_logic\#Modeling}{ Wikipedia}. In two words, the TBox is the model structure, including the taxonomy, while the ABox is the instances).

While not enforceable at the API level, modification of the TBox through the \texttt{add}, \texttt{remove} and \texttt{update} methods is strongly discouraged, since most of the underlying implementations are likely not to support it.

For instance \texttt{add ["rdfs:subClassOf(Cup, Tableware)"]} is not recommended because it changes the model's taxonomy.

To alter the model's TBox, the method \texttt{addrule} can be used instead. The previous example would translate into: \texttt{addrule "Cup(?x){$\rightarrow$}Tableware(?x)"}.


\subsection{Discussion on design issues}


\subsubsection{General}


\begin{itemize}
\item  Balance between ease of use/understand and completeness
\item  Should we specify a communication protocol (for instance JSON-RPC)?
\item  API = specification only, or as well a "middle-layer" that convert to a common, unique representation?
\end{itemize}

\subsubsection{Expressiveness}


\begin{itemize}
\item  Should we be more specific on the required expressiveness?
\item  What are the pros/cons of not having requirements?
\item  Do we (and how much do we) limit research on knowledge representation system by designing this API?
\end{itemize}

\subsubsection{Probabilities}


\begin{itemize}
\item  At statement level? At set of statements level? at model level?
\end{itemize}

\subsubsection{Extensibility}


\begin{itemize}
\item  Policies can easily be extended
\item  method \texttt{special} allows implementation specific extensions
\end{itemize}
-


\subsection{Other ideas, out of the API}


\begin{itemize}
\item  events ?
\begin{itemize}
\item  registerEvent(String, String, String, List): registers an event. Expected parameters are: type, triggering type, variable, event pattern.
\item  registerEvent(String, String, List): registers an event. Expected parameters are: type, triggering type, event pattern.
\end{itemize}

\end{itemize}

\begin{itemize}
\item  SPARQL ?
\begin{itemize}
\item  sparql(String, String): performs one SPARQL query on the ontology
\end{itemize}

\end{itemize}



