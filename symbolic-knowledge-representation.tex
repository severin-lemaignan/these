\chapter{Symbolic knowledge representation}

\fxnote{Support material: \emph{What is a knowledge representation} by Davis,
Shrobe and Szolovits,
\url{http://groups.csail.mit.edu/medg/ftp/psz/k-rep.html}}

\section{What do we call "knowledge"?}
\label{sect|on-knowledge}


Since we will discuss at length the concept of knowledge in the context
of robotics in the coming pages, it is useful to make our terminology explicit.  No general
agreement on a definition of ``knowledge'' exists. In our context, we call
``knowledge'' \emph{an explicit set of interrelated logical facts that are
meaningful to the robot executive controller} (by \emph{meaningful} we mean
that can possibly be interpreted to lead to a purposeful action).

The relation of \emph{data} and \emph{information} to knowledge is a debated
epistemology question (the interested reader is invited to refer to the
Wikipedia page on the ``DIKW'' hierarchy). In this thesis, we will associate
data to low-level material like raw sensor output, and information to
uncontextualized symbolic facts.

To give a example, image a human watching at a book and being tracked by a
Kinect sensor: the pose of the human skeleton in the world would be the data,
the fact \concept{looksAt(human, book)} as computed by a geometric reasoning
module would be the information, the fact \concept{looksAt(john,
war\_and\_peace)}, fully grounded and connected to the whole knowledge base of
the robot would be proper knowledge.

Note that the origins of AI where in purely symbolic models, that were
not usable either. Rich interleaving between geometric reasoning and symbolic
models is required.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Typology of Knowledge Representation Systems Requirements for Robotics}
\label{sect|features}

We presented an imaginary cooking scenario where numerous desirable
features for a knowledge representation system for robotics have been
identified in no particular order.

This section proposes a more formal typology of desirable features for a such a
system. For each feature, we provide a short definition along with links to
relevant literature.

Table~\ref{table|contribution-by-systems}, at the end of the article,
summarizes all these features with the main domain of contribution of each
surveyed KRS.

\subsection{Expressiveness: What Can be Represented?}
\label{sect|expressiveness}

\subsubsection{Introduction: Main Logic Formalisms}

The main role of a knowledge representation system is to provide an adequate
representation system to store facts and concepts that can be informally
described in natural language.

Formal logic aims at providing such a representation system with the added
value of providing a tractable support for inference and reasoning.

Most (but not all) of the systems we survey rely on a particular logic
formalism. The choice of the formalism has a strong impact, on one side, on the
range of ideas that can be expressed conveniently (\emph{practical
expressiveness}) or at all (\emph{theoretical expressiveness}), on the other
side, on the ability to solve the inference problem (called
\emph{satisfiability}: is a given logical sentence true in my model?) in a
tractable manner.

A large number of logic formalism do exist, we shall summarize below the most
relevant for systems actually deployed in current robotic architectures.

\emph{Predicate logic} is the family of logic formalisms the most commonly
found in knowledge representation. It distinguishes itself from the simpler
\emph{propositional logic} by the use of quantification to increase generality.
\emph{First-order logic} (FOL) is the subpart of \emph{predicate logic} where the
objects of \emph{predicates} (or \emph{formulae}) are simple \emph{terms},
while in \emph{higher-order logics}, predicates can be themselves objects of
other predicates.

\emph{Horn clauses} are an important subset of FOL because the satisfiability
of a set of such clauses is a $P$-complete problem. A Horn clause is a
disjunction of literals (a \emph{clause}) with at most one positive literal:
$\neg p \vee \neg q \vee \cdots \vee \neg t \vee u$, which can also be
represented as $(p \wedge q \wedge \cdots \wedge t) \rightarrow u$.  Important
logic programming languages like Prolog are based on Horn clauses.

The family of \emph{Description Logics}~\cite{Baader2008} also play an
important role. It is also a subset of the first-order logic, with some
extensions in second-order logic. Description logics are notable because most
of them are known to be decidable. In description logic, axioms are build from
\emph{concepts}, \emph{roles} (that are unary or binary predicates) and
\emph{individuals}. The W3C OWL-DL standard is a widely-used language to
describe domains with the description logic.

Because Description Logics have been originally created from the perspective of
a \emph{knowledge representation language} and not a logic language, their
terminology (\emph{concept} or \emph{class}, \emph{role} or \emph{property},
\emph{individual},...) is well-suited to knowledge description and we may use
it in the remaining of this paper outside of the strict context of  Description
Logics.

\emph{Modal logic}, that allow for statement qualification like
\emph{possibility} or \emph{necessity}, have been shown to be closely related
to description logics. Modal logic allows to represent conveniently parallel
possible worlds and facts like ``the robot knows \emph{that the human knows}
how to read a recipe''.

\fxfatal{On modal logics, see the remark of McCarthy, in \cite{McCarthy2007}, section 3}

\emph{Temporal logic} are designed to represent and manipulate assertions whose
truth value may vary in time.

One last class of logics that is of particular relevance for robotic
applications is the \emph{probabilistic logics} or \emph{Bayesian logics}.
These logics provide a formal framework to reason on propositions whose truth
or falsity is uncertain. We elaborate below on the representation of uncertainty.

Note that most of these logic formalisms are still active research field by
their own, and practical considerations (especially the availability of
reasoners efficient enough for on-line use on a robot) often constraint the
choice of a logical formalism and a level of expressive power.


\paragraph{Some examples}

\emph{The robot knows that a blue bottle is laying on the table.}

\emph{The robot knows that the human knows about the position of the bottle,
but the robot does not know what the human actually know about it.}

\subsubsection{Expressive Power}

Logical formalisms each bring a certain level of expressive power. For
instance, the following classical syllogism can not be represented in
propositional logic because of the use of \emph{universal quantification}:

\begin{quote}
\begin{enumerate}
    \item All men are mortal,
    \item Socrates is a man,
    \item Therefore, Socrates is mortal
\end{enumerate}
\end{quote}

However, the following weak version of the syllogism can be represented in
propositional logic:

\begin{quote}
\begin{enumerate}
    \item If Socrates is a man, then Socrates is mortal,
    \item Socrates is a man,
    \item Therefore, Socrates is mortal
\end{enumerate}
\end{quote}

Generally speaking, expressive power comes at the cost of more complex
\emph{satisfiability} and \emph{consistency}\footnote{We precise these concepts
at section~\ref{sect|reasoning}.} computations, possibly leading to
untractable, if not undecidable (\ie systems where it is proven that a
proposition can not be decided to be true or false) problems.

\begin{figure}
    \centering
    \includegraphics[width=0.6\columnwidth]{typology/expressive_overlap_dl_horn.pdf}
    \caption{expressiveness overlap of Description Logics and logic programs
    based on Horn clauses, taken from~\cite{Grosof2003}}
    \label{fig|overlap_dl_horn}
\end{figure}

Figure~\ref{fig|overlap_dl_horn} shows that the expressive power of description
logics and Horn clauses partially overlaps. In section~\ref{sect|reasoning} we
mention extensions to description logics based on rule systems to bring closer
the two approaches.

The relationships between expressive power and reasoning complexity that follow
has been extensively studied for Description Logics.
Zolin~\cite{ZolinDLComplexityNavigator} maintains a ``complexity navigator''
that allows to conveniently explore these relationships and indexes most of the
literature on that subject.

\subsubsection{Open World and Close World Assumptions}

The \emph{close world} (CWA) vs. \emph{open world} (OWA) assumption names a
modelling choice on the \emph{completeness} of a knowledge domain. In the close
world assumption, a proposition that can not be proven true is assumed to be
false (\emph{negation by failure}), while in the open world assumption, a
proposition may be considered either true, false or unknown.

This distinction is important in robotics were the robot may have to manipulate
concepts with only partial knowledge on them. For instance, let imagine a robot
that sees a bottle on a table, whose bottom is hidden by another object. The
robot can not prove that the bottle is indeed \emph{on} the table. A knowledge
representation system relying on the closed world assumption would then assume
the bottle is \emph{not} on the table ($\lnot R^{CWA}_{isOn}(bottle, table)$)
whereas with the open world assumption, the proposition $R^{OWA}_{isOn}(bottle,
table)$ would be undecided. Example in table~\ref{table|cwa-owa-example} provides
a simple, concrete example of consequences of the CWA/OWA choice on reasoning.

\begin{table}
	\begin{center}
	\begin{tabular}{ll}
	{\bf Action} & {\bf Part involved} \\
	\hline
	{\tt PickSoftly} & hand \\
	{\tt PickAndPlace} & arm, hand \\
	{\tt MoveArm} & arm \\
	\hline
	\end{tabular}
	\end{center}
	\caption{Assuming the question is: \emph{select actions that do not require
	to move the arm}, a CWA reasoner would return {\tt PickSoftly} whereas an
	OWA reasoner would not return anything if the {\tt PickSoftly} action is
	not explicitly said not to involve the arm.}
	\label{table|cwa-owa-example}
\end{table}

Domains constrained with the closed world assumption lead to more tractable
inference problems, and allow for instance the use of logic languages like
Prolog. Thus, several approaches exists to \emph{locally close} a domain (\cf
Levesque~\cite{Levesque2008}, section 24.3.2 for a summary of those).


\subsubsection{Representation of uncertainty and likelihood}

Sources of uncertainty for a robot are two-fold: uncertainty \emph{intrinsic}
to facts (like \emph{``It may rain tomorrow''}), uncertainty caused by
imperfect perception of the world (\emph{``Is the bottle really on the
table?''}). Most logics do not account explicitly for uncertainty. It must be
either relied on specific logics (like Bayesian logics) or on extensions of
classical logics.

\subsubsection{Meta-cognition: knowledge on the knowledge}

As stated by Josyula and Raja, meta-cognition is composed of both
\emph{``meta-level control of cognitive activities and the introspective
monitoring of such activities to evaluate and to explain them"}.

A knowledge representation system endowed with \emph{meta-cognition} is not
only able to manipulate knowledge but also to exhibit and manipulate the
structure of its knowledge and the reasoning process. For instance, the ability
to explain a logical inconsistency in a KRS is a meta-cognitive function.
Sloman~\cite{Sloman2011}

At section~\ref{sect|introspection} below, we discuss the idea of
introspection.  Meta-cognition can be viewed as the technical facet of the
introspection in general.


%%%%%%%%%%
\subsection{How things are represented?}
\label{sect|higher-level-domain-representation}

\subsubsection{Role Representations}

Spatio-Temporal Representations:

\paragraph{Representation of time}

As an agent acting at human-like time scale and dealing with temporal concepts
(like actions), a robot may want to represent, and possibly to reason, about
time. Time representation is split into two distinct abilities: representing
time points (both in the past -- which is roughly equivalent to assignment of
timestamps to events the robot perceives -- and in the future), and
representing \emph{passing time} (durations, timespans) like in \emph{``the
eggs will be cooked in 10 min''}.

\fxfatal{Discuss time chronicles~\cite{Ghallab1996}}

We call a system that do not account for time (\ie that permanently lives in
present) \emph{atemporal}.

\paragraph{Representation of space}

\paragraph{Representation of events and actions}

\subsubsection{Context modeling}

\emph{Knowledge is contextualized information}\fxfatal{Find someone respectable
how said that :-)}: it is essential for the robot to associate the facts it
represents to a \emph{context}. The context carries the keys for the
interpretation of the information. It covers the \emph{domain of validity} of
the facts, the \emph{common-sense} knowledge required to fill the gaps in the
representation\fxfatal{give an example}, \fxfatal{What more?}.

\subsubsection{Possible-Worlds and representing what others know}
\label{sect|possible-worlds}

    
Linked to the context representation, but seen from another angle, knowledge
representation systems may provide explicit ways to model other point of view
on the world. This ability is often referred as the \emph{perspective taking}
ability.

\cite{Levesque2008}, p. 4

\subsubsection{Introspection: Who am I? What can I do?}
\label{sect|introspection}

\paragraph{The introspective robot}

Introspection is the ability to self-describe: what are my capabilities, what
is my state (performing some action, idling, etc.), what are my beliefs (\ie my
knowledge), what are my intentions and my plans?

Introspection must be distinguished from meta-cognition: While introspection
may require meta-cognition (for instance to be able to expose its internal
knowledge), it is not always mandatory. The current state of the robot can be
represented as a simple instantiation of a specific category (for instance, if
the robot give an object to the human, this state could be represented with the
triplets \setstmt{robot performs action1, action1 isA Give}.

\paragraph{Modelling of the robot capabilities}

A particularly important aspect of introspection relates to the description of
its own capabilities: which sensors/actuators/computation services exist and
are currently available ?  While at a first level, these descriptions can be
static (\eg the robot has one laser scanner and two arms), at a second level,
the description is updated and reflect the current (and possibly past and
future) state of the robot. Note that these description may also involve
geometric descriptions (a kinematic chain, the pose of a device, etc.) that may
be deported outside of the main knowledge base. Efforts trying to formalize,
maintain and expose the capabilities and state of a robot are not new (and
ground themselves in work and techniques for self-descriptive remote procedure
calls in computing science), but take a renewed importance with applications
for high-level multi-robot cooperation. Recent work in that direction
include~\cite{Kunze2011}.

\subsubsection{Memory}
\label{sect|memory}

Memory has been studied at length in the cognitive psychology and
neuropsychology communities: Atkinson and Shiffrin~\cite{Atkinson1968}
introduce the idea of \emph{short-term} and \emph{long-term} memory,
Anderson~\cite{Anderson1976} splits memory into \emph{declarative} (explicit)
and \emph{procedural} (implicit) memories, Tulving~\cite{Tulving1985} organizes
the concepts of \emph{procedural}, \emph{semantic} and \emph{episodic} memories
into a hierarchy. Short-term memory is refined with the concept of
\emph{working memory} by Baddeley~\cite{Baddeley2010}
(Figure~\ref{fig|memory_models}).

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{typology/memory_models.pdf}
    \caption{Overview of the main types of memories}
    \label{fig|memory_models}
\end{figure}

Most knowledge representation systems offers some kind of memory as a pool of
facts that are not forgotten by the robot until it is halted (this memory is
often referred as a \emph{working memory}, but with a meaning unrelated to
Baddeley's definition). Some systems may propose persistent storages that allow
the robot knowledge to grow over time, while other may offer a larger range of
memory categories, like short term memory (that lasts for a couple of seconds)
or episodic memory (that allows the robot to selectively remember facts
associated to specific events).

In the larger field of cognitive architectures, the {\sc Soar}
architecture~\cite{Lehman2006} is one of those that tries to reproduce a
human-like memory organization.

%%%%%%%%%%
\subsection{Reasoning Techniques}
\label{sect|reasoning}

\subsubsection{Standard reasoning techniques}

Being based on logical languages to represent the knowledge, most of the
systems we survey allow various forms of \emph{reasoning}.

We call \emph{standard reasoning techniques} techniques based on logical
inference, using resolution algorithms like \emph{forward chaining},
\emph{backward chaining} or \emph{semantic tableaux}.

Main reasoning problems include \emph{concept satisfiability},
\emph{consistency checking} and \emph{instance checking}.

Concept satisfiability verifies if it is possible to find a non-empty
\emph{interpretation} of a concept (or an expression defining a concept) in the
knowledge model. For instance, the formula \concept{Plant} $\land$
\concept{isRed}, which defines the concept of red plants, is satisfiable in a
model \concept{KB} iff $\exists a, $ \concept{Plant}$(a) \land$
\concept{isRed}$(a)$, \ie if we can find at least one red plant $a$ in our
model.

Checking the consistency of a model is equivalent to checking the
satisfiability of each of the concept defined in the knowledge model.

Instance checking consists in verifying that an individual $a$ is an
interpretation of a concept (or concept expression) $C$ in the knowledge model.
A typical example would be that we are provided with an instance
\concept{object1} and we want to know if this object is a kind of
\concept{Bottle} or \concept{Glass}.

Reasoners can then provide other type of inferences, like:

\begin{itemize}
    \item class subsumption (similar to inheritance)

    \item reasoning on roles properties, including:
        \begin{itemize}
        \item entailments based on roles domain and range (for instance, if the
        domain of the role \concept{thinksOf} is known to be
        \concept{ThinkingAgent}, then \concept{thinksOf}$(a, b) \to
        $\concept{ThinkingAgent}$(a)$),

        \item universal, existential and cardinality constraints (including \concept{allValue}, 
        \concept{someValue}, \concept{hasValue}),

        \item property characteristics (symmetry, transitivity, etc.)

        \end{itemize}

    \item complex concept expressions like: \par \footnotesize \concept{Bottle}
    $\equiv$ \concept{Artifact} {\bf that} (\concept{hasShape} {\bf value}
    \concept{cylinderShape})\footnote{This example uses the \emph{Manchester
    syntax}, \url{http://www.w3.org/TR/owl2-manchester-syntax/}} \normalsize

    \item set operations like: \par \footnotesize \concept{Color} $\equiv$ {\bf
    unionOf}(\concept{blue}, \concept{green}, \concept{orange},
    \concept{black}...) \normalsize

\end{itemize}

\paragraph{Rule Languages}

As mentioned earlier, knowledge models based on description logics can be extended through rule languages.

Intersection of properties is an example of expression that can only be represented with rules: for instance, 

generic SWRL ({\em Semantic Web Rule Language}) rules like: \par
        \footnotesize \concept{looksAt(?agt, ?obj)} $\land$
        \concept{pointsAt(?agt,?obj)} \par $\Rightarrow$ \concept{focusesOn(?agt, ?obj)}
        \normalsize 


\subsubsection{Alteration of the knowledge structure}

In systems

All systems allow to modify the ABox, not always possible to alter the TBox


\subsubsection{Lazy evaluation}
\label{sect|lazy-evaluation}


\subsubsection{Reasoning with uncertainty}


\subsubsection{(Non) Monotonic Reasoning}

\emph{Monotonic reasoning} means that addition of new assertions to a knowledge base
can only extend the set of assertions that can be inferred, while a
\emph{non-monotonic} reasoning scheme may lead to retraction of facts.
McCarthy coined a famous example to illustrate the need of non-monotonic reasoning:

\begin{quotation}
Consider putting an axiom in a common sense database asserting that birds can
fly. Clearly the axiom must be qualified in some way since penguins, dead birds
and birds whose feet are encased in concrete can't fly. A careful construction
of the axiom might succeed in including the exceptions of penguins and dead
birds, but clearly we can think up as many additional exceptions like birds
with their feet encased in concrete as we like. Formalized non-monotonic
reasoning provides a way of saying that a bird can fly unless there
is an abnormal circumstance and reasoning that only the abnormal circumstances
whose existence follows from the facts being taken into account will be
considered.
\end{quotation}

Another important application of non-monotonic reasoning is representation of
change: for example, to make an omelette, you need to crack eggs and wipe them.
The eggs disappear and are replaced by an omelette:

\concept{Egg}$(a) \wedge $ \concept{Egg} $(b) \wedge $
\concept{MakeOmelette}$(a, b, c) \to \lnot $ \concept{Egg}$(a) \wedge \lnot $
\concept{Egg}$(b) \wedge $ \concept{Omelette}$(c)$

The insertion of the proposition \concept{MakeOmelette}$(a, b, c)$ leads to
retraction of other facts. This rule requires non-monotonic reasoning to be
applied.

\emph{Default logic} is one of the formal logic that account for representing
general truth and exceptions to it (for instance, \emph{tomatoes are red, in
general}). However, due to computational complexity of these model (most of
inferences in default logic are known to be $NP$-complete problem), classical
logics and most of the existing reasoners do not allow non-monotonic reasoning.
For instance, the SWRL rule language, usually associated to the OWL-DL ontology
language, do not allow non-monotonic reasoning.

\fxfatal{Make clear 'who does not allow non-monotonic-reasoning': logics? rule
languages? reasoner?}

One important exception is the \emph{negation as failure} inference rule, as
implemented by {\sc Prolog} for instance, that allows for non-monotonicity, but
only \emph{within the closed world assumption}.

\fxfatal{Give here an example of non-monotonic reasoning with Prolog}
\fxwarning{Do we mention here Answer Set Programming?}

A monotonic system does not theoretically allow for knowledge retraction,
which is an important issue in the robotic context where the world model is
likely to be often altered.  However it is a practical issue only if the
reasoning process is \emph{continuous} during the whole robot's activity
lifespan. It is often possible to stop the reasoner, alter the knowledge, and
restart the inference process on a new domain.
\fxfatal{Rephrase to emphasize that when new evidences appear, it is anyway often a
good idea to restart the reasoner.}

\fxfatal{Mention that the 'change of world' issue can also be dealt with appropriate
time representation.}
\fxfatal{Mention that probabilistic reasoning lead to implicit non-monotonic reasoning}


\subsubsection{Presupposition Accommodation}
\label{sect|presupposition-accommodation}

\emph{Presupposition accommodation} is the ability for the system to
automatically create a context allowing to make sense of a proposition.

For instance, we can imagine a human telling a robot \emph{Please get me the
bottle that is behind you}. If the robot has not yet see what is behind it, it
needs to assume (and represents in its knowledge model) that a undefined bottle
can be found somewhere in the half of space behind it.

A knowledge representation system able to cope with presupposition
accommodation would be able to take into account this (usually under-defined)
information that is not grounded into perception for later inferences.

This ability to imagine a physically state of the world that is not actually
perceived can be seen as the converse of the grounding ability.

Note also that presupposition accommodation implies a bidirectional link of the
symbolic knowledge model with a geometric (or physical) model of the
environment. This article focuses on symbolic knowledge representation systems,
but we shall mention when a KRS explicitly provides support for presupposition
accommodation.

\subsubsection{Prediction, projection and diagnosis tasks}
\label{sect|prediction-projection}

Levesque~\cite{Levesque2008} distinguish two main tasks, the \emph{projection
task} and the \emph{legality task}.

\paragraph{Projection task}: determining whether or not some condition while
hold after a sequence of actions.

\paragraph{Legality task}: determining whether a sequence of action can be
performed starting in some initial state.

\paragraph{Diagnosis}: this corresponds to the ability to rewind on past events
in case of failure to provide possible explanation. This can be seen as the
temporal reverse of the projection task.

\subsubsection{Planning}
\label{sect|planning}

Making decision based on prediction

\subsubsection{Physics-based reasoning}
\label{sect|physics}

As embodied entities, robots have to interact with physical entities.
\emph{Naive physics reasoning} covers all the everyday reasoning the humans
unconsciously perform, like taking into account gravity (``if I drop a ball, it
falls down'') or common physical properties of objects (``a glass may break if
dropped'', etc.). Many of the interactions with our everyday environments are
ruled by such laws that are difficult to exhaustively encode.

Some systems \cite{Kunze2011a} rely on external dedicated physics engine to
compute symbolic facts from on-demand physics simulation. This, however,
requires a tight integration between the symbolic model and a geometric model
that carries the geometries and physical properties of objects.

\subsubsection{Learning}
\label{sect|learning}

%%%%%%%%%%%%%%%%%
\subsection{Acquiring Knowledge}

\subsubsection{Knowledge acquisition and modalities merging}
\label{sect|knowledge-acquisition}

In the survey inclusion criteria, we have insisted on only considering robotic
systems that acquire knowledge by themselves, during their runtime.

\emph{Acquiring knowledge} means in our context building new statements
(usually as new logical facts) connected to our existing knowledge from
external sources of information. We consider mainly three of them:
proprioceptive/exteroceptive sensors, interaction with other agents, humans or
robots, and remote databases. This process has generally at least two steps:
the information acquisition by itself, and the \emph{transformation} of the
information into knowledge, \emph{aligned} with the robot existing model
(following our terminology for information and knowledge, as discussed in the
introduction).

Knowledge acquisition is generally not done directly in the knowledge
representation system. On the contrary, several, if not numerous, external
components are usually required to convert percepts into symbolic facts and to
ground them.

While we do not review in this article all these systems, the whole process of
knowledge acquisition is central in cognitive robotic architecture and the
design of knowledge representation systems can influence or be influenced by
the approach to knowledge acquisition.

In particular, complex robotic systems often require multi-modal perception
capabilities (for instance, a robot can only interpret an utterance like ``this
is a plate'' if it is able to understand gestures, understand natural language
and merge them in a timely manner). Multi-modal interpretation can take place
at various levels, but in many cases (especially if the modalities are of very
different natures, like in the example above) merging will require
symbolic-level reasoning. The KRS has a direct impact on the feasibility and
ease of such operations.

\subsubsection{Grounding/anchoring strategies}
\label{sect|grounding}

\emph{Grounding} (also called \emph{anchoring} when specifically referring to
building links between \emph{percepts} and \emph{physical
objects}~\cite{Coradeschi2003}) is the task consisting in building and
maintaining a bi-directional link between sub-symbolic representations (sensors
data, low-level actuation) and symbolic representations that can be
manipulated and reasoned about~\cite{Harnad1990}.

Being embodied entities with interaction with other embodied entities as a
fundamental requirement, robots and robotic is deeply concerned by the
grounding issue.

Being actually implemented on real service robots, all the symbolic knowledge
representation systems that we review in this study have some kind of grounding
process. Numerous approaches exist, like amodal
\emph{proxies}~\cite{Jacobsson2008}, grounded amodal
representations~\cite{Alami2011, Mavridis2006}, semantic maps
(Figure~\ref{fig|semanticmap}, \cite{Nuechter2008, Galindo2008,Blodow2011}) or
affordance-based planing and object classification~\cite{Lorken2008,
Varadarajan2011}.

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{typology/semanticmaps_hertzberg.png}
    \caption{Example of a semantic map, taken from~\cite{Nuechter2008}.}
    \label{fig|semanticmap}
\end{figure}

\subsubsection{Ability to automatically create new object instances}
\label{sect|new-instances}

%%%%%%%%%%%%%%%%%
\subsection{Practical Integration in Robotic Architectures}
\label{sect|integration-robot}

Knowledge representation systems do not mean anything to robots if they are
considered in isolation. This section proposes categories of features related
to the integration of the KRS into a larger software architecture that includes
perception routines, decision-making processes and actuation control.

We also mention some practical aspects of a real-world system, like
performances and monitoring tools that come along with the KRS.

\subsubsection{Integration with sensori-motor layers}
\label{sect|integration-sensorimotor}

We have previously discussed (section~\ref{sect|grounding}) the principles of
the grounding process that aims at establishing and maintaining a connection
between percepts (and to a lesser extend, low-level actions) and symbols.

While every real-world cognitive robot need some kind of grounding, the actual
implementations lead to very different information flows.

The systems can be roughly split into two classes: \emph{passive} knowledge
repositories that process symbolic facts produced by lower-level sensori-motor
layers (\emph{push} flow); \emph{active} knowledge managers that directly query
(possibly by polling or on-demand) low-level layers.

This macroscopic distinction is however mostly a matter of defining the
frontiers of the KRS: some systems like KnowRob~\cite{Tenorth2009a} encompass
geometric reasoning layers that would be considered as external by other
systems like ORO~\cite{Lemaignan2010} that focus on the symbolic fact storage
and rely on a ecosystem of independent modules to provide and consume symbolic
knowledge.

\fxwarning{Some archi with the ability to ``listen'' to the robot internal
structures -> which ones?}

Other systems do not fit either in such a partition between active and passive
systems because they do not stand as independent modules but exist as diffuse,
\emph{ubiquitous} knowledge manipulation system~\cite{Jacobsson2008}, for
instance because they are primarily language~\cite{Ferrein2008, Sabri2011}.

\subsubsection{Integration with executive layers}
\label{sect|integration-executive-layers}

Conversely, the knowledge management module need a tight integration with the
decision-making processes. As for the integration with sensori-motor
layers, the borders of the KRS can be fuzzy and vary from one architecture to
another: many consider symbolic task planning as an integral role of the KRS,
while other have dedicated extensions for planning, some integrate learning as
an on-the-flight process that is part of the KRS, others as an independent
deliberative process, etc.

The actual integration techniques vary also widely, from language extensions
(like the integration of CRAM~\cite{Beetz2010} with KnowRob) and client-server
architectures, to event-driven models (SHARY and ORO~\cite{Alami2011}). Choices
at this level have notable consequences on the whole design of the upper
control architecture of the robot, in particular regarding its modularity and
the ease of addition of new components.

\subsubsection{Language Processing}
\label{sect|nlp}

The scope of this survey is service robotic, which suggest human-robot
interaction. While numerous modalities of communication between humans and
robots do exist and have been extensively studied, the most challenging of
these interactions is natural, bidirectional, unconstrained verbal
communication. Kruijff et al. provides in~\cite{Kruijff2010} an up-to-date
survey of literature on situated human-robot dialogue, focusing on formal
representation systems, bi-directionality of the interaction and context
building.

Natural Language Processing (NLP) is a large research field by itself, and
while the robotic community may be lagging behind on many theoretical aspects,
it brings one important aspect: the embodiment. Because the interactors, both
the robot and the human, are establishing a communication within a shared
physical context, the verbal communication channel is complemented by deictic
channels, back channels and possibly shared physical experiences: a human can
show something to a robot, saying ``Give me this''. This is simply not possible
for a virtual agent.

Several of the knowledge representation systems we present have developed
specific built-in mechanisms or extensions to parse, ground and possibly
rebuild natural language.

\subsubsection{Monitoring and debugging}
\label{sect|debugging}

It is common to have knowledge representation systems at the heart of a
cognitive robotic architecture, and therefore KRS are easily ``burried'' in the
system.

At the same time, the symbolic model often provides a valuable synthetic view
on the whole state of the robot, furthermore easily understandable by the human
developer (the fact \stmt{human1 isSitting true} is easier to interpret than
the suite of relative coordinates of each joints of the human skeleton, as
provided by the human tracker, for instance).

That is the reason why having at hand good tools to trace and visualize at
run-time the evolution of the knowledge structure and contents, as well as
post-processing tools that can be run on the trace to precisely analyze the
cognitive behaviour of the robot, is useful.

\subsubsection{Evaluation of performances}
\label{sect|performances}

Benchmarks of symbolic systems for robots are notoriously hard to conduct for
several reasons: identifying good metrics for robotic experiments in general is
difficult because of the complex interactions between tenth of modules running
in parallel. Isolating the role of one specific component is thus hard.
Furthermore, knowledge representation systems are usually tightly connected to
numerous other modules, and the lack of standard API for knowledge services
makes it especially hard to switch between KRS to compare them. Also, because
service robots are design to act in rich, dynamic environments, possibly with
humans, building repeatable experiments is challenging.

Cognitive architectures and KRS in particular are consequently often
benchmarked with synthetic datasets (which leads to other issues: how to assess
the meaningfulness of the performance of a reasoner on an artificial
ontology?~\cite{Bail2010}) or \emph{toy} experiments~\cite{Chong2009} that do
not reflect real-world complexity.

While evaluating the performances of knowledge representation systems
themselves may be difficult, the cognitive performances of the robot as a whole
can be aptly evaluated through ``classical'' tests from the psychology (like
False-Belief experiments~\cite{Leslie2000} or the Token
test~\cite{DiSimoni1978}).

However, the performance evaluation of current tools is mostly qualitative and
scenario-specific (``did my system do the job for my task?''), and amongst the
surveyed KRS, only few systems proposes an evaluation of their performances
that could be compared to other system.

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Knowledge instantiation}

How much knowledge is available? Which content? How big is the knowledge base?

\begin{itemize}
	\item  Which underlying knowledge (\emph{common-sense}, \emph{upper knowledge}\ldots{})
	\begin{itemize}
		\item  top-down approach?
	\end{itemize}

\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Surveyed systems}
\label{sect|surveyed-systems}

Table \ref{table|surveyed-systems} presents the eight knowledge representation
systems surveyed in this article.

This section briefly presents each of them.

\begin{landscape}
\begin{table}\footnotesize
\begin{center}

\begin{tabular}{p{2.2cm}p{1.6cm}p{4cm}lp{2.4cm}p{3.4cm}p{2.8cm}p{1.5cm}}
\toprule
{\bf Project} & {\bf Category} & {\bf Authors (Institution)} & {\bf Project homepage} & {\bf Programming language} & {\bf Knowledge model/Logical Formalism} & {\bf Reasoner} & Main reference \\
\midrule
{\sc KnowRob} & KRS & Tenorth, Beetz \par (TU Munich) & \url{ias.in.tum.de/kb/wiki} & {\sc Prolog} & {\sc Prolog} + OWL-DL & Custom \par ({\sc Prolog}) & \cite{Tenorth2009a} \\
ORO & KRS & Lemaignan, Alami \par (LAAS-CNRS) & \url{oro.openrobots.org} & {\sc Java} & OWL-DL ({\sc Jena}) & {\sc Pellet} & \cite{Lemaignan2010} \\
PEIS KR\&R & KRS & Daoutis, Coradeshi, Loutfi, Saffiotti \par (Örebro Univ.) & \url{www.aass.oru.se/~peis} & {\sc C}, {\sc CycL} & CycL (1st and 2nd order logics, modal logics) & & \cite{Daoutis2009} \\
CAST Proxies & Ubiquitous & Wyatt, Hawes, Jacobsson, Kruijff (Brimingham Univ., DFKI Saarbrücken) & & & Amodal proxies & & \cite{Jacobsson2008} \\
NKRL & Language & Zarri et al. \par (Paris Est Créteil Univ.) & & NKRL & & & \cite{Sabri2011} \\
GSM & KRS & Mavridis, Roy \par (MIT MediaLab) & & ? & & & \cite{Mavridis2006} \\
OMRKF & KRS & Suh et al. \par (Hanyang Univ.) & \url{incorl.hanyang.ac.kr/xe} & ? & Horn Clauses & ? & \cite{Suh2007} \\
Ke Jia Project & KRS & Chen et al. \par (Univ. of Science and Technology of China) & \url{www.wrighteagle.org/en} & ASP & ASP & ASP & \cite{Chen2010} \\
ARMAR/Tapas & KRS & Holzapfel, Waibel \par (Karlsruhe TH) & & ? & TFS (Typed Feature Structures) & & \cite{Holzapfel2008}\\
Golog & Language & Levesque (Toronto Univ.) & & {\sc Prolog} & & & \\
OBOC & KRS & Mendoza & & & & & \cite{Mendoza2005} \\
% & & Varadarajan, Vincze \par (TU Wien) & & & & & \cite{Varadarajan2011} \\ % -> affordances, but no implementation on a robot
% & & Kaelbling, Lozano-Pérez \par (MIT CSAIL) & & & & & \cite{Kaelbling2011} \\ % -> mostly planning under uncertainty
% & & Hertzberg (Osnabrück Univ.) \\ % -> affordances, semantic mapping
% (based on {\sc KnowRob} & & (JSK) \\

\bottomrule

\end{tabular}
\end{center}

\caption{List of surveyed systems. Projects are listed by category (\emph{KRS}
for systems that are explicit knowledge representation and reasoning modules,
\emph{Ubiquitous} for systems where knowledge processing is fully distributed,
\emph{Language} for languages used as KRS on robots), then names.}

\label{table|surveyed-systems}
\end{table}
\end{landscape}

\fxfatal{Bielefeld -> could not find much...}
\fxfatal{Kollar/Tellex -> really focusing on the natural language grounding}

\subsection{KnowRob}
\label{sect|knowrob}

\subsection{ORO}
\label{sect|oro}

\subsection{PEIS KR\&R}
\label{sect|peis-ecology}


{\sc PEIS Ecology}~\cite{Saffiotti2005} is a software \emph{ecosystem} that aim to binds autonomous
robotics with ambient intelligence (network of sensors). \emph{PEIS} stands for
\emph{Physically Embedded Intelligent System}: every robots or intelligent
device in the environment is abstracted as a PEIS.

Each PEIS physical component is running a \emph{PEIS Kernel} instance. Communication
between instance relies on a custom P2P communication protocol.

The PEIS architecture allows for adding new abilities through software components sharing the common \emph{tuple space}.

We survey here the semantic layer~\cite{Daoutis2009}, referred as \emph{PEIS KR\&R}, that includes symbolic representation and reasoning.

% More in details:
% - object identification based on viewpoint independent SIFT features
% - formalized anchoring system that explicitly match percieved attributes to predicates
% - Cyc predicates
% - ground 12 colors, based on a paper on color perception. Could be useful for us.
% - idem, they cite a paper on what spatial relations to compute
% - location of objects based on a previously provided semantic map (but not much on this semantic map)
% - two "memories": the robot memory stores the current list of percieved objects ; the archive memory stores what is not percieved anymore
% - uses directly Cyc (ie, 250 000 common sense concepts...), via CycL language -> 2nd and higher order logics (quantification over predicates, functions, etc)
% Remark: using 2nd order logic (ie meta statements), it would be easy to store the knowledge of each agent
% - disambiguation in concept name by asking human to decide amongst all concepts known by Cyc
% - template based natural language
% - experiment conducted in a "smart" indoor environmement + simple robot

\paragraph{Knowledge model} The PEIS Knowledge representation system relies on
the {\sc ResearchCyc} and {\sc CycL} language to represent knowledge. The {\sc CycL} language
allows to represent first order logic sentences and has extensions for modal logics and higher order logics.

\fxfatal{Is modal logics and higher order logics actually used in PEIS?} 

As a system relying on {\sc CycL}, contexts can be expressed as
\emph{microtheories}: the truth or falsity of a set of statement depends of the
\emph{microtheory} in which these statements are evaluated.

\fxfatal{OWA/CWA?}

\begin{figure}
	\centering
	\includegraphics[width=0.9\columnwidth]{stateofart/peis-architecture.pdf}
	\caption{The PEIS knowledge representation system, taken from~\cite{Daoutis2009}}
	\label{fig|peis-archi}
\end{figure}

The PEIS KR\&R system is deeply integrated to the general PEIS Ecology
\emph{smart} environment. Figure~\ref{fig|peis-archi} gives an overview of the
interactions between PEIS knowledge processing layers.

\paragraph{Knowledge Acquisition} The primary source for knowledge acquisition
is perception.  The PEIS ecosystem provides a SIFT-based object recognizer used
in conjunction with ceiling cameras for object localization.  Other perceptual
modalities are available (like human tracking, ambient environment monitoring).

A template-based natural language parsing system may also be used to add new
assertions to the system.

The system can ask the human for help to disambiguate between concept names.

\paragraph{Anchoring} Daoutis et al. formalize the issue of anchoring as
finding a \emph{predicate grounding relation} $g \subseteq \mathcal{P} \times
\Phi \times D(\Phi)$, where $\mathcal{P}$ is a set of predicate symbols, $\Phi$
a set of percept's attributes, and $D(\Phi)$ the domain of these attributes.

In the current implementation, object category (returned by the SIFT
classifier), color, location, spatial relations (both topological -- \emph{at},
\emph{near} -- and relative to the robot -- \emph{left}, \emph{behind}, etc.)
and visibility are the five classes of extracted attributes.

\paragraph{Integration in the robot architecture}
\label{sect|peis-integration}

The PEIS framework offers through the \emph{PEIS middleware} a practical way to
insert a new component into the shared \emph{tuple space}.  Thus, the KR\&R
module can be seamlessly integrated into the PEIS ecosystem.

\subsubsection{Notable experiments}
\label{sect|peis-expe}

\subsection{NKRL}
\label{sect|nkrl}

\emph{NKRL} stands for \emph{Narrative Knowledge Representation Language}.
While this language is developed since a long time by Zarri~\cite{Zarri1997,
Zarri2008}, recent research direction include application to the robotic
field~\cite{Sabri2011}. NKRL is not {\it per-se} a knowledge representation
system, as it is primarily a language. However, it is used as the
representation and reasoning mechanism for robots by Sabri et al.

\subsubsection{Intrinsic language features}
\label{sect|nkrl-intrinsic-features}

\paragraph{Expressiveness}

\subsubsection{Integration with physical world and in the robot architecture}
\label{sect|nkrl-integration}

...seem to be mostly WIP...


\subsection{CAST Knowledge model}
\label{sect|cast}

CAS (\emph{CoSy Architecture Schema}) Toolkit~\cite{Hawes2007} is a
comprehensive toolkit aimed at building cognitive architectures for robots
through a set of interconnected SAs (\emph{subarchitectures}). The CAS does not
expose a central knowledge base as seen in previous works. It instead
represents knowledge as unrooted \emph{proxies}. Those proxies are formally
defined in \cite{Jacobsson2008} as $p= \langle F_p, u_p \rangle$ where $F_p$ is
a set of instantiated features (like $\phi^{Colour}_{red}$) and $u_p$ a
\emph{proxies union} that form an equivalence class corresponding to one
entity.

A union of proxies forms a global amodal representation of an entity, that can
be explicitly shared and manipulated. Being not centralized, the knowledge
model can be qualified of \emph{ubiquitous}. Furthermore, knowledge source in
the CAS architecture is tightly bound to the on-line grounding process (be it
grounded in perception or in dialogue). While nothing seems to prevent it, no
{\it a priori} knowledge (including common-sense knowledge) is used.

Knowledge sharing is ensured by the event mechanism of CAST: modules can
monitor proxies for alteration by other modules. Jacobsson et al. mention how
this can apply to reinforcement learning: the vision module creates a proxy for
an orange object. This proxy get monitored by a learning module. In parallel,
the proxy is bound to an union by the natural language understanding module
that add new a feature like \emph{"this object is a fruit"}. The learning
module is called back, and can add this new information to its model.

In the presented implementation, the CAST knowledge model does no allow for
effectively representing actions or temporal information.\fxfatal{What about reasoning? can they retrieve for example 'all proxies for colorful objects'?}

\subsection{GSM}
\label{sect|gsm}

GSM (for \emph{Grounded Situation Model})~\cite{Mavridis2006} is a knowledge
representation system primarily built to ``facilitate cross-modal
interoperability'',  especially in the context of verbal interaction with a
robot.

Unlike most of the previously presented systems, GSM does not rely on any
formal language but rather on a layered data structure (Figure~\ref{fig|gsm})
that organizes the surrounding world into agents and relations between agents.
Each agent (any animate or inanimate object) is attached to a physical model (made
of \emph{body parts} that have properties like their position, color, etc.) and
a mental model (which is a recursively embedded GSM, thus allowing a sort of
theory of mind).

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{stateofart/gsm.pdf}

    \caption{Simplified hierarchical structure of the Grounded Situation Model,
    based on~\cite{Mavridis2006}}

    \label{fig|gsm}
\end{figure}

Properties are represented in three layers: a stochastic representation, close
to sensory percepts, a \emph{continuous single-valued} encoding of the
stochastic model, and a discrete, categorical model.

One notable feature of GSM is the \emph{bidirectionality} of the grounding
process: not only sensor percepts are abstracted into categories suitable for
human conversation, but human utterance (like ``There is a red ball in the
center of the table'') can also be turned into property descriptions. This
basically enable the knowledge representation system of the robot to
\emph{imagine} entities.

GSM also features several strategies for managing time and events.
\emph{Moments} are created by storing timestamped snap-shots of GSM, and
\emph{event classifiers} allow to define and detect events.

\paragraph{Experiments} GSM has mostly been tested on table-top manipulation
and interaction tasks (a ``conversational helping hand'' as stated by the
authors) realized on a 7-DOF arm equipped with force feedback, cameras for blob
tracking and speech recognition (Sphinx4). Mavridis and Roy provide in addition
an in-depth analysis of the performance of GSM by the mean of a standard
psycholinguistic test, the \emph{Token test}~\cite{DiSimoni1978}.

\subsection{OMRKF}
\label{sect|omrkf}

The Ontology-based Multi-layered Robot Knowledge Framework~\cite{Suh2007}
(OMRKF) is a knowledge representation system based on four inter-related
\emph{classes} of knowledge (Figure~\ref{fig|omrkf}). It proposes a layered
approach to knowledge representation that allows to integrate the grounding
process to the knowledge representation process. OMRKF knowledge model is
implemented with Horn clauses.

\begin{figure}
    \centering
    \includegraphics[width=0.8\columnwidth]{stateofart/omrkf.pdf}

    \caption{OMRKF organizes knowledge into four \emph{classes}, each composed
    of three \emph{levels}. The figure shows some examples of link between
    knowledge classes and knowledge levels. Based on~\ref{Suh2007}.}

    \label{fig|omrkf}
\end{figure}

Each level of knowledge is build as three stages of ontological realization: a
\emph{meta-concept} (the level itself, like ``temporal context'', ``behaviour''
or ``object feature'', a taxonomy of concepts inside this level (for instance
$cup : Object \sqsubseteq tableware : Object$) and an instantiation of the
taxonomy ($cup1 : cup$).

Environment is represented in OMRKF in the $space : Model$ knowledge level as a
classical three layers mapping (metric, topological and semantic maps). Objects
(in $object : Model$) are localized in $space : Model$ through Voronoi nodes.

The knowledge class $Context$ proposes an explicit statement of spatial context
(mostly geometric relations between objects), temporal context and a more
general \emph{high-level} context, inferred from spatial and temporal contexts.

Finally, the $Activity$ knowledge class store compound actions in a HTN-like
structure, exploited at run-time by a planner.

\paragraph{Experiments} Experiments conducted with OMRKF include finding
kitchen objects and reporting about their state to a human.  This experiment
also shows how OMRKF can deal with objects only partially matched by their
descriptor by introducing a $candidate()$ function.

\subsection{Ke Jia Project}
\label{sect|kejia}

The Ke Jia project~\cite{Chen2010} integrates on a mobile platform a knowledge
representation language with natural language processing, task planing and
motion planing.

Knowledge representation relies on \emph{Action Language C}, itself based on
\emph{Answer Set Programming} (ASP)~\cite{Gelfond2008}. These languages, that
are syntactically close to Prolog, are based on \emph{stable models} of logic
programs, and support non-monotonic reasoning. Default and non-monotonic
reasoning has been especially researched within the Ke Jia project for symbolic
task planing~\cite{Ji2011} and underspecified natural language processing.

Amongst other features, the natural language processing capabilities of the
system support acquisition of new logical rules at run-time.

\paragraph{Experiments} The Ke Jia robot has been demonstrated in several tasks
involving human-robot interaction with natural language. These tasks include a
task with multiple \emph{pick \& carry} that are globally optimized, naive
physics reasoning via taught rules or more complex scenarii with the robot
delivering drinks, taking into account changing and mutually exclusive
preferences of users.

\subsection{ARMAR/Tapas}

{\sc Tapas} is the name of the knowledge representation system and dialogue
manager found on the ARMARIII robot~\cite{Holzapfel2008}.

Knowledge in {\sc Tapas} exists as procedural knowledge (plans) and declarative
knowledge. The later is split into \emph{lexical knowledge}, \emph{semantic
knowledge} and a database of identified objects (with their properties). The
\emph{lexical knowledge} contains lexical and grammatical informations about
the objects. The \emph{semantic knowledge} is organized into an ontology
relying on \emph{typed feature structures} (TFS,~\cite{Carpenter1992}, a
formalism originating from the computational linguistics community, and a
superset of first-order logic).

{\sc Tapas} has a strong focus on natural language grounding. It proceeds by
generating grammars from properties represented in the ontology to parse and
understand dialogue.

Another focus is put on handling unknown words and objects. {\sc Tapas}
provides routines to recognize unknown entities, and propose and interactive
and iterative verbal process to categorize (including adding new categories)
those new concepts.

%%%%%%%%%% Underlying knowledge model table %%%%%%%%
\begin{table}
\begin{center}

\begin{tabular}{lp{4cm}}
\toprule
{\bf Project} & {\bf Common-sense \par knowledge source} \\
\midrule
{\sc KnowRob} & {\sc OpenCyc}, processed web content, custom OWL-DL ontology \\
ORO & {\sc OpenCyc}, custom OWL-DL ontology \\
PEIS Ecology & {\sc ResearchCyc} \\
NKLR &  None \\
CAST Proxies &  None \\
GSM &  Predefined categories \\
OMRKF & {\it A priori} knowledge structure and axioms, custom set of instances\\
Ke Jia & None \\
ARMAR/{\sc Tapas} & Custom ontology related to the kitchen\\

\bottomrule

\end{tabular}
\end{center}
\caption{Underlying knowledge sources for each project}
\label{table|knowledge-sources}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Towards the next generation of Knowledge Representation Systems for Robotics}
\label{sect|conclusion}

This section tries to summary the various approaches we surveyed in the
previous sections and to draw some research perspectives. This section does not
exactly follow the list of compared features presented in
section~\ref{sect|features}. We have organized it along four axis: \emph{what
can be represented?}, \emph{How knowledge is created and grounded?}, \emph{What
can be done with the knowledge?} and \emph{How to use knowledge in the whole,
larger robot architecture?}.

Table~\ref{table|contribution-by-systems} proposes a summary of the main scopes
of \emph{contribution} of the systems we have surveyed. This overview helps to
identify the main weaknesses of current approaches of knowledge representation
regarding the long-term goal of \emph{human-level robots}.

\begin{landscape}
\begin{table}\footnotesize
\begin{center}

\begin{tabular}{cp{4.5cm}p{2.3cm}p{2cm}p{1cm}p{1cm}p{1cm}p{1cm}p{1.5cm}p{2cm}p{1.5cm}p{1.5cm}}
\toprule
\multicolumn{2}{c}{\bf Category} & {\sc KnowRob} & {ORO} & {\sc PEIS} & {\sc CAST} & {\sc Ke Jia} & {\sc NKRL} & {\sc GSM} & {\sc OMRKF} & {\sc ARMAR} & {\sc Hertzberg} \\

\midrule

\multirow{5}{0.7cm}[0.2cm]{\turn[1.5cm]{90}{\bf Expr. power}} & Logical formalism & Prolog & DL (OWL) & {\sc CycL} &  & ASP &  & & Horn clauses & TFS & \\
 & OWA/CWA & CWA & OWA & ? & ? & CWA & ? & ? & ? & ? & ? \\
 & Modeling uncertainty & +++ (extension) & & & & & & ++ (stochastic) & + (\emph{candidate} entities) \\
 & Meta-cognition & ++ & ++ & & & & & & \\
\hline
\multirow{6}{0.2cm}{\turn{90}{\bf Model}} & Space Representation & & & & & & & & ++  \\
 & Time representation & & & & & & & + (snapshots) & +  \\
 & Context & & & & & & & & ++  \\
 & Alternative worlds & & +++ & & & & ++ & &  \\
 & Introspection & + (SRDF) & & & & & & &  \\
 & Memory models & & + & & & & & &  \\
\hline
\multirow{10}{0.2cm}{\turn{90}{\bf Reasoning}} & Standard FOL reasoning & +++ & +++ & & & +++ & & & +  \\
 & Knowledge structure alteration & & ++ & & & ++ & & & & ++ &  \\
 & Lazy evaluation & +++ & & & & & & &  \\
 & Reasoning under uncertainty & ++ (extension) & & & & & + & & \\
 & Non-monotonic reasoning & & & & & +++ & & &  \\
 & Presupposition accommodation & & & & & & & +++ & \\
 & Prediction, projection, diagnosis & & & & & & & & \\
 & Physics-based reasoning & +++ (extension) & & & & & & &  \\
 & Planning & + & +++ (external) & & & ++ & & & + \\
 & Learning & & + & & & & & & & ++ &  \\
\hline
\multirow{3}{0.7cm}{\turn[1cm]{90}{\bf Knw. acq.}} & Acquisition techniques & & & & & & & & \\
 & Grounding & +++ \par (semantic maps) & ++ (external amodal model) & & & & & ++ & ++ \\
 & instantiation & & & & & & & & \\
\hline
\multirow{4}{0.2cm}{\turn{90}{\bf Integ.}} & ...with sensori-motor layers & & & & & & & + & + \\
 & ...with executive layers & & & & & & & + & ++ \\
 & NLP & & +++ (external) & & & +++ & & +++ & & +++ & \\
 & Monitoring and debugging & & & & & & & & \\
 & Performances & & & & & + & & +++ (Token test) & & & \\

\bottomrule

%\end{tabularx}
\end{tabular}
\end{center}
\caption{Main domain of contribution of each surveyed systems. Rating goes from +++ (major focus) to + (secondary interest). An empty cell means that 
the system does not consider this domain.}
\label{table|contribution-by-systems}
\end{table}
\end{landscape}

