\chapter{Symbolic knowledge representation}

\fxnote{Support material: \emph{What is a knowledge representation} by Davis,
Shrobe and Szolovits,
\url{http://groups.csail.mit.edu/medg/ftp/psz/k-rep.html}}

\section{What do we call "knowledge"?}
\label{sect|on-knowledge}

First, we want to represent knowledge and not only information.  Both knowledge
and information are contextualized. Knowledge also has a \emph{cultural}
context that enable interpretation. 

Note that the origins of AI where in purely symbolic models, that were
not usable either. Rich interleaving between geometric reasoning and symbolic
models is required. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Typology of Knowledge Representation Systems Requirements for Robotics}
\label{sect|features}

We presented an imaginary cooking scenario where numerous desirable
features for a knowledge representation system for robotics have been
identified in no particular order.

This section proposes a more formal typology of desirable features for a such a
system. For each feature, we provide a short definition along with links to
relevant literature.

Table~\ref{table|contribution-by-systems}, at the end of the article,
summarizes all these features with the main domain of contribution of each
surveyed KRS.

\subsection{Expressiveness: What Can be Represented?}
\label{sect|expressiveness}

\subsubsection{Introduction: Main Logic Formalisms}

The main role of a knowledge representation system is to provide an adequate
representation system to store facts and concepts that can be informally
described in natural language.

Formal logic aims at providing such a representation system with the added
value of providing a tractable support for inference and reasoning.

Most (but not all) of the systems we survey rely on a particular logic
formalism. The choice of the formalism has a strong impact, on one side, on the
range of ideas that can be expressed conveniently (\emph{practical
expressiveness}) or at all (\emph{theoretical expressiveness}), on the other
side, on the ability to solve the inference problem (called
\emph{decidability}: is a given logical sentence true in my model?) in a
tractable manner.

A large number of logic formalism do exist, we shall summarize below the most
relevant for systems actually deployed in current robotic architectures.

\emph{Predicate logic} is the family of logic formalisms the most commonly
found in knowledge representation. It distinguishes itself from the simpler
\emph{propositional logic} by the use of quantification to increase generality.
\emph{First-order logic} (FOL) is the subpart of \emph{predicate logic} where the
objects of \emph{predicates} (or \emph{formulae}) are simple \emph{terms},
while in \emph{higher-order logics}, predicates can be themselves objects of
other predicates.

The family of \emph{description logics}~\cite{Baader2008} also play an
important role. It is a subset of the first-order logic, with some extensions
in second-order logic. Description logics are notable because most of them are
known to be decidable. In description logic, axioms are build from
\emph{concepts}, \emph{roles} (that are unary or binary predicates) and
\emph{individuals}. The W3C OWL-DL standard is a widely-used language to describe
domains with the description logic.

Because Description Logics have been originally created from the perspective of
a \emph{knowledge representation language} and not a logic language, their
terminology (\emph{concept} or \emph{class}, \emph{role} or \emph{property},
\emph{individual},...) is well-suited to knowledge description and we may use
it in the remaining of this paper outside of the strict context of  Description
Logics.

\emph{Modal logic}, that allow for statement qualification like
\emph{possibility} or \emph{necessity}, have been shown to be closely related
to description logics. Modal logic allows to represent conveniently parallel
possible worlds and facts like ``the robot knows \emph{that the human knows}
how to read a recipe''.

\fxfatal{On modal logics, see the remark of McCarthy, in \cite{McCarthy2007}, section 3}

\emph{Temporal logic} are designed to represent and manipulate assertions whose
truth value may vary in time.

One last class of logics that is of particular relevance for robotic
applications is the \emph{probabilistic logics} or \emph{Bayesian logics}.
These logics provide a formal framework to reason on propositions whose truth
or falsity is uncertain. We elaborate below on the representation of uncertainty.

Note that most of these logic formalisms are still active research field by
their own, and practical considerations (especially the availability of
reasoners performant enough for on-line use on a robot) often constraint the
choice of a logical formalism and a level of expressive power.


\paragraph{Some examples}

\emph{The robot knows that a blue bottle is laying on the table.}

\emph{The robot knows that the human knows about the position of the bottle,
but the robot does not know what the human actually know about it.}

\subsubsection{Expressive Power}

Logical formalisms each bring a certain level of expressive power. For
instance, the following classical syllogism can not be represented in
propositional logic because of the use of \emph{universal quantification}:

\begin{quote}
\begin{enumerate}
    \item All men are mortal,
    \item Socrates is a man,
    \item Therefore, Socrates is mortal
\end{enumerate}
\end{quote}

However, the following weak version of the syllogism can be represented in
propositional logic:

\begin{quote}
\begin{enumerate}
    \item If Socrates is a man, then Socrates is mortal,
    \item Socrates is a man,
    \item Therefore, Socrates is mortal
\end{enumerate}
\end{quote}

Generally speaking, expressive power comes at the cost of ease of
\emph{satisfiability} and \emph{consistency}\footnote{We precise these concepts
at section~\ref{sect|reasoning}.} tests, possibly leading to undecidable formal
systems, \ie systems where it is proven that a proposition can not be decided
to be true or false.

The relationship between expressive power and the complexity of reasoning problem that
follow has been partially formalized for Description Logics.
Zolin~\cite{ZolinDLComplexityNavigator} maintains a ``complexity navigator''
for Description Logics that allows to conveniently explore this relationship
and indexes most of the litterature on that subject.

\subsubsection{Open World and Close World Assumptions}

The \emph{close world} (CWA) vs. \emph{open world} (OWA) assumption names a
modelling choice on the \emph{completeness} of a knowledge domain. In the close
world assumption, a proposition that can not be proven true is assumed to be
false (\emph{negation by failure}), while in the open world assumption, a
proposition may be considered either true, false or unknown.

This distinction is important in robotics were the robot may have to manipulate
concepts with only partial knowledge on them. For instance, let imagine a robot
that sees a bottle on a table, whose bottom is hidden by another object. The
robot can not prove that the bottle is indeed \emph{on} the table. A knowledge
representation system relying on the closed world assumption would then assume
the bottle is \emph{not} on the table ($\lnot R^{CWA}_{isOn}(bottle, table)$)
whereas with the open world assumption, the proposition $R^{OWA}_{isOn}(bottle,
table)$ would be undecided. Example in table~\ref{table|cwa-owa-example} provides
a simple, concrete example of consequences of the CWA/OWA choice on reasoning.

\begin{table}
	\begin{center}
	\begin{tabular}{ll}
	{\bf Action} & {\bf Part involved} \\
	\hline
	{\tt PickSoftly} & hand \\
	{\tt PickAndPlace} & arm, hand \\
	{\tt MoveArm} & arm \\
	\hline
	\end{tabular}
	\end{center}
	\caption{Assuming the question is: \emph{select actions that do not require
	to move the arm}, a CWA reasoner would return {\tt PickSoftly} whereas an
	OWA reasoner would not return anything if the {\tt PickSoftly} action is
	not explicitly said not to involve the arm.}
	\label{table|cwa-owa-example}
\end{table}

Domains constrained with the closed world assumption lead to more tractable
inference problems, and allow for instance the use of logic languages like
Prolog. Thus, several approaches exists to \emph{locally close} a domain (\cf
Levesque~\cite{Levesque2008}, section 24.3.2 for a summary of those).


\subsubsection{Representation of uncertainty and likelihood}

Sources of uncertainty for a robot are two-fold: uncertainty \emph{intrinsic}
to facts (like \emph{``It may rain tomorrow''}), uncertainty caused by
imperfect perception of the world (\emph{``Is the bottle really on the
table?''}). Most logics do not account explicitly for uncertainty. It must be
either relied on specific logics (like Bayesian logics) or on extensions of
classical logics.

\subsubsection{Meta-cognition: knowledge on the knowledge}

As stated by Josyula and Raja, meta-cognition is composed of both
\emph{``meta-level control of cognitive activities and the introspective
monitoring of such activities to evaluate and to explain them"}.

A knowledge representation system endowed with \emph{meta-cognition} is not
only able to manipulate knowledge but also to exhibit and manipulate the
structure of its knowledge and the reasoning process. For instance, the ability
to explain a logical inconsistency in a KRS is a meta-cognitive function.
Sloman~\cite{Sloman2011}

At section~\ref{sect|introspection} below, we discuss the idea of
introspection.  Meta-cognition can be viewed as the technical facet of the
introspection in general.


%%%%%%%%%%
\subsection{How things are represented?}
\label{sect|higher-level-domain-representation}

\subsubsection{Role Representations}

Spatio-Temporal Representations:

\paragraph{Representation of time}

As an agent acting at human-like time scale and dealing with temporal concepts
(like actions), a robot may want to represent, and possibly to reason, about
time. Time representation is split into two distinct abilities: representing
time points (both in the past -- which is roughly equivalent to assignment of
timestamps to events the robot perceives -- and in the future), and
representing \emph{passing time} (durations, timespans) like in \emph{``the
eggs will be cooked in 10 min''}.

\fxfatal{Discuss time chronicles~\cite{Ghallab1996}}

We call a system that do not account for time (\ie that permanently lives in
present) \emph{atemporal}.

\paragraph{Representation of space}

\paragraph{Representation of events and actions}

\subsubsection{Context modeling}

\emph{Knowledge is contextualized information}\fxfatal{Find someone respectable
how said that :-)}: it is essential for the robot to associate the facts it
represents to a \emph{context}. The context carries the keys for the
interpretation of the information. It covers the \emph{domain of validity} of
the facts, the \emph{common-sense} knowledge required to fill the gaps in the
representation\fxfatal{give an example}, \fxfatal{What more?}.

\subsubsection{Possible-Worlds and representing what others know}
\label{sect|possible-worlds}

    
Linked to the context representation, but seen from another angle, knowledge
representation systems may provide explicit ways to model other point of view
on the world. This ability is often referred as the \emph{perspective taking}
ability.

\cite{Levesque2008}, p. 4

\subsubsection{Introspection: Who am I? What can I do?}
\label{sect|introspection}

\paragraph{The introspective robot}

Introspection is the ability to self-describe: what are my capabilities, what
is my state (performing some action, idling, etc.), what are my beliefs (\ie my
knowledge), what are my intentions and my plans?

Introspection must be distinguished from meta-cognition: While introspection
may require meta-cognition (for instance to be able to expose its internal
knowledge), it is not always mandatory. The current state of the robot can be
represented as a simple instanciation of a specific category (for instance, if
the robot give an object to the human, this state could be represented with the
triplets \setstmt{robot performs action1, action1 isA Give}.

\paragraph{Modelling of the robot capabilities}

A particularly important aspect of introspection relates to the description of
its own capabilities: which sensors/actuators/computation services exist and
are currently available ?  While at a first level, these descriptions can be
static (\eg the robot has one laser scanner and two arms), at a second level,
the description is updated and reflect the current (and possibly past and
future) state of the robot. Note that these description may also invlove
geometric descriptions (a kinematic chain, the pose of a device, etc.) that may
be deported outside of the main knowledge base. Efforts trying to formalize,
maintain and expose the capabilities and state of a robot are not new (and
ground themselves in work and techniques for self-descriptive remote procedure
calls in computing science), but take a renewed importance with applications
for high-level multi-robot cooperation. Recent work in that direction
include~\cite{Kunze2011}.

\subsubsection{Memory}
\label{sect|memory}

Memory has been studied at lenght in the cognitive psychology and
neuropsychology communities: Atkinson and Shiffrin~\cite{Atkinson1968}
introduce the idea of \emph{short-term} and \emph{long-term} memory,
Anderson~\cite{Anderson1976} splits memory into \emph{declarative} (explicit)
and \emph{procedural} (implicit) memories, Tulving~\cite{Tulving1985} organizes
the concepts of \emph{procedural}, \emph{semantic} and \emph{episodic} memories
into a hierarchy. Short-term memory is refined with the concept of
\emph{working memory} by Baddeley~\cite{Baddeley2010}
(Figure~\ref{fig|memory_models}).

\begin{figure}
    \centering
    \includegraphics[width=0.9\columnwidth]{typology/memory_models.pdf}
    \caption{Overview of the main types of memories}
    \label{fig|memory_models}
\end{figure}

Most knowledge representation systems offers some kind of memory as a pool of
facts that are not forgotten by the robot until it is halted (this memory is
often refered as a \emph{working memory}, but with a meaning unrelated to
Baddeley's definition). Some systems may propose persistent storages that allow
the robot knowledge to grow over time, while other may offer a larger range of
memory categories, like short term memory (that lasts for a couple of seconds)
or episodic memory (that allows the robot to selectively remember facts
associated to specific events).


%%%%%%%%%%
\subsection{Reasoning Techniques}
\label{sect|reasoning}

\subsubsection{Standard reasoning techniques}

Being based on logical languages to represent the knowledge, most of the
systems we survey allow various forms of \emph{reasoning}.

We call \emph{standard reasoning techniques} techniques based on logical
inference, using resolution algorithms like \emph{forward chaining},
\emph{backward chaining} or \emph{semantic tableaux}.

Main reasoning problems include \emph{concept satisfiability},
\emph{consistency checking} and \emph{instance checking}.

Concept satisfiability verifies if it is possible to find a non-empty
\emph{interpretation} of a concept (or an expression defining a concept) in the
knowledge model. For instance, the formula \concept{Plant} $\land$
\concept{isRed}, which defines the concept of red plants, is satisfiable in a
model \concept{KB} iff $\exists a, $ \concept{Plant}$(a) \land$
\concept{isRed}$(a)$, \ie if we can find at least one red plant $a$ in our
model.

Checking the consistency of a model is equivalent to checking the
satisfiability of each of the concept defined in the knowledge model.

Instance checking consists in verifying that an individual $a$ is an
interpretation of a concept (or concept expression) $C$ in the knowledge model.
A typical example would be that we are provided with an instance
\concept{object1} and we want to know if this object is a kind of
\concept{Bottle} or \concept{Glass}.

Reasoners can then provide other type of inferences, like:

\begin{itemize}
    \item class subsumption (similar to inheritance)

    \item reasoning on roles properties, including:
        \begin{itemize}
        \item entailments based on roles domain and range (for instance, if the
        domain of the role \concept{thinksOf} is known to be
        \concept{ThinkingAgent}, then \concept{thinksOf}$(a, b) \to
        $\concept{ThinkingAgent}$(a)$),

        \item universal, existantial and cardinality constraints (including \concept{allValue}, 
        \concept{someValue}, \concept{hasValue}),

        \item property characteristics (symmetry, transitivity, etc.)

        \end{itemize}

    \item complex concept expressions like: \par \footnotesize \concept{Bottle}
    $\equiv$ \concept{Artifact} {\bf that} (\concept{hasShape} {\bf value}
    \concept{cylinderShape})\footnote{This example uses the \emph{Manchester
    syntax}, \url{http://www.w3.org/TR/owl2-manchester-syntax/}} \normalsize

    \item set operations like: \par \footnotesize \concept{Color} $\equiv$ {\bf
    unionOf}(\concept{blue}, \concept{green}, \concept{orange},
    \concept{black}...) \normalsize

\end{itemize}

\paragraph{Rule Languages}
\item generic SWRL ({\em Semantic Web Rule Language}) rules like: \par
        \footnotesize \concept{looksAt(?agt, ?obj)} $\land$
        \concept{pointsAt(?agt,?obj)} \par $\Rightarrow$ \concept{focusesOn(?agt, ?obj)}
        \normalsize 


\subsubsection{Alteration of the knowledge structure}

In systems

All systems allow to modify the ABox, not always possible to alter the TBox


\subsubsection{Lazy evaluation}
\label{sect|lazy-evaluation}


\subsubsection{Reasoning with uncertainty}


\subsubsection{(Non) Monotonic Reasoning}

\emph{Monotonic reasoning} means that addition of new assertions to a knowledge base
can only extend the set of assertions that can be inferred, while a
\emph{non-monotonic} reasoning scheme may lead to retraction of facts.
McCarthy coined a famous example to illustrate the need of non-monotonic reasoning:

\begin{quotation}
Consider putting an axiom in a common sense database asserting that birds can
fly. Clearly the axiom must be qualified in some way since penguins, dead birds
and birds whose feet are encased in concrete can't fly. A careful construction
of the axiom might succeed in including the exceptions of penguins and dead
birds, but clearly we can think up as many additional exceptions like birds
with their feet encased in concrete as we like. Formalized non-monotonic
reasoning provides a way of saying that a bird can fly unless there
is an abnormal circumstance and reasoning that only the abnormal circumstances
whose existence follows from the facts being taken into account will be
considered.
\end{quotation}

Another important application of non-monotonic reasoning is representation of
change: for example, to make an omelette, you need to crack eggs and wipe them.
The eggs disappear and are replaced by an omelette:

\concept{Egg}$(a) \wedge $ \concept{Egg} $(b) \wedge $
\concept{MakeOmelette}$(a, b, c) \to \lnot $ \concept{Egg}$(a) \wedge \lnot $
\concept{Egg}$(b) \wedge $ \concept{Omelette}$(c)$

The insertion of the proposition \concept{MakeOmelette}$(a, b, c)$ leads to
retraction of other facts. This rule requires non-monotonic reasoning to be
applied.

\emph{Default logic} is one of the formal logic that account for representing
general truth and exceptions to it. However, due to computational complexity of
these model (most of inferences in default logic are known to be $NP$-complete
problem), classical logics and most of the existing reasoners do not allow
non-monotonic reasoning. For instance, the SWRL rule language, usually
associated to the OWL-DL ontology language, do not allow non-monotonic
reasoning.

\fxfatal{Make clear 'who does not allow non-monotonic-reasoning': logics? rule
languages? reasoner?}

One important exception if the \emph{Negation as failure} inference rule, as
implemented by {\sc Prolog} for instance, that allows for non-monotonicity, but
only \emph{within the closed world assumption}.

\fxfatal{Give here an example of non-monotonic reasoning with Prolog}

A monotonic system does not theoretically allow for knowledge retractation,
which is an important issue in the robotic context where the world model is
likely to be often altered.  However it is a practical issue only if the
reasoning process is \emph{continuous} during the whole robot's activity
lifespan. It is often possible to stop the reasoner, alter the knowledge, and
restart the inference process on a new domain.
\fxfatal{Rephrase to emphasize that when new evidences appear, it is anyway often a
good idea to restart the reasoner.}

\fxfatal{Mention that the 'change of world' issue can also be dealt with appropriate
time representation.}
\fxfatal{Mention that probabilistic reasoning lead to implicit non-monotonic reasoning}


\subsubsection{Presupposition accommodation}
\label{sect|presupposition-accomodation}

\emph{Presupposition accommodation} is the ability for the system to
automatically create a context allowing to make sense of a proposition.

For instance, we can imagine a human telling a robot \emph{Please get me the
bottle that is behind you}. If the robot has not yet see what is behind it, it
needs to assume (and represents in its knowledge model) that a undefined bottle
can be found somewhere in the half of space behind it.

A knowledge representation system able to copte with presupposition
accommodation would be able to take into account this (usually under-defined)
information that is not grounded into perception for later inferences.

This ability to imagine a physically state of the world that is not actually
perceived can be seen as the converse of the grounding ability.

Note also that presupposition accommodation implies a bidirectional link of the
symbolic knowledge model with a geometric (or physical) model of the
environment. This article focuses on symbolic knowledge representation systems,
but we shall mention when a KRS explicitly provides support for presupposition
accommodation.

\subsubsection{Prediction, projection and diagnosis tasks}
\label{sect|prediction-projection}

Levesque~\cite{Levesque2008} distinguish two main tasks, the \emph{projection
task} and the \emph{legality task}.

\paragraph{Projection task}: determining whether or not some condition while
hold after a sequence of actions.

\paragraph{Legality task}: determining whether a sequence of action can be
performed starting in some initial state.

\paragraph{Diagnosis}: this corresponds to the ability to rewind on past events
in case of failure to provide possible explanation. This can be seen as the
temporal reverse of the projection task.

\subsubsection{Physics-based reasoning}
\label{sect|physics}

As embodied entities, robots have to interact with physical entities.
\emph{Naive physics reasoning} covers all the everyday reasoning the humans
unconsciouly perform, like taking into account gravity (``if I drop a ball, it
falls down'') or common physical properties of objects (``a glass may break if
dropped'', etc.). Many of the interactions with our everyday environments are
ruled by such laws that are difficult to exhaustively encode.

Some systems \cite{Kunze2011a} rely on external dedicated physics engine to
compute symbolic facts from on-demand physics simulation. This, however,
requires a tight integration between the symbolic model and a geometric model
that carries the geometries and physical properties of objects.

\subsubsection{Planning}
\label{sect|planning}

Making decision based on prediction

\subsubsection{Alteration of the knowledge structure}

In systems

All systems allow to modify the ABox, not always possible to alter the TBox

\subsubsection{Learning}
\label{sect|learning}

%%%%%%%%%%%%%%%%%
\subsection{Acquiring Knowledge}

\subsubsection{Knowledge acquisition and modalities merging}
\label{sect|knowledge-acquisition}

In the survey inclusion criteria, we have insisted on only considering robotic
systems that acquire knowledge by themselves, during their runtime.

\emph{Acquiring knowledge} means in our context building new statements
(usually as new logical facts) connected to our existing knowledge from
external sources of information. We consider mainly three of them:
proprioceptive/exteroceptive sensors, interaction with other agents, humans or
robots, and remote databases. This process has generally at least two steps:
the information acquisition by itself, and the \emph{transformation} of the
information into knowledge, \emph{aligned} with the robot existing model
(following our terminology for information and knowledge, as discussed in the
introduction).

Knowledge acquision is generally not done directly in the knowledge
representation system. On the contrary, several, if not numerous, external
components are usually required to convert percepts into symbolic facts and to
ground them.

While we do not review in this article all these systems, the whole process of
knowledge acquisition is central in cognitive robotic architecture and the
design of knowledge representation systems can influence or be influenced by
the approach to knowledge acquisition.

In particular, complex robotic systems often require multi-modal perception
capabilities (for instance, a robot can only interpret an utterance like ``this
is a plate'' if it is able to understand gestures, understand natural language
and merge them in a timely manner). Multi-modal interpretation can take place
at various levels, but in many cases (especially if the modalities are of very
different natures, like in the example above) merging will require
symbolic-level reasoning. The KRS has a direct impact on the feasability and
ease of such operations.

\paragraph{Perception}
\paragraph{Interaction}
\paragraph{External sources (Web, upper ontologies, ...)}
\paragraph{Learning}

\subsubsection{Grounding/anchoring strategies}
\label{sect|grounding}

\subsubsection{Ability to automatically create new object instances}
\label{sect|new-instances}

%%%%%%%%%%%%%%%%%
\subsection{Practical Integration in Robotic Architectures}
\label{sect|integration-robot}

\subsubsection{Integration with sensori-motor layers}
\label{sect|integration-sensorimotor}

Ability to ``listen'' to the robot internal structures.

\subsubsection{Integration with executive layers}
\label{sect|integration-executive-layers}

\paragraph{Language integration}

\paragraph{Events}

\subsubsection{Monitoring and debugging}
\label{sect|debugging}

\subsubsection{Is it fast enough? Scalability and responsiveness}
\label{sect|scalability}

%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Knowledge instanciation}

How much knowledge is available? Which content? How big is the knowledge base?

\begin{itemize}
	\item  Which underlying knowledge (\emph{common-sense}, \emph{upper knowledge}\ldots{})
	\begin{itemize}
		\item  top-down approach?
	\end{itemize}

\end{itemize}





\section{State of the art in knowledge representation systems for robotics}
\label{sect|krs-survey}


